[
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/usage-r.rst.txt",
    "html": "With R\n======\n\n\nIntroduction\n------------\n\n\n`R <https://www.r-project.org/>`_ is a free software environment for statistical computing and graphics. The `reticulate <https://rstudio.github.io/reticulate>`_ package provides a comprehensive set of tools for seamless interoperability between Python and R. It basically allows for execution of Python code inside an R session, so that Python packages can be used with minimal adaptations, which is ideal for those who would rather operate from R than having to go back and forth between languages and environments.\n\nThe package provides several ways to integrate Python code into R projects:\n\n- Python in R Markdown\n- Importing Python modules\n- Sourcing Python scripts\n- An interactive Python console within R.\n\nComplete vignette: `Calling Python from R <https://rstudio.github.io/reticulate/articles/calling_python.html>`_.\n\n\nThis tutorial shows how to import a Python scraper straight from R and use the results directly with the usual R syntax: `Web scraping with R: Text and metadata extraction  <https://adrien.barbaresi.eu/blog/web-scraping-text-metadata-r.html>`_.\n\n\nInstallation\n------------\n\n\nThe reticulate package can be easily installed from CRAN as follows:\n\n.. code-block:: R\n\n    > install.packages(\"reticulate\")\n\n\nA recent version of Python 3 is necessary. Some systems already have such an environment installed, to check it just run the following command in a terminal window:\n\n.. code-block:: bash\n\n    $ python3 --version\n    Python 3.8.6 # version 3.6 or higher is fine\n\nIn case Python is not installed, please refer to the excellent `Djangogirls tutorial: Python installation <https://tutorial.djangogirls.org/en/python_installation/>`_.\n\n\n\n``Trafilatura`` has to be installed with `pip <installation.html>`_, `conda <https://docs.conda.io/en/latest/>`_, or `py_install <https://rstudio.github.io/reticulate/reference/py_install.html>`_. Skip the installation of  Miniconda if it doesn't seem necessary, you should only be prompted once; or see `Installing Python Packages <https://rstudio.github.io/reticulate/articles/python_packages.html>`_.\n\nHere is a simple example using the ``py_install()`` function included in ``reticulate``:\n\n.. code-block:: R\n\n    > library(reticulate)\n    > py_install(\"trafilatura\")\n\n\n\nDownload and extraction\n-----------------------\n\nText extraction from HTML documents (including downloads) is available in a straightforward way:\n\n.. code-block:: R\n\n    # getting started\n    > install.packages(\"reticulate\")\n    > library(reticulate)\n    > trafilatura <- import(\"trafilatura\")\n\n    # get a HTML document as string\n    > url <- \"https://example.org/\"\n    > downloaded <- trafilatura$fetch_url(url)\n\n    # extraction\n    > trafilatura$extract(downloaded)\n    [1] \"This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\\nMore information...\"\n\n    # extraction with arguments\n    > trafilatura$extract(downloaded, output_format=\"xml\", url=url)\n    [1] \"<doc sitename=\\\"example.org\\\" title=\\\"Example Domain\\\" source=\\\"https://example.org/\\\" hostname=\\\"example.org\\\" categories=\\\"\\\" tags=\\\"\\\" fingerprint=\\\"lxZaiIwoxp80+AXA2PtCBnJJDok=\\\">\\n  <main>\\n    <div>\\n      <head>Example Domain</head>\\n      <p>This domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.</p>\\n      <p>More information...</p>\\n    </div>\\n  </main>\\n  <comments/>\\n</doc>\"\n\nFor a full list of arguments see `extraction documentation <corefunctions.html#extraction>`_.\n\nAlready stored documents can also be read directly from R, for example with CSV/TSV output and ``read_delim()``, see information on `data import in R <https://r4ds.had.co.nz/data-import.html>`_.\n\nThe ``html2txt`` function extracts all possible text on the webpage, it can be used as follows:\n\n.. code-block:: R\n\n    > trafilatura$html2txt(downloaded)\n\n\nOther functions\n---------------\n\nSpecific parts of the package can also be imported on demand, which provides access to functions not directly exported by the package. For a list of relevant functions and arguments see `core functions <corefunctions.html>`_.\n\n\n.. code-block:: R\n\n    # using the code for link discovery in sitemaps\n    > sitemapsfunc <- py_run_string(\"from trafilatura.sitemaps import sitemap_search\")\n    > sitemapsfunc$sitemap_search(\"https://www.sitemaps.org/\")\n    [1] \"https://www.sitemaps.org\"\n    [2] \"https://www.sitemaps.org/protocol.html\"\n    [3] \"https://www.sitemaps.org/faq.html\"\n    [4] \"https://www.sitemaps.org/terms.html\"\n    # and so on...\n\n    # import the metadata part of the package as a function\n    > metadatafunc <- py_run_string(\"from trafilatura.metadata import extract_metadata\")\n    > downloaded <- trafilatura$fetch_url(\"https://github.com/rstudio/reticulate\")\n    > metadatafunc$extract_metadata(downloaded)\n    $title\n    [1] \"rstudio/reticulate\"\n\n    $author\n    [1] \"Rstudio\"\n\n    $url\n    [1] \"https://github.com/rstudio/reticulate\"\n\n    $hostname\n    [1] \"github.com\"\n    # and so on...\n\n\nGoing further\n-------------\n\n- `Basic Text Processing in R <https://programminghistorian.org/en/lessons/basic-text-processing-in-r>`_\n- `Quanteda <https://quanteda.io>`_ is an R package for managing and analyzing text:\n   - `Quickstart <https://quanteda.io/articles/pkgdown/quickstart.html>`_\n   - `Quanteda tutorials <https://tutorials.quanteda.io/>`_\n   - `Advancing Text Mining with R and quanteda <https://www.r-bloggers.com/2019/10/advancing-text-mining-with-r-and-quanteda/>`_\n\n"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/usage-python.rst.txt",
    "html": "With Python\n===========\n\n.. meta::\n    :description lang=en:\n        This tutorial focuses on text extraction from web pages with Python code snippets.\n        Data mining with this library encompasses HTML parsing and language identification.\n\n\n\nThe Python programming language\n-------------------------------\n\nPython can be easy to pick up whether you're a first time programmer or you're experienced with other languages:\n\n-  Official `Python Tutorial <https://docs.python.org/3/tutorial/>`_\n-  `The Hitchhiker’s Guide to Python <https://docs.python-guide.org/>`_\n-  `Learn Python Programming Step by Step <https://www.techbeamers.com/python-tutorial-step-by-step/>`_\n-  `The Best Python Tutorials (freeCodeCamp) <https://www.freecodecamp.org/news/best-python-tutorial/>`_\n\n\n\nStep-by-step\n------------\n\nQuickstart\n^^^^^^^^^^\n\n.. code-block:: python\n\n    # load necessary components\n    >>> from trafilatura import fetch_url, extract\n\n    # download a web page\n    >>> url = 'https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/'\n    >>> downloaded = fetch_url(url)\n    >>> downloaded is None  # assuming the download was successful\n    False\n\n    # extract information from HTML\n    >>> result = extract(downloaded)\n    >>> print(result)\n    # newlines preserved, TXT output ...\n\nThe only required argument is the input document (here a downloaded HTML file), the rest is optional.\n\n.. note::\n    For a hands-on tutorial see also the Python Notebook `Trafilatura Overview <https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb>`_.\n\n\n\nFormats\n^^^^^^^\n\nDefault output is set to TXT (bare text) without metadata.\n\nThe following formats are available: bare text, text with Markdown formatting, CSV, JSON, XML, and XML following the guidelines of the Text Encoding Initiative (TEI).\n\n\n.. hint::\n    Combining TXT, CSV and JSON formats with certain structural elements (e.g. formatting or links) triggers output in TXT+Markdown format.\n\nThe variables from the example above can be used further:\n\n\n.. code-block:: python\n\n    # newlines preserved, TXT output\n    >>> extract(downloaded)\n\n    # TXT/Markdown output\n    >>> extract(downloaded, include_links=True)\n\n    # some formatting preserved in basic XML structure\n    >>> extract(downloaded, output_format='xml')\n\n    # source URL provided for inclusion in metadata\n    >>> extract(downloaded, output_format='xml', url=url)\n\n    # links preserved in XML, converting relative links to absolute where possible\n    >>> extract(downloaded, output_format='xml', include_links=True)\n\n    # source URL must be provided to convert relative links to absolute with TXT output\n    >>> extract(downloaded, include_links=True, url=url)\n\n\n\nChoice of HTML elements\n^^^^^^^^^^^^^^^^^^^^^^^\n\nSeveral elements can be included or discarded:\n\n* Text elements: comments, tables\n* Structural elements: formatting, images, links\n\nTheir inclusion can be activated or deactivated using paramaters passed to the ``extract()`` function:\n\n\n.. code-block:: python\n\n    # no comments in output\n    >>> result = extract(downloaded, include_comments=False)\n\n    # skip tables examination\n    >>> result = extract(downloaded, include_tables=False)\n\n    # output with links\n    >>> result = extract(downloaded, include_links=True)\n    # and so on...\n\n\n.. note::\n    Including extra elements works best with conversion to XML formats (``output_format=\"xml\"``) or ``bare_extraction()``. Both ways allow for direct display and manipulation of the elements. Certain elements are only visible in the output if the chosen format allows it (e.g. images and XML).\n\n\n``include_formatting=True``\n    Keep structural elements related to formatting (``<b>``/``<strong>``, ``<i>``/``<emph>`` etc.)\n``include_links=True``\n    Keep link targets (in ``href=\"...\"``)\n``include_images=True``\n    Keep track of images along with their targets (``<img>`` attributes: alt, src, title)\n``include_tables=True``\n    Extract text from HTML ``<table>`` elements.\n\n\nOnly ``include_tables`` is activated by default.\n\n\n.. hint::\n    If the output is buggy removing a constraint (e.g. formatting) can greatly improve the result.\n\n\nOptimizing for precision and recall\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe parameters ``favor_precision`` & ``favor_recall`` can be passed to the ``extract()`` & ``bare_extraction()`` functions:\n\n.. code-block:: python\n\n    >>> result = extract(downloaded, url, favor_precision=True)\n\nThey slightly affect processing and volume of textual output, respectively concerning precision/accuracy (i.e. more selective extraction, yielding less and more central elements) and recall (i.e. more opportunistic extraction, taking more elements into account).\n\n\n\nhtml2txt\n^^^^^^^^\n\nThis function emulates the behavior of similar functions in other packages, it is normally used as a last resort during extraction but can be called specifically in order to output all possible text:\n\n.. code-block:: python\n\n    >>> from trafilatura import html2txt\n    >>> html2txt(downloaded)\n\n\nLanguage identification\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe target language can also be set using 2-letter codes (`ISO 639-1 <https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes>`_), there will be no output if the detected language of the result does not match and no such filtering if the identification component has not been installed (see above `installation instructions <installation.html>`_) or if the target language is not available.\n\n.. code-block:: python\n\n    >>> result = extract(downloaded, url, target_language=\"de\")\n\n.. note::\n    Additional components are required: ``pip install trafilatura[all]``\n\n\nOptimizing for speed\n^^^^^^^^^^^^^^^^^^^^\n\nExecution speed not only depends on the platform and on supplementary packages (``trafilatura[all]``, ``htmldate[speed]``), but also on the extraction strategy.\n\nThe available fallbacks make extraction more precise but also slower. The use of fallback algorithms can also be bypassed in *fast* mode, which should make extraction about twice as fast:\n\n.. code-block:: python\n\n    # skip algorithms used as fallback\n    >>> result = extract(downloaded, no_fallback=True)\n\nThe following combination can lead to shorter processing times:\n\n.. code-block:: python\n\n    >>> result = extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\n\n\nContent hashing\n^^^^^^^^^^^^^^^\n\nFunctions used to build content hashes can be found in `hashing.py <https://github.com/adbar/trafilatura/blob/master/trafilatura/hashing.py>`_.\n\n\n.. code-block:: python\n\n    # create a filename-safe string by hashing the given content\n    >>> from trafilatura.hashing import generate_hash_filename\n    >>> generate_hash_filename(\"This is a text.\")\n    'qAgzZnskrcRgeftk'\n\n\nThe `SimHash <https://en.wikipedia.org/wiki/SimHash>`_ method (also called Charikar's hash) allows for near-duplicate detection. It implements a `locality-sensitive hashing <https://en.wikipedia.org/wiki/Locality-sensitive_hashing>`_ method based on a rolling hash and comparisons using the hamming distance. Overall it is reasonably fast and accurate for web texts and can be used to detect near duplicates by fixing a similarity threshold.\n\n\n.. code-block:: python\n\n    # create a Simhash for near-duplicate detection\n    >>> from trafilatura.hashing import Simhash\n    >>> first = Simhash(\"This is a text.\")\n    >>> second = Simhash(\"This is a test.\")\n    >>> second.similarity(first)\n    0.84375\n\n    # use existing Simhash\n    >>> first_copy = Simhash(existing_hash=first.hash)\n    >>> first_copy.similarity(first)\n    1.0\n\n\nExtraction settings\n-------------------\n\n.. hint::\n    See also `settings page <settings.html>`_.\n\n\nDisabling ``signal``\n^^^^^^^^^^^^^^^^^^^^\n\nA timeout exit during extraction can be turned off if malicious data are not an issue or if you run into an error like `signal only works in main thread <https://github.com/adbar/trafilatura/issues/202>`_. In this case, the following code can be useful as it explicitly changes the required setting:\n\n.. code-block:: python\n\n    >>> from trafilatura.settings import use_config\n    >>> newconfig = use_config()\n    >>> newconfig.set(\"DEFAULT\", \"EXTRACTION_TIMEOUT\", \"0\")\n    >>> extract(downloaded, config=newconfig)\n\n\nMetadata extraction\n^^^^^^^^^^^^^^^^^^^\n\nDate\n~~~~\n\nAmong metadata extraction, dates are handled by an external module: `htmldate <https://github.com/adbar/htmldate>`_. By default, focus is on original dates and the extraction replicates the *fast/no_fallback* option.\n\n`Custom parameters <https://htmldate.readthedocs.io/en/latest/corefunctions.html#handling-date-extraction>`_ can be passed through the extraction function or through the ``extract_metadata`` function in ``trafilatura.metadata``, most notably:\n\n-  ``extensive_search`` (boolean), to activate pattern-based opportunistic text search,\n-  ``original_date`` (boolean) to look for the original publication date,\n-  ``outputformat`` (string), to provide a custom datetime format,\n-  ``max_date`` (string), to set the latest acceptable date manually (YYYY-MM-DD format).\n\n.. code-block:: python\n\n    # import the extract() function, use a previously downloaded document\n    # pass the new parameters as dict\n    >>> extract(downloaded, output_format=\"xml\", date_extraction_params={\n            \"extensive_search\": True, \"max_date\": \"2018-07-01\"\n        })\n\n\nURL\n~~~\n\nEven if the page to process has already been downloaded it can still be useful to pass the URL as an argument. See this `previous bug <https://github.com/adbar/trafilatura/issues/75>`_ for an example:\n\n.. code-block:: python\n\n    # define a URL and download the example\n    >>> url = \"https://web.archive.org/web/20210613232513/https://www.thecanary.co/feature/2021/05/19/another-by-election-headache-is-incoming-for-keir-starmer/\"\n    >>> downloaded = fetch_url(url)\n\n    # content discarded since necessary metadata couldn't be extracted\n    >>> bare_extraction(downloaded, with_metadata=True)\n    >>>\n\n    # date found in URL, extraction successful\n    >>> bare_extraction(downloaded, with_metadata=True, url=url)\n\n\nMemory use\n^^^^^^^^^^\n\nTrafilatura uses caches to speed up extraction and cleaning processes. This may lead to memory leaks in some cases, particularly in large-scale applications. If that happens you can reset all cached information in order to release RAM:\n\n.. code-block:: python\n\n    >>> from trafilatura.meta import reset_caches\n\n    # at any given point\n    >>> reset_caches()\n\n\nInput/Output types\n------------------\n\nPython objects as output\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe extraction can be customized using a series of parameters, for more see the `core functions <corefunctions.html>`_ page.\n\nThe function ``bare_extraction`` can be used to bypass output conversion, it returns Python variables for  metadata (dictionary) as well as main text and comments (both LXML objects).\n\n.. code-block:: python\n\n    >>> from trafilatura import bare_extraction\n    >>> bare_extraction(downloaded)\n\n\nRaw HTTP response objects\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``fetch_url()`` function can pass a urllib3 response object straight to the extraction by setting the optional ``decode`` argument to ``False``.\n\nThis can be useful to get the final redirection URL with ``response.url`` and then pass is directly as a URL argument to the extraction function:\n\n.. code-block:: python\n\n    # necessary components\n    >>> from trafilatura import fetch_url, bare_extraction\n    # load an example\n    >>> response = fetch_url(\"https://www.example.org\", decode=False)\n    # perform extract() or bare_extraction() on Trafilatura's response object\n    >>> bare_extraction(response, url=response.url) # here is the redirection URL\n\n\nLXML objects\n^^^^^^^^^^^^\n\nThe input can consist of a previously parsed tree (i.e. a *lxml.html* object), which is then handled seamlessly:\n\n.. code-block:: python\n\n    # define document and load it with LXML\n    >>> from lxml import html\n    >>> my_doc = \"\"\"<html><body><article><p>\n                    Here is the main text.\n                    </p></article></body></html>\"\"\"\n    >>> mytree = html.fromstring(my_doc)\n    # extract from the already loaded LXML tree\n    >>> extract(mytree)\n    'Here is the main text.'\n\n\nNavigation\n----------\n\nFeeds\n^^^^^\n\n\nThe function ``find_feed_urls`` is a all-in-one utility that attemps to discover the feeds from a webpage if required and/or downloads and parses feeds. It returns the extracted links as list, more precisely as a sorted list of unique links.\n\n.. code-block:: python\n\n    # import the feeds module\n    >>> from trafilatura import feeds\n\n    # use the homepage to automatically retrieve feeds\n    >>> mylist = feeds.find_feed_urls('https://www.theguardian.com/')\n    >>> mylist\n    ['https://www.theguardian.com/international/rss', '...'] # and so on\n\n    # use a predetermined feed URL directly\n    >>> mylist = feeds.find_feed_urls('https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml')\n    >>> mylist is not []\n    True # it's not empty\n\n\n.. note::\n    The links are seamlessly filtered for patterns given by the user, e.g. using ``https://www.un.org/en/`` as argument implies taking all URLs corresponding to this category.\n\n\nAn optional argument ``target_lang`` makes it possible to filter links according to their expected target language. A series of heuristics are applied on the link path and parameters to try to discard unwanted URLs, thus saving processing time and download bandwidth.\n\n\n.. code-block:: python\n\n    # the feeds module has to be imported\n    # search for feeds in English\n    >>> mylist = feeds.find_feed_urls('https://www.un.org/en/rss.xml', target_lang='en')\n    >>> mylist is not []\n    True # links found as expected\n\n    # target_lang set to Japanese, the English links are discarded\n    >>> mylist = feeds.find_feed_urls('https://www.un.org/en/rss.xml', target_lang='ja')\n    >>> mylist\n    []\n\nFor more information about feeds and web crawling see:\n\n- This blog post: `Using RSS and Atom feeds to collect web pages with Python <https://adrien.barbaresi.eu/blog/using-feeds-text-extraction-python.html>`_\n- This Youtube tutorial: `Extracting links from ATOM and RSS feeds <https://www.youtube.com/watch?v=NW2ISdOx08M&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci&index=2&t=136s>`_\n\n\nSitemaps\n^^^^^^^^\n\n- Youtube tutorial: `Learn how to process XML sitemaps to extract all texts present on a website <https://www.youtube.com/watch?v=uWUyhxciTOs>`_\n\n.. code-block:: python\n\n    # load sitemaps module\n    >>> from trafilatura import sitemaps\n\n    # automatically find sitemaps by providing the homepage\n    >>> mylinks = sitemaps.sitemap_search('https://www.theguardian.com/')\n\n    # the target_lang argument works as explained above\n    >>> mylinks = sitemaps.sitemap_search('https://www.un.org/', target_lang='en')\n\nThe links are also seamlessly filtered for patterns given by the user, e.g. using ``https://www.theguardian.com/society`` as argument implies taking all URLs corresponding to the society category.\n"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/usage-cli.rst.txt",
    "html": "On the command-line\n===================\n\n.. meta::\n    :description lang=en:\n        This tutorial focuses on text extraction from HTML web pages without writing code.\n        Bulk parallel processing and data mining are also described.\n\n\nIntroduction\n------------\n\nTrafilatura includes a `command-line interface <https://en.wikipedia.org/wiki/Command-line_interface>`_ and can be conveniently used without writing code.\n\nFor the very first steps please refer to this multilingual, step-by-step `Introduction to the command-line interface <https://tutorial.djangogirls.org/en/intro_to_command_line/>`_ and this `section of the Introduction to Cultural Analytics & Python <https://melaniewalsh.github.io/Intro-Cultural-Analytics/01-Command-Line/01-The-Command-Line.html>`_.\n\nFor instructions related to specific platforms see:\n\n- `Comment Prompt <https://www.lifewire.com/how-to-open-command-prompt-2618089>`_ (tutorial for Windows systems)\n- `Introduction to the Windows Command Line with PowerShell <https://programminghistorian.org/en/lessons/intro-to-powershell>`_\n- `How to use the Terminal command line in macOS <https://macpaw.com/how-to/use-terminal-on-mac>`_\n- or `An introduction to the Linux Terminal <https://www.digitalocean.com/community/tutorials/an-introduction-to-the-linux-terminal>`_\n\nAs well as these compendia:\n\n- `Introduction to the Bash Command Line <https://programminghistorian.org/en/lessons/intro-to-bash>`_ (The Programming Historian)\n- `Basic Bash Command Line Tips You Should Know <https://www.freecodecamp.org/news/basic-linux-commands-bash-tips-you-should-know/>`_ (freeCodeCamp)\n\n\nQuickstart\n----------\n\nURLs can be used directly (``-u/--URL``):\n\n.. code-block:: bash\n\n    # outputs main content and comments as plain text ...\n    $ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n    # outputs main text with basic XML structure ...\n    $ trafilatura --xml --URL \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n    # displays help message\n    $ trafilatura -h\n\n\nYou can also pipe a HTML document (and response body) to trafilatura:\n\n.. code-block:: bash\n\n    # use the contents of an already existing file\n    $ cat myfile.html | trafilatura\n\n    # alternative syntax\n    $ < myfile.html trafilatura\n\n    # use a custom download utility and pipe it to trafilatura\n    $ wget -qO- \"https://de.creativecommons.org/index.php/was-ist-cc/\" | trafilatura\n\n\nExtraction parameters\n---------------------\n\n\nChoice of HTML elements\n~~~~~~~~~~~~~~~~~~~~~~~\n\nSeveral elements can be included or discarded (see list of options below):\n\n* Text elements: comments, tables\n* Structural elements: formatting, images, links\n\nOnly comments and text extracted from HTML ``<table>`` elements are extracted by default, ``--no-comments`` and ``--no-tables`` deactivate this setting.\n\nFurther options:\n\n``--formatting``\n    Keep structural elements related to formatting (``<b>``/``<strong>``, ``<i>``/``<emph>`` etc.)\n``--links``\n    Keep link targets (in ``href=\"...\"``), converting relative URLs to absolute where possible\n``--images``\n    Keep track of images along with their targets (``<img>`` attributes: alt, src, title)\n\n.. note::\n    Certain elements are only visible in the output if the chosen format allows it (e.g. images and XML).\n    \n    Including extra elements works best with conversion to XML/XML-TEI. If the output is buggy removing a constraint (e.g. formatting) can greatly improve the result.\n\n\nOutput format\n~~~~~~~~~~~~~\n\nOutput as TXT without metadata is the default, another format can be selected in two different ways:\n\n-  ``--csv``, ``--json``, ``--xml`` or ``--xmltei``\n-  ``-out`` or ``--output-format`` {txt,csv,json,xml,xmltei}\n\n.. hint::\n    Combining TXT, CSV and JSON formats with certain structural elements (e.g. formatting or links) triggers output in TXT+Markdown format.\n\n\nOptimizing for precision and recall\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe arguments ``--precision`` & ``--recall`` can be passed to the extractor.\n\nThey slightly affect processing and volume of textual output, respectively concerning precision/accuracy (i.e. more selective extraction, yielding less and more central elements) and recall (i.e. more opportunistic extraction, taking more elements into account).\n\n\nLanguage identification\n~~~~~~~~~~~~~~~~~~~~~~~\n\nPassing the argument ``--target-language`` along with a 2-letter code (`ISO 639-1 <https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes>`_) will trigger language filtering of the output if the identification component has been `installed <installation.html>`_ and if the target language is available.\n\n.. note::\n    Additional components are required: ``pip install trafilatura[all]``\n\n\n\nChanging default settings\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSee `documentation page on settings <settings.html>`_.\n\n\n\nProcess files locally\n---------------------\n\nIn case web pages have already been downloaded and stored, it is possible to process single files or directories as a whole. It can be especially helpful to separate download and extraction to circumvent blocking mechanisms, either by scrambling IPs used to access the pages or by using web browser automation software to bypass issues related to cookies and paywalls.\n\nTrafilatura will work as well provided web pages (HTML documents) are used as input. Two major command line arguments are necessary:\n\n-  ``--input-dir`` to select a directory to read files from\n-  ``-o`` or ``--output-dir`` to define a directory to eventually store the results\n\n\n.. note::\n    In case no directory is selected, results are printed to standard output (*STDOUT*, e.g. in the terminal window).\n\n\n\nProcess a list of links\n-----------------------\n\n.. note::\n    Beware that there should be a tacit scraping etiquette and that a server may block you after the download of a certain number of pages from the same website/domain in a short period of time.\n\n    In addition, some websites may block the ``requests`` `user-agent <https://en.wikipedia.org/wiki/User_agent>`_. Thus, *trafilatura* waits a few seconds per default between requests.\n\n    For more information see the `page on downloads <downloads.html>`_.\n\n\nTwo major command line arguments are necessary here:\n\n-  ``-i`` or ``--input-file`` to select an input list to read links from.\n\n   This option allows for bulk download and processing of a list of URLs from a file listing one link per line. The input list will be read sequentially, only lines beginning with a valid URL will be read, the file can thus contain other information which will be discarded.\n\n-  ``-o`` or ``--output-dir`` to define a directory to eventually store the results.\n\n   The output directory can be created on demand, but it must be writable.\n\n\n.. code-block:: bash\n\n    $ trafilatura -i list.txt -o txtfiles/\t\t# output as raw text\n    $ trafilatura --xml -i list.txt -o xmlfiles/\t# output in XML format\n\n\n.. hint::\n    Backup of HTML sources can be useful for archival and further processing:\n    \n    ``$ trafilatura --input-file links.txt --output-dir converted/ --backup-dir html-sources/ --xml``\n\n\nInternet Archive\n~~~~~~~~~~~~~~~~\n\nUsing the option ``--archived`` will trigger queries to the `Internet Archive <https://web.archive.org/>`_ for web pages which could not be downloaded.\n\nThere is a fair chance to find archived versions for larger websites, whereas pages of lesser-known websites may not have been preserved there. The retrieval process is slow as it depends on a single web portal only, it is best performed for a relatively small number of URLs.\n\n\nLink discovery\n--------------\n\nLink discovery can be performed over `web feeds <https://en.wikipedia.org/wiki/Web_feed>`_ (Atom and RSS) or `sitemaps <https://en.wikipedia.org/wiki/Sitemaps>`_.\n\nBoth homepages and particular sitemaps or feed URLs can be used as input.\n\nThe ``--list`` option is useful to list URLs prior to processing. This option can be combined with an input file (``-i``) containing a list of sources which will then be processed in parallel.\n\nFor more information please refer to the `tutorial on content discovery <tutorial0.html#content-discovery>`_.\n\nFeeds\n~~~~~\n\n.. code-block:: bash\n\n    # automatically detecting feeds starting from the homepage\n    $ trafilatura --feed \"https://www.dwds.de/\" --list\n\n    # already known feed\n    $ trafilatura --feed \"https://www.dwds.de/api/feed/themenglossar/Corona\" --list\n\n    # processing a list in parallel\n    $ trafilatura -i mylist.txt --feed --list\n\n\n.. raw:: html\n\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/NW2ISdOx08M?start=406\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\nYoutube tutorial: `Extracting links from web feeds <https://www.youtube.com/watch?v=NW2ISdOx08M&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci&index=2&t=398s>`_\n\n\nSitemaps\n~~~~~~~~\n\n.. code-block:: bash\n\n    # run link discovery through a sitemap for sitemaps.org and store the resulting links in a file\n    $ trafilatura --sitemap \"https://www.sitemaps.org/\" --list > mylinks.txt\n\n    # using an already known sitemap URL\n    $ trafilatura --sitemap \"https://www.sitemaps.org/sitemap.xml\" --list\n\n    # targeting webpages in German\n    $ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --target-language \"de\"\n\n\nFor more information on sitemap use and filters for lists of links see this blog post: `Using sitemaps to crawl websites <https://adrien.barbaresi.eu/blog/using-sitemaps-crawl-websites.html>`_.\n\n\n.. raw:: html\n\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/uWUyhxciTOs?start=330\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\nYoutube tutorial: `Listing all website contents with sitemaps <https://www.youtube.com/watch?v=uWUyhxciTOs&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci&index=3&t=330s>`_\n\n\nURL inspection prior to download and processing\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n.. code-block:: bash\n\n    $ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"https://www.sitemaps.org/de\"\n    $ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"protocol\"\n\nUsing a subpart of the site also acts like a filter, for example ``--sitemap \"https://www.sitemaps.org/de/\"``.\n\nFor more information on sitemap use and filters for lists of links see this blog post: `Using sitemaps to crawl websites <https://adrien.barbaresi.eu/blog/using-sitemaps-crawl-websites.html>`_ and this `tutorial on link filtering <tutorial0.html#link-filtering>`_.\n\n\nFurther information\n-------------------\n\n\n.. hint::\n    See also `how to modify the default settings <settings.html>`_.\n\n\nFor all usage instructions see ``trafilatura -h``:\n\n.. code-block:: bash\n\n    trafilatura [-h] [-i INPUTFILE | --input-dir INPUTDIR | -u URL]\n                   [--parallel PARALLEL] [-b BLACKLIST] [--list]\n                   [-o OUTPUTDIR] [--backup-dir BACKUP_DIR] [--keep-dirs]\n                   [--hash-as-name] [--feed [FEED] | --sitemap [SITEMAP] |\n                   --crawl [CRAWL] | --explore [EXPLORE]] [--archived]\n                   [--url-filter URL_FILTER [URL_FILTER ...]] [-f]\n                   [--formatting] [--links] [--images] [--no-comments]\n                   [--no-tables] [--only-with-metadata]\n                   [--target-language TARGET_LANGUAGE] [--deduplicate]\n                   [--config-file CONFIG_FILE]\n                   [-out {txt,csv,json,xml,xmltei} | --csv | --json | --xml | --xmltei]\n                   [--validate-tei] [-v] [--version]\n\n\nCommand-line interface for Trafilatura\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --verbose         increase logging verbosity (-v or -vv)\n  --version             show version information and exit\n\nInput:\n  URLs, files or directories to process\n\n  -i INPUTFILE, --input-file INPUTFILE\n                        name of input file for batch processing\n  --input-dir INPUTDIR   read files from a specified directory (relative path)\n  -u URL, --URL URL     custom URL download\n  --parallel PARALLEL   specify a number of cores/threads for downloads and/or\n                        processing\n  -b BLACKLIST, --blacklist BLACKLIST\n                        file containing unwanted URLs to discard during\n                        processing\n\nOutput:\n  Determines if and how files will be written\n\n  --list                display a list of URLs without downloading them\n  -o OUTPUTDIR, --output-dir OUTPUTDIR\n                        write results in a specified directory (relative path)\n  --backup-dir BACKUP_DIR\n                        preserve a copy of downloaded files in a backup\n                        directory\n  --keep-dirs           keep input directory structure and file names\n  --hash-as-name        use hash value as output file name instead of random\n                        default\n\nNavigation:\n  Link discovery and web crawling\n\n  --feed URL            look for feeds and/or pass a feed URL as input\n  --sitemap URL         look for sitemaps for the given website and/or enter a\n                        sitemap URL\n  --crawl URL           crawl a fixed number of pages within a website\n                        starting from the given URL\n  --explore URL         explore the given websites (combination of sitemap and\n                        crawl)\n  --archived            try to fetch URLs from the Internet Archive if\n                        downloads fail\n  --url-filter URL_FILTER\n                        only process/output URLs containing these patterns\n                        (space-separated strings)\n\nExtraction:\n  Customization of text and metadata processing\n\n  -f, --fast            fast (without fallback detection)\n  --formatting          include text formatting (bold, italic, etc.)\n  --links               include links along with their targets (experimental)\n  --images              include image sources in output (experimental)\n  --no-comments         don't output any comments\n  --no-tables           don't output any table elements\n  --only-with-metadata  only output those documents with title, URL and date\n                        (for formats supporting metadata)\n  --target-language TARGET_LANGUAGE\n                        select a target language (ISO 639-1 codes)\n  --deduplicate         filter out duplicate documents and sections\n  --config-file CONFIG_FILE\n                        override standard extraction parameters with a custom\n                        config file\n  --precision           favor extraction precision (less noise, possibly less\n                        text)\n  --recall              favor extraction recall (more text, possibly more\n                        noise)\n\n\nFormat:\n  Selection of the output format\n\n  -out, --output-format\n                        determine output format, possible choices:\n                        txt, csv, json, xml, xmltei\n  --csv                 CSV output\n  --json                JSON output\n  --xml                 XML output\n  --xmltei              XML TEI output\n  --validate-tei        validate XML TEI output\n\n"
  },
  {
    "title": "trafilatura.xml — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/xml.html#xmltotxt",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.xml\nSource code for trafilatura.xml\n# pylint:disable-msg=E0611,I1101\n\"\"\"\nAll functions related to XML generation, processing and validation.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\nimport logging\nimport lzma\nfrom html import unescape\nfrom json import dumps as json_dumps\nfrom pathlib import Path\nfrom pickle import load as load_pickle\n\nfrom lxml.etree import (Element, RelaxNG, SubElement, XMLParser, fromstring,\n                        tostring)\n\nfrom . import __version__\nfrom .filters import text_chars_test\nfrom .utils import sanitize\n\nLOGGER = logging.getLogger(__name__)\n# validation\nTEI_SCHEMA = str(Path(__file__).parent / 'data/tei-schema-pickle.lzma')\nTEI_VALID_TAGS = {'ab', 'body', 'cell', 'code', 'del', 'div', 'graphic', 'head', 'hi', \\\n                  'item', 'lb', 'list', 'p', 'quote', 'ref', 'row', 'table'}\nTEI_VALID_ATTRS = {'rend', 'rendition', 'role', 'target', 'type'}\nTEI_RELAXNG = None  # to be downloaded later if necessary\nTEI_REMOVE_TAIL = {\"ab\", \"p\"}\n\nCONTROL_PARSER = XMLParser(remove_blank_text=True)\n\nNEWLINE_ELEMS = {\n    'cell': '|',\n    'item': '\\n- ',\n    **{tag: '\\n' for tag in ['code', 'graphic', 'head', 'lb', 'list', 'p', 'quote', 'row', 'table']}\n}\nSPECIAL_FORMATTING = {'del', 'head', 'hi'}\nWITH_ATTRIBUTES = {'cell', 'del', 'graphic', 'head', 'hi', 'item', 'list', 'ref'}\n\nNESTING_WHITELIST = {\"cell\", \"figure\", \"item\", \"note\", \"quote\"}\n\nMETA_ATTRIBUTES = [\n    'sitename', 'title', 'author', 'date', 'url', 'hostname',\n    'description', 'categories', 'tags', 'license', 'id',\n    'fingerprint', 'language'\n]\n\nHI_FORMATTING = {'#b': '**', '#i': '*', '#u': '__', '#t': '`'}\n\n\ndef build_json_output(docmeta):\n    '''Build JSON output based on extracted information'''\n    outputdict = {slot: getattr(docmeta, slot, None) for slot in docmeta.__slots__}\n    outputdict.update({\n        'source': outputdict.pop('url'),\n        'source-hostname': outputdict.pop('sitename'),\n        'excerpt': outputdict.pop('description'),\n        'categories': ';'.join(outputdict.pop('categories')),\n        'tags': ';'.join(outputdict.pop('tags')),\n        'text': xmltotxt(outputdict.pop('body'), include_formatting=False),\n    })\n\n    commentsbody = outputdict.pop('commentsbody')\n    if commentsbody is not None:\n        outputdict['comments'] = xmltotxt(commentsbody, include_formatting=False)\n\n    return json_dumps(outputdict, ensure_ascii=False)\n\n\ndef clean_attributes(tree):\n    '''Remove unnecessary attributes.'''\n    for elem in tree.iter('*'):\n        if elem.tag not in WITH_ATTRIBUTES:\n            elem.attrib.clear()\n    return tree\n\n\ndef remove_empty_elements(tree):\n    '''Remove text elements without text.'''\n    for element in tree.iter('*'):  # 'head', 'hi', 'item', 'p'\n        if len(element) == 0 and text_chars_test(element.text) is False and text_chars_test(element.tail) is False:\n            parent = element.getparent()\n            # not root element or element which is naturally empty\n            # do not remove elements inside <code> to preserve formatting\n            if parent is not None and element.tag != \"graphic\" and parent.tag != 'code':\n                element.getparent().remove(element)\n    return tree\n\n\ndef strip_double_tags(tree):\n    \"Prevent nested tags among a fixed list of tags.\"\n    for elem in reversed(tree.xpath(\".//head | .//code | .//p\")):\n        for subelem in elem.iterdescendants(\"code\", \"head\", \"p\"):\n            if subelem.getparent().tag in NESTING_WHITELIST:\n                continue\n            if subelem.tag == elem.tag:\n                merge_with_parent(subelem)\n    return tree\n\n\ndef build_xml_output(docmeta):\n    '''Build XML output tree based on extracted information'''\n    output = Element('doc')\n    output = add_xml_meta(output, docmeta)\n    docmeta.body.tag = 'main'\n    # clean XML tree\n    output.append(clean_attributes(docmeta.body))\n    if docmeta.commentsbody is not None:\n        docmeta.commentsbody.tag = 'comments'\n        output.append(clean_attributes(docmeta.commentsbody))\n# XML invalid characters\n# https://chase-seibert.github.io/blog/2011/05/20/stripping-control-characters-in-python.html\n    return output\n\n\ndef control_xml_output(output_tree, output_format, tei_validation, docmeta):\n    '''Make sure the XML output is conform and valid if required'''\n    control_string = sanitize(tostring(output_tree, encoding='unicode'))\n    # necessary for cleaning\n    output_tree = fromstring(control_string, CONTROL_PARSER)\n    # validate\n    if output_format == 'xmltei' and tei_validation is True:\n        result = validate_tei(output_tree)\n        LOGGER.debug('TEI validation result: %s %s %s', result, docmeta.id, docmeta.url)\n    return tostring(output_tree, pretty_print=True, encoding='unicode').strip()\n\n\ndef add_xml_meta(output, docmeta):\n    '''Add extracted metadata to the XML output tree'''\n    for attribute in META_ATTRIBUTES:\n        value = getattr(docmeta, attribute, None)\n        if value is not None:\n            output.set(attribute, value if isinstance(value, str) else ';'.join(value))\n    return output\n\n\ndef build_tei_output(docmeta):\n    '''Build TEI-XML output tree based on extracted information'''\n    # build TEI tree\n    output = write_teitree(docmeta)\n    # filter output (strip unwanted elements), just in case\n    # check and repair\n    output = check_tei(output, docmeta.url)\n    return output\n\n\ndef check_tei(xmldoc, url):\n    '''Check if the resulting XML file is conform and scrub remaining tags'''\n    # convert head tags\n    for elem in xmldoc.iter('head'):\n        elem.tag = 'ab'\n        elem.set('type', 'header')\n        parent = elem.getparent()\n        if len(elem) > 0:\n            new_elem = _tei_handle_complex_head(elem)\n            parent.replace(elem, new_elem)\n            elem = new_elem\n        if parent.tag == \"p\":\n            _move_element_one_level_up(elem)\n    # convert <lb/> when child of <div> to <p>\n    for element in xmldoc.findall(\".//text/body//div/lb\"):\n        if element.tail and element.tail.strip():\n            element.tag = 'p'\n            element.text = element.tail\n            element.tail = None\n    # look for elements that are not valid\n    for element in xmldoc.findall('.//text/body//*'):\n        if element.tag in TEI_REMOVE_TAIL and element.tail and element.tail.strip():\n            _handle_unwanted_tails(element)\n        # check elements\n        if element.tag not in TEI_VALID_TAGS:\n            # disable warnings for chosen categories\n            # if element.tag not in ('div', 'span'):\n            LOGGER.warning('not a TEI element, removing: %s %s', element.tag, url)\n            merge_with_parent(element)\n            continue\n        if element.tag == \"div\":\n            _handle_text_content_of_div_nodes(element)\n            _wrap_unwanted_siblings_of_div(element)\n        # check attributes\n        for attribute in element.attrib:\n            if attribute not in TEI_VALID_ATTRS:\n                LOGGER.warning('not a valid TEI attribute, removing: %s in %s %s', attribute, element.tag, url)\n                element.attrib.pop(attribute)\n    return xmldoc\n\n\n\n\n\n[docs]\n\ndef validate_tei(xmldoc):  # , filename=\"\"\n    '''Check if an XML document is conform to the guidelines of the Text Encoding Initiative'''\n    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        # load validator\n        with lzma.open(TEI_SCHEMA, 'rb') as schemafile:\n            schema_data = load_pickle(schemafile)\n        TEI_RELAXNG = RelaxNG(fromstring(schema_data))\n    result = TEI_RELAXNG.validate(xmldoc)\n    if result is False:\n        LOGGER.warning('not a valid TEI document: %s', TEI_RELAXNG.error_log.last_error)\n    return result\n\n\n\n\ndef replace_element_text(element, include_formatting):\n    '''Determine element text based on text and tail'''\n    # handle formatting: convert to markdown\n    if include_formatting is True and element.text is not None:\n        if element.tag in ('del', 'head'):\n            if element.tag == 'head':\n                try:\n                    number = int(element.get('rend')[1])\n                except (TypeError, ValueError):\n                    number = 2\n                element.text = f'{\"#\" * number} {element.text}'\n            elif element.tag == 'del':\n                element.text = f'~~{element.text}~~'\n        elif element.tag == 'hi':\n            rend = element.get('rend')\n            if rend in HI_FORMATTING:\n                element.text = f'{HI_FORMATTING[rend]}{element.text}{HI_FORMATTING[rend]}'\n    # handle links\n    if element.tag == 'ref':\n        if element.text is not None:\n            link_text = f'[{element.text}]'\n            if element.get('target') is not None:\n                element.text = f\"{link_text}({element.get('target')})\"\n            else:\n                LOGGER.warning('missing link attribute: %s %s', element.text, element.attrib)\n                element.text = link_text\n        else:\n            LOGGER.warning('empty link: %s %s', element.text, element.attrib)\n    # handle text\n    return (element.text or '') + (element.tail or '')\n\n\ndef merge_with_parent(element, include_formatting=False):\n    '''Merge element with its parent and convert formatting to markdown.'''\n    parent = element.getparent()\n    if not parent:\n        return\n\n    full_text = replace_element_text(element, include_formatting)\n\n    previous = element.getprevious()\n    if previous is not None:\n        # There is a previous node, append text to its tail\n        if previous.tail is not None:\n            previous.tail = f'{previous.tail} {full_text}'\n        else:\n            previous.tail = full_text\n    elif parent.text is not None:\n        parent.text = f'{parent.text} {full_text}'\n    else:\n        parent.text = full_text\n    parent.remove(element)\n\n\n\n\n\n[docs]\n\ndef xmltotxt(xmloutput, include_formatting):\n    '''Convert to plain text format and optionally preserve formatting as markdown.'''\n    returnlist = []\n    # strip_tags(xmloutput, 'div', 'main', 'span')\n    # iterate and convert to list of strings\n    for element in xmloutput.iter('*'):\n        if element.text is None and element.tail is None:\n            if element.tag == 'graphic':\n                # add source, default to ''\n                text = f'{element.get(\"title\", \"\")} {element.get(\"alt\", \"\")}'\n                returnlist.extend(['![', text.strip(), ']', '(', element.get('src', ''), ')'])\n            # newlines for textless elements\n            if element.tag in ('graphic', 'row', 'table'):\n                returnlist.append('\\n')\n            continue\n        # process text\n        textelement = replace_element_text(element, include_formatting)\n        # common elements\n        if element.tag in NEWLINE_ELEMS:\n            returnlist.extend([NEWLINE_ELEMS[element.tag], textelement, '\\n'])\n        elif element.tag == 'comments':\n            returnlist.append('\\n\\n')\n        else:\n            if element.tag not in SPECIAL_FORMATTING:\n                LOGGER.debug('unprocessed element in output: %s', element.tag)\n            returnlist.extend([textelement, ' '])\n    return unescape(sanitize(''.join(returnlist)))\n\n\n\n\ndef write_teitree(docmeta):\n    '''Bundle the extracted post and comments into a TEI tree'''\n    teidoc = Element('TEI', xmlns='http://www.tei-c.org/ns/1.0')\n    header = write_fullheader(teidoc, docmeta)\n    textelem = SubElement(teidoc, 'text')\n    textbody = SubElement(textelem, 'body')\n    # post\n    postbody = clean_attributes(docmeta.body)\n    postbody.tag = 'div'\n    postbody.set('type', 'entry') # rendition='#pst'\n    textbody.append(postbody)\n    # comments\n    if docmeta.commentsbody is not None:\n        commentsbody = clean_attributes(docmeta.commentsbody)\n        commentsbody.tag = 'div'\n        commentsbody.set('type', 'comments') # rendition='#cmt'\n        textbody.append(commentsbody)\n    return teidoc\n\n\ndef _define_publisher_string(docmeta):\n    '''Construct a publisher string to include in TEI header'''\n    if docmeta.hostname and docmeta.sitename:\n        publisherstring = f'{docmeta.sitename.strip()} ({docmeta.hostname})'\n    elif docmeta.hostname:\n        publisherstring = docmeta.hostname\n    elif docmeta.sitename:\n        publisherstring = docmeta.sitename\n    else:\n        LOGGER.warning('no publisher for URL %s', docmeta.url)\n        publisherstring = 'N/A'\n    return publisherstring\n\n\ndef write_fullheader(teidoc, docmeta):\n    '''Write TEI header based on gathered metadata'''\n    # todo: add language info\n    header = SubElement(teidoc, 'teiHeader')\n    filedesc = SubElement(header, 'fileDesc')\n    bib_titlestmt = SubElement(filedesc, 'titleStmt')\n    bib_titlemain = SubElement(bib_titlestmt, 'title', type='main')\n    bib_titlemain.text = docmeta.title\n    if docmeta.author:\n        bib_author = SubElement(bib_titlestmt, 'author')\n        bib_author.text = docmeta.author\n    publicationstmt_a = SubElement(filedesc, 'publicationStmt')\n    publisher_string = _define_publisher_string(docmeta)\n    # license, if applicable\n    if docmeta.license:\n        publicationstmt_publisher = SubElement(publicationstmt_a, 'publisher')\n        publicationstmt_publisher.text = publisher_string\n        availability = SubElement(publicationstmt_a, 'availability')\n        avail_p = SubElement(availability, 'p')\n        avail_p.text = docmeta.license\n    # insert an empty paragraph for conformity\n    else:\n        publicationstmt_p = SubElement(publicationstmt_a, 'p')\n    notesstmt = SubElement(filedesc, 'notesStmt')\n    if docmeta.id:\n        idno = SubElement(notesstmt, 'note', type='id')\n        idno.text = docmeta.id\n    fingerprint = SubElement(notesstmt, 'note', type='fingerprint')\n    fingerprint.text = docmeta.fingerprint\n    sourcedesc = SubElement(filedesc, 'sourceDesc')\n    source_bibl = SubElement(sourcedesc, 'bibl')\n    # determination of sigle string\n    if docmeta.sitename and docmeta.date:\n        sigle = docmeta.sitename + ', ' + docmeta.date\n    elif not docmeta.sitename and docmeta.date:\n        sigle = docmeta.date\n    elif docmeta.sitename:\n        sigle = docmeta.sitename\n    else:\n        LOGGER.warning('no sigle for URL %s', docmeta.url)\n        sigle = ''\n    if docmeta.title:\n        source_bibl.text = docmeta.title + '. ' + sigle\n    else:\n        source_bibl.text = '. ' + sigle\n    source_sigle = SubElement(sourcedesc, 'bibl', type='sigle')\n    source_sigle.text = sigle\n    biblfull = SubElement(sourcedesc, 'biblFull')\n    bib_titlestmt = SubElement(biblfull, 'titleStmt')\n    bib_titlemain = SubElement(bib_titlestmt, 'title', type='main')\n    bib_titlemain.text = docmeta.title\n    if docmeta.author:\n        bib_author = SubElement(bib_titlestmt, 'author')\n        bib_author.text = docmeta.author\n    publicationstmt = SubElement(biblfull, 'publicationStmt')\n    publication_publisher = SubElement(publicationstmt, 'publisher')\n    publication_publisher.text = publisher_string\n    if docmeta.url:\n        publication_url = SubElement(publicationstmt, 'ptr', type='URL', target=docmeta.url)\n    publication_date = SubElement(publicationstmt, 'date')\n    publication_date.text = docmeta.date\n    profiledesc = SubElement(header, 'profileDesc')\n    abstract = SubElement(profiledesc, 'abstract')\n    abstract_p = SubElement(abstract, 'p')\n    abstract_p.text = docmeta.description\n    if len(docmeta.categories) > 0 or len(docmeta.tags) > 0:\n        textclass = SubElement(profiledesc, 'textClass')\n        keywords = SubElement(textclass, 'keywords')\n        if len(docmeta.categories) > 0:\n            cat_list = SubElement(keywords, 'term', type='categories')\n            cat_list.text = ','.join(docmeta.categories)\n        if len(docmeta.tags) > 0:\n            tags_list = SubElement(keywords, 'term', type='tags')\n            tags_list.text = ','.join(docmeta.tags)\n    encodingdesc = SubElement(header, 'encodingDesc')\n    appinfo = SubElement(encodingdesc, 'appInfo')\n    application = SubElement(appinfo, 'application', version=__version__, ident='Trafilatura')\n    label = SubElement(application, 'label')\n    label.text = 'Trafilatura'\n    pointer = SubElement(application, 'ptr', target='https://github.com/adbar/trafilatura')\n    return header\n\n\ndef _handle_text_content_of_div_nodes(element):\n    if element.text and element.text.strip():\n        if element.getchildren() and element[0].tag == 'p':\n            p_text = element[0].text or \"\"\n            element[0].text = f'{element.text} {p_text}'.strip()\n        else:\n            new_child = Element(\"p\")\n            new_child.text = element.text\n            element.insert(0, new_child)\n        element.text = None\n\n    if element.tail and element.tail.strip():\n        if element.getchildren() and element[-1].tag == 'p':\n            p_text = element[-1].text or \"\"\n            element[-1].text = f'{p_text} {element.tail}'.strip()\n        else:\n            new_child = Element(\"p\")\n            new_child.text = element.tail\n            element.append(new_child)\n        element.tail = None\n\n\ndef _handle_unwanted_tails(element):\n    \"Handle tail on p and ab elements\"\n    if element.tag == 'p':\n        if element.text:\n            element.text += ' ' + element.tail.strip()\n        else:\n            element.text = element.tail\n    else:\n        new_sibling = Element('p')\n        new_sibling.text = element.tail.strip()\n        parent = element.getparent()\n        parent.insert(parent.index(element) + 1 , new_sibling)\n    element.tail = None\n\n\ndef _tei_handle_complex_head(element):\n    new_element = Element('ab', attrib=element.attrib)\n    new_element.text = element.text.strip() if element.text is not None else None\n    for child in element.iterchildren():\n        if child.tag == 'p':\n            if len(new_element) > 0 or new_element.text:\n                # add <lb> if <ab> has no children or last tail contains text\n                if len(new_element) == 0 or new_element[-1].tail:\n                    SubElement(new_element, 'lb')\n                new_element[-1].tail = child.text\n            else:\n                new_element.text = child.text\n        else:\n            new_element.append(child)\n    if element.tail is not None and element.tail.strip():\n        new_element.tail = element.tail.strip()\n    return new_element\n\n\ndef _wrap_unwanted_siblings_of_div(div_element):\n    new_sibling = Element(\"div\")\n    new_sibling_index = None\n    parent = div_element.getparent()\n    # check siblings after target element\n    for sibling in div_element.itersiblings():\n        if sibling.tag == \"div\":\n            break\n        if sibling.tag in {\"p\", \"list\", \"table\", \"quote\", \"ab\"}:\n            if new_sibling_index is None:\n                new_sibling_index = parent.index(sibling)\n            new_sibling.append(sibling)\n        # some elements (e.g. <lb/>) can appear next to div, but\n        # order of elements should be kept, thus add and reset new_sibling\n        else:\n            if new_sibling_index is not None and len(new_sibling) != 0:\n                parent.insert(new_sibling_index, new_sibling)\n                new_sibling = Element(\"div\")\n                new_sibling_index = None\n    if new_sibling_index is not None and len(new_sibling) != 0:\n        parent.insert(new_sibling_index, new_sibling)\n\n\ndef _move_element_one_level_up(element):\n    parent = element.getparent()\n    new_elem = Element(\"p\")\n    new_elem.extend(sibling for sibling in element.itersiblings())\n\n    parent.addnext(element)\n\n    if element.tail is not None and element.tail.strip():\n        new_elem.text = element.tail.strip()\n        element.tail = None\n    if len(new_elem) != 0 or new_elem.text:\n        element.addnext(new_elem)\n    if len(parent) == 0 and parent.text is None:\n        parent.getparent().remove(parent)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/used-by.rst.txt",
    "html": "Uses & citations\n================\n\n.. meta::\n    :description lang=en:\n        Trafilatura is used at several institutions, included in other software packages and cited in research publications. This page lists projects and publications mentioning the library.\n\n\nTrafilatura is used at several institutions, included in other software packages and cited in research publications, especially in linguistics and natural language processing, social science, information science, and in the context of large language models. This page lists projects and publications mentioning the library.\n\nTo add further references, please `edit this page <https://github.com/adbar/trafilatura/edit/master/docs/used-by.rst>`_ and suggest changes.\n\n\n\nNotable projects using this software\n------------------------------------\n\nKnown institutional users\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n- `Data against Feminicide <https://datoscontrafeminicidio.net/>`_\n- `Kagi search engine <https://kagi.com/>`_ (notably Teclis component)\n- `Media Cloud platform <https://mediacloud.org>`_ for media analysis\n- The Internet Archive's `sandcrawler <https://github.com/internetarchive/sandcrawler>`_ which crawls and processes the scholarly web for the `Fatcat catalog <https://fatcat.wiki/>`_ of research publications\n- `SciencesPo médialab <https://medialab.sciencespo.fr>`_ through its `Minet <https://github.com/medialab/minet>`_ webmining package\n\n\nVarious software repositories\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n- `Benson <https://github.com/timoteostewart/benson>`_, to turn a list of URLs into mp3s of the contents of each web page\n- `CommonCrawl downloader <https://github.com/leogao2/commoncrawl_downloader>`_, to derive massive amounts of language data\n- `GLAM Workbench <https://glam-workbench.github.io/web-archives/>`_ for cultural heritage (web archives section)\n- `llama-hub <https://github.com/emptycrown/llama-hub>`_, a library of data loaders for large language models\n- `Obsei <https://obsei.com/>`_, a text collection and analysis tool\n- `Vulristics <https://github.com/leonov-av/vulristics>`_, a framework for analyzing publicly available information about vulnerabilities\n- `Website-to-Chatbot <https://github.com/Anil-matcha/Website-to-Chatbot>`_, a personalized chatbot\n\nFor more see this list of `software using Trafilatura <https://github.com/adbar/trafilatura/network/dependents>`_.\n\n\nCitations in papers\n-------------------\n\nTrafilatura as a whole\n^^^^^^^^^^^^^^^^^^^^^^\n\n\n**To reference this software in a publication please cite the following paper:**\n\n- Barbaresi, A. \"`Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction <https://aclanthology.org/2021.acl-demo.15/>`_\", in *Proceedings of ACL/IJCNLP 2021: System Demonstrations*, 2021, p. 122-131. DOI: 10.18653/v1/2021.acl-demo.15\n\n\n.. image:: https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue\n    :target: https://aclanthology.org/2021.acl-demo.15/\n    :alt: Reference DOI: 10.18653/v1/2021.acl-demo.15\n\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3460969.svg\n   :target: https://doi.org/10.5281/zenodo.3460969\n   :alt: Zenodo archive DOI: 10.5281/zenodo.3460969\n\n.. code-block:: shell\n\n    @inproceedings{barbaresi-2021-trafilatura,\n      title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n      author = \"Barbaresi, Adrien\",\n      booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n      pages = \"122--131\",\n      publisher = \"Association for Computational Linguistics\",\n      url = \"https://aclanthology.org/2021.acl-demo.15\",\n      year = 2021,\n    }\n\n\nDate extraction (htmldate)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe date extraction component ``htmldate`` is referenced in the following publication:\n\n- Barbaresi, A. \"`htmldate: A Python package to extract publication dates from web pages <https://doi.org/10.21105/joss.02439>`_\", *Journal of Open Source Software*, 5(51), 2439, 2020. DOI: 10.21105/joss.02439\n\n.. image:: https://joss.theoj.org/papers/10.21105/joss.02439/status.svg\n   :target: https://doi.org/10.21105/joss.02439\n   :alt: JOSS article\n\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3459599.svg\n   :target: https://doi.org/10.5281/zenodo.3459599\n   :alt: Zenodo archive\n\n.. code-block:: shell\n\n    @article{barbaresi-2020-htmldate,\n      title = {{htmldate: A Python package to extract publication dates from web pages}},\n      author = \"Barbaresi, Adrien\",\n      journal = \"Journal of Open Source Software\",\n      volume = 5,\n      number = 51,\n      pages = 2439,\n      url = {https://doi.org/10.21105/joss.02439},\n      publisher = {The Open Journal},\n      year = 2020,\n    }\n\n\nPublications citing Trafilatura\n-------------------------------\n\n\n- Alakukku, L. (2022). \"Domain specific boilerplate removal from web pages with entropy and clustering\", Master's thesis, University of Aalto.\n- Alhamzeh, A., Bouhaouel, M., Egyed-Zsigmond, E., & Mitrović, J. (2021). \"DistilBERT-based Argumentation Retrieval for Answering Comparative Questions\", Proceedings of CLEF 2021 – Conference and Labs of the Evaluation Forum.\n- Bender, M., Bubenhofer, N., Dreesen, P., Georgi, C., Rüdiger, J. O., & Vogel, F. (2022). Techniken und Praktiken der Verdatung. Diskurse–digital, 135-158.\n- Bevendorff, J., Gupta, S., Kiesel, J., & Stein, B. (2023). An Empirical Comparison of Web Content Extraction Algorithms.\n- Bozarth, L., & Budak, C. (2021). \"An Analysis of the Partnership between Retailers and Low-credibility News Publishers\", Journal of Quantitative Description: Digital Media, 1.\n- Brandon, C., Doherty, A. J., Kelly, D., Leddin, D., & Margaria, T. (2023). HIPPP: Health Information Portal for Patients and Public. Applied Sciences, 13(16), 9453.\n- Braun, D. (2021). \"Automated Semantic Analysis, Legal Assessment, and Summarization of Standard Form Contracts\", PhD Thesis, Technische Universität München.\n- Chen, X., Zeynali, A., Camargo, C., Flöck, F., Gaffney, D., Grabowicz, P., ... & Samory, M. (2022). SemEval-2022 Task 8: Multilingual news article similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) (pp. 1094-1106).\n- Di Giovanni, M., Tasca, T., & Brambilla, M. (2022). DataScience-Polimi at SemEval-2022 Task 8: Stacking Language Models to Predict News Article Similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) (pp. 1229-1234).\n- Dumitru, V., Iorga, D., Ruseti, S., & Dascalu, M. (2023). Garbage in, garbage out: An analysis of HTML text extractors and their impact on NLP performance. In 2023 24th International Conference on Control Systems and Computer Science (CSCS) (pp. 403-410). IEEE.\n- Fröbe, M., Hagen, M., Bevendorff, J., Völske, M., Stein, B., Schröder, C., ... & Potthast, M. (2021). \"The Impact of Main Content Extraction on Near-Duplicate Detection\". arXiv preprint arXiv:2111.10864.\n- Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", arXiv preprint arXiv:2101.00027.\n- Gopalakrishnan, S., Chen, V. Z., Dou, W., Hahn-Powell, G., Nedunuri, S., & Zadrozny, W. W. (2023). Text to Causal Knowledge Graph: A Framework to Synthesize Knowledge from Unstructured Business Texts into Causal Graphs.\n- Harrando, I., & Troncy, R. (2021). \"Explainable Zero-Shot Topic Extraction Using a Common-Sense Knowledge Graph\", In 3rd Conference on Language, Data and Knowledge (LDK 2021). OpenAccess Series in Informatics, Dagstuhl Publishing.\n- Hartmann, S. (2023). Open Corpus Linguistics–or How to overcome common problems in dealing with corpus data by adopting open research practices.\n- Hunter, B., Mathews, F., & Weeds, J. (2023). Using hierarchical text classification to investigate the utility of machine learning in automating online analyses of wildlife exploitation. Ecological Informatics, 102076.\n- Indig, B., Sárközi-Lindner, Z., & Nagy, M. (2022). Use the Metadata, Luke!–An Experimental Joint Metadata Search and N-gram Trend Viewer for Personal Web Archives. In Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities (pp. 47-52).\n- Johannsen, B. (2023). Fußball und safety: Eine framesemantische Perspektive auf Diskurse über trans Sportler* innen. Queere Vielfalt im Fußball, 176.\n- Jung, G., Han, S., Kim, H., Kim, K., & Cha, J. (2022). Extracting the Main Content of Web Pages Using the First Impression Area. IEEE Access.\n- Karabulut, M., & Mayda, İ. (2020). \"Development of Browser Extension for HTML Web Page Content Extraction\", In 2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA) (pp. 1-6). IEEE.\n- Khusainov, A., Suleymanov, D., Gilmullin, R., Minsafina, A., Kubedinova, L., & Abdurakhmonova, N. \"First Results of the “TurkLang-7” Project: Creating Russian-Turkic Parallel Corpora and MT Systems\", In CMCL (pp. 90-101).\n- Küehn, P., Relke, D. N., & Reuter, C. (2023). Common Vulnerability Scoring System Prediction based on Open Source Intelligence Information Sources. Computers & Security, 103286.\n- Kuehn, P., Schmidt, M., & Reuter, C. (2023). ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. arXiv preprint arXiv:2304.11960.\n- Laippala, V., Rönnqvist, S., Hellström, S., Luotolahti, J., Repo, L., Salmela, A., ... & Pyysalo, S. (2020). \"From Web Crawl to Clean Register-Annotated Corpora\", Proceedings of the 12th Web as Corpus Workshop (pp. 14-22).\n- Laippala, V., Salmela, A., Rönnqvist, S., Aji, A. F., Chang, L. H., Dhifallah, A., ... & Pyysalo, S. (2022). Towards better structured and less noisy Web data: Oscar with Register annotations. In Proceedings of the Eighth Workshop on Noisy User-generated Text (W-NUT 2022) (pp. 215-221).\n- Madrid-Morales, D. (2021). \"Who Set the Narrative? Assessing the Influence of Chinese Media in News Coverage of COVID-19 in 30 African Countries\", Global Media and China, 6(2), 129-151.\n- Meier-Vieracker, S. (2022). \"Fußballwortschatz digital–Korpuslinguistische Ressourcen für den Sprachunterricht.\" Korpora Deutsch als Fremdsprache (KorDaF), 2022/01 (pre-print).\n- Meng, K. (2021). \"An End-to-End Computational System for Monitoring and Verifying Factual Claims\" (pre-print).\n- Miquelina, N., Quaresma, P., & Nogueira, V. B. (2022). Generating a European Portuguese BERT Based Model Using Content from Arquivo. pt Archive. In International Conference on Intelligent Data Engineering and Automated Learning (pp. 280-288). Springer, Cham.\n- Nissopoulou, T. X. (2023). Web content classification analysis, MSc thesis, International Hellenic University.\n- Nolda, A., Barbaresi, A., & Geyken, A. (2023). Korpora für die lexikographische Beschreibung diatopischer Variation in der deutschen Standardsprache. Korpora in der germanistischen Sprachwissenschaft: Mündlich, schriftlich, multimedial, 29.\n- Öhman, J., Verlinden, S., Ekgren, A., Gyllensten, A. C., Isbister, T., Gogoulou, E., ... & Sahlgren, M. (2023). The Nordic Pile: A 1.2 TB Nordic Dataset for Language Modeling. arXiv preprint arXiv:2303.17183.\n- Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Pannier, B., ... & Launay, J. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.\n- Piskorski, J., Stefanovitch, N., Da San Martino, G., & Nakov, P. (2023). Semeval-2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. In Proceedings of the the 17th International Workshop on Semantic Evaluation (SemEval-2023) (pp. 2343-2361).\n- Robertson, F., Lagus, J., & Kajava, K. (2021). \"A COVID-19 news coverage mood map of Europe\", Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation (pp. 110-115).\n- Salmela, A. (2022). \"Distinguishing Noise and Main Text Content from Web-Sourced Plain Text Documents Using Sequential Neural Networks\", Master's thesis, University of Turku.\n- Sawczyn, A., Binkowski, J., Janiak, D., Augustyniak, Ł., & Kajdanowicz, T. (2021). \"Fact-checking: relevance assessment of references in the Polish political domain\", Procedia Computer Science, 192, 1285-1293.\n- Schamel, T., Braun, D., & Matthes, F. (2022). Structured Extraction of Terms and Conditions from German and English Online Shops. In Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5) (pp. 181-190).\n- Sutter, T., Bozkir, A. S., Gehring, B., & Berlich, P. (2022). Avoiding the Hook: Influential Factors of Phishing Awareness Training on Click-Rates and a Data-Driven Approach to Predict Email Difficulty Perception. IEEE Access, 10, 100540-100565.\n- Ter-Akopyan, B. (2022). \"Identification of Political Leaning in German News\", Master's thesis, Ludwig Maximilian University of Munich.\n- Varlamov, M., Galanin, D., Bedrin, P., Duda, S., Lazarev, V., & Yatskov, A. (2022). A Dataset for Information Extraction from News Web Pages. In 2022 Ivannikov Ispras Open Conference (ISPRAS) (pp. 100-106). IEEE.\n- Waheed, A., Qunaibi, S., Barradas, D., & Weinberg, Z. (2022). Darwin's Theory of Censorship: Analysing the Evolution of Censored Topics with Dynamic Topic Models. In Proceedings of the 21st Workshop on Privacy in the Electronic Society (pp. 103-108).\n- Zinn, J. O., & Müller, M. (2021). \"Understanding discourse and language of risk\", Journal of Risk Research, 1-14.\n\n\n\nPublications citing Htmldate\n----------------------------\n\nSee `citation page of htmldate's documentation <https://htmldate.readthedocs.io/en/latest/used-by.html>`_.\n\n\n\nPorts\n-----\n\nGo port\n    `go-trafilatura <https://github.com/markusmobius/go-trafilatura>`_\n\n\n"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/background.rst.txt",
    "html": "Background\n==========\n\n\nThe pages below provide background information on scientific approaches to web data collection and processing, corpus linguistics, digital humanities, and natural language processing.\n\n\n.. toctree::\n    :maxdepth: 2\n\n    compendium\n    sources\n    corpus-data\n\n"
  },
  {
    "title": "trafilatura.utils — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/utils.html#decode_response",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.utils\nSource code for trafilatura.utils\n# pylint:disable-msg=E0611,I1101\n\"\"\"\nModule bundling functions related to HTML and text processing.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\n# import csv\nimport logging\nimport re\n\n# if brotli is installed\ntry:\n    import brotli\nexcept ImportError:\n    brotli = None\n\nfrom difflib import SequenceMatcher\nfrom functools import lru_cache\nfrom gzip import decompress\nfrom html import unescape\nfrom itertools import islice\nfrom unicodedata import normalize\n\n# CChardet is faster and can be more accurate\ntry:\n    from cchardet import detect as cchardet_detect\nexcept ImportError:\n    cchardet_detect = None\nfrom charset_normalizer import from_bytes\nfrom lxml.html import HtmlElement, HTMLParser, fromstring\n# response types\nfrom urllib3.response import HTTPResponse\n\nLOGGER = logging.getLogger(__name__)\n\nUNICODE_ALIASES = {'utf-8', 'utf_8'}\n\nDOCTYPE_TAG = re.compile(\"^< ?! ?DOCTYPE.+?/ ?>\", re.I)\n\n# note: htmldate could use HTML comments\n# huge_tree=True, remove_blank_text=True\nHTML_PARSER = HTMLParser(collect_ids=False, default_doctype=False, encoding='utf-8', remove_comments=True, remove_pis=True)\n\nLINES_TRIMMING = re.compile(r'(?<![p{P}>])\\n', flags=re.UNICODE|re.MULTILINE)\n\nURL_BLACKLIST_REGEX = re.compile(r'^https?://|/+$')\n\n# Regex to check image file extensions\nIMAGE_EXTENSION = re.compile(r'[^\\s]+\\.(avif|bmp|gif|hei[cf]|jpe?g|png|webp)(\\b|$)')\n\nAUTHOR_PREFIX = re.compile(r'^([a-zäöüß]+(ed|t))? ?(written by|words by|words|by|von|from) ', flags=re.IGNORECASE)\nAUTHOR_REMOVE_NUMBERS = re.compile(r'\\d.+?$')\nAUTHOR_TWITTER = re.compile(r'@[\\w]+')\nAUTHOR_REPLACE_JOIN = re.compile(r'[._+]')\nAUTHOR_REMOVE_NICKNAME = re.compile(r'[\"‘({\\[’\\'][^\"]+?[‘’\"\\')\\]}]')\nAUTHOR_REMOVE_SPECIAL = re.compile(r'[^\\w]+$|[:()?*$#!%/<>{}~¿]')\nAUTHOR_REMOVE_PREPOSITION = re.compile(r'\\b\\s+(am|on|for|at|in|to|from|of|via|with|—|-|–)\\s+(.*)', flags=re.IGNORECASE)\nAUTHOR_EMAIL = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\nAUTHOR_SPLIT = re.compile(r'/|;|,|\\||&|(?:^|\\W)[u|a]nd(?:$|\\W)', flags=re.IGNORECASE)\nAUTHOR_EMOJI_REMOVE = re.compile(\n    \"[\"\n    u\"\\U00002700-\\U000027BF\"  # Dingbats\n    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n    u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n    u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n    u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n    u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n    \"]+\", flags=re.UNICODE)\nAUTHOR_REMOVE_HTML = re.compile(r'<[^>]+>')\nCLEAN_META_TAGS = re.compile(r'[\"\\']')\n\nSTRIP_EXTENSION = re.compile(r\"\\.[^/?#]{2,63}$\")\n\n\ndef handle_compressed_file(filecontent):\n    \"\"\"Tell if a file's magic number corresponds to the GZip format\n       and try to decode it. Alternatively, try Brotli if the package\n       is installed.\"\"\"\n    if isinstance(filecontent, bytes):\n        # source: https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed\n        if filecontent[:2] == b'\\x1f\\x8b':\n            # decode GZipped data\n            try:\n                filecontent = decompress(filecontent)\n            except (EOFError, OSError):\n                logging.warning('invalid GZ file')\n        # try brotli\n        elif brotli is not None:\n            try:\n                filecontent = brotli.decompress(filecontent)\n            except brotli.error:\n                pass  # logging.debug('invalid Brotli file')\n    return filecontent\n\n\ndef isutf8(data):\n    \"\"\"Simple heuristic to determine if a bytestring uses standard unicode encoding\"\"\"\n    try:\n        data.decode('UTF-8')\n    except UnicodeDecodeError:\n        return False\n    return True\n\n\ndef detect_encoding(bytesobject):\n    \"\"\"\"Read all input or first chunk and return a list of encodings\"\"\"\n    # alternatives: https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py\n    # unicode-test\n    if isutf8(bytesobject):\n        return ['utf-8']\n    guesses = []\n    # additional module\n    if cchardet_detect is not None:\n        cchardet_guess = cchardet_detect(bytesobject)['encoding']\n        if cchardet_guess is not None:\n            guesses.append(cchardet_guess.lower())\n    # try charset_normalizer on first part, fallback on full document\n    detection_results = from_bytes(bytesobject[:15000]) or from_bytes(bytesobject)\n    # return alternatives\n    if len(detection_results) > 0:\n        guesses.extend([r.encoding for r in detection_results])\n    # it cannot be utf-8 (tested above)\n    return [g for g in guesses if g not in UNICODE_ALIASES]\n\n\n\n\n\n[docs]\n\ndef decode_response(response):\n    \"\"\"Read the urllib3 object corresponding to the server response,\n       check if it could be GZip and eventually decompress it, then\n       try to guess its encoding and decode it to return a unicode string\"\"\"\n    # urllib3 response object / bytes switch\n    resp_content = response if isinstance(response, bytes) else response.data\n    return decode_file(resp_content)\n\n\n\n\ndef decode_file(filecontent):\n    \"\"\"Guess bytestring encoding and try to decode to Unicode string.\n       Resort to destructive conversion otherwise.\"\"\"\n    # init\n    if isinstance(filecontent, str):\n        return filecontent\n    htmltext = None\n    # GZip and Brotli test\n    filecontent = handle_compressed_file(filecontent)\n    # encoding\n    for guessed_encoding in detect_encoding(filecontent):\n        try:\n            htmltext = filecontent.decode(guessed_encoding)\n        except (LookupError, UnicodeDecodeError): # VISCII: lookup\n            LOGGER.warning('wrong encoding detected: %s', guessed_encoding)\n            htmltext = None\n        else:\n            break\n    # return original content if nothing else succeeded\n    return htmltext or str(filecontent, encoding='utf-8', errors='replace')\n\n\ndef is_dubious_html(beginning: str) -> bool:\n    \"Assess if the object is proper HTML (awith a corresponding tag or declaration).\"\n    return \"html\" not in beginning\n\n\ndef strip_faulty_doctypes(htmlstring: str, beginning: str) -> str:\n    \"Repair faulty doctype strings to make then palatable for libxml2.\"\n    # libxml2/LXML issue: https://bugs.launchpad.net/lxml/+bug/1955915\n    if \"doctype\" in beginning:\n        firstline, _, rest = htmlstring.partition(\"\\n\")\n        return DOCTYPE_TAG.sub(\"\", firstline, count=1) + \"\\n\" + rest\n    return htmlstring\n\n\ndef fromstring_bytes(htmlobject):\n    \"Try to pass bytes to LXML parser.\"\n    tree = None\n    try:\n        tree = fromstring(htmlobject.encode('utf8', 'surrogatepass'), parser=HTML_PARSER)\n    except Exception as err:\n        LOGGER.error('lxml parser bytestring %s', err)\n    return tree\n\n\n\n\n\n[docs]\n\ndef load_html(htmlobject):\n    \"\"\"Load object given as input and validate its type\n    (accepted: lxml.html tree, trafilatura/urllib3 response, bytestring and string)\n    \"\"\"\n    # use tree directly\n    if isinstance(htmlobject, HtmlElement):\n        return htmlobject\n    # use trafilatura or urllib3 responses directly\n    if isinstance(htmlobject, HTTPResponse) or hasattr(htmlobject, 'data'):\n        htmlobject = htmlobject.data\n    # do not accept any other type after this point\n    if not isinstance(htmlobject, (bytes, str)):\n        raise TypeError('incompatible input type', type(htmlobject))\n    # start processing\n    tree = None\n    # try to guess encoding and decode file: if None then keep original\n    htmlobject = decode_file(htmlobject)\n    # sanity checks\n    beginning = htmlobject[:50].lower()\n    check_flag = is_dubious_html(beginning)\n    # repair first\n    htmlobject = strip_faulty_doctypes(htmlobject, beginning)\n    # first pass: use Unicode string\n    fallback_parse = False\n    try:\n        tree = fromstring(htmlobject, parser=HTML_PARSER)\n    except ValueError:\n        # \"Unicode strings with encoding declaration are not supported.\"\n        tree = fromstring_bytes(htmlobject)\n        fallback_parse = True\n    except Exception as err:\n        LOGGER.error('lxml parsing failed: %s', err)\n    # second pass: try passing bytes to LXML\n    if (tree is None or len(tree) < 1) and not fallback_parse:\n        tree = fromstring_bytes(htmlobject)\n    # rejection test: is it (well-formed) HTML at all?\n    # log parsing errors\n    if tree is not None and check_flag is True and len(tree) < 2:\n        LOGGER.error('parsed tree length: %s, wrong data type or not valid HTML', len(tree))\n        tree = None\n    return tree\n\n\n\n\ndef txttocsv(text, comments, docmeta):\n    '''Output the result in CSV format (tab-separated values)'''\n    # outputwriter = csv.writer(sys.stdout, delimiter='\\t', quoting=csv.QUOTE_NONE)\n    # outputwriter.writerow()\n    # with newlines: '\\\\n'.join()\n    text = trim(' '.join(text.splitlines()))\n    if comments is not None:\n        comments = trim(' '.join(comments.splitlines()))\n    tsv_output = \\\n        f'{docmeta.url}\\t{docmeta.fingerprint}\\t{docmeta.hostname}\\t{docmeta.title}\\t{docmeta.image}\\t{docmeta.date}\\t{text}\\t{comments}\\t{docmeta.license}\\t{docmeta.pagetype}\\n'\n    # add id up front if provided\n    if docmeta.id is not None:\n        tsv_output = docmeta.id + '\\t' + tsv_output\n    return tsv_output\n\n\n@lru_cache(maxsize=2**14)  # sys.maxunicode = 1114111\ndef return_printables_and_spaces(char):\n    'Return a character if it belongs to certain classes'\n    return char if char.isprintable() or char.isspace() else ''\n\n\ndef remove_control_characters(string):\n    '''Prevent non-printable and XML invalid character errors'''\n    return ''.join(map(return_printables_and_spaces, string))\n\n\ndef normalize_unicode(string, unicodeform='NFC'):\n    'Normalize the given string to the specified unicode format.'\n    return normalize(unicodeform, string)\n\n\n@lru_cache(maxsize=1024)\ndef line_processing(line):\n    '''Remove HTML space entities, then discard incompatible unicode\n       and invalid XML characters on line level'''\n    # spacing HTML entities: https://www.w3.org/MarkUp/html-spec/html-spec_13.html\n    # unique code spaces\n    line = line.replace('&#13;', '\\r').replace('&#10;', '\\n').replace('&nbsp;', '\\u00A0').replace(';cs;', ' ')\n    # remove newlines that are not related to punctuation or markup\n    # remove non-printable chars and normalize space characters (including Unicode spaces)\n    line = trim(remove_control_characters(LINES_TRIMMING.sub(r' ', line)))\n    # prune empty lines\n    if all(map(str.isspace, line)):\n        line = None\n    return line\n\n\n\n\n\n[docs]\n\ndef sanitize(text):\n    '''Convert text and discard incompatible and invalid characters'''\n    try:\n        return '\\n'.join(filter(None, (line_processing(l) for l in text.splitlines())))\n    except AttributeError:\n        return None\n\n\n\n\n\n\n\n[docs]\n\n@lru_cache(maxsize=1024)\ndef trim(string):\n    '''Remove unnecessary spaces within a text string'''\n    try:\n        # remove newlines that are not related to punctuation or markup + proper trimming\n        # return LINES_TRIMMING.sub(r' ', string).strip(' \\t\\n\\r\\v')\n        # faster:\n        return ' '.join(string.split()).strip()\n    except (AttributeError, TypeError):\n        return None\n\n\n\n\ndef normalize_tags(tags):\n    '''Remove special characters of tags'''\n    tags = CLEAN_META_TAGS.sub(r'', trim(unescape(tags)))\n    return \", \".join(filter(None, tags.split(\", \")))\n\n\ndef is_image_file(imagesrc):\n    '''Check if the observed string corresponds to a valid image extension,\n       return False otherwise'''\n    return bool(imagesrc is not None and IMAGE_EXTENSION.search(imagesrc))\n\n\ndef normalize_authors(current_authors, author_string):\n    '''Normalize author info to focus on author names only'''\n    new_authors = []\n    if author_string.lower().startswith('http') or AUTHOR_EMAIL.match(author_string):\n        return current_authors\n    if current_authors is not None:\n        new_authors = current_authors.split('; ')\n    # fix to code with unicode\n    if '\\\\u' in author_string:\n        author_string = author_string.encode().decode('unicode_escape')\n    # fix html entities\n    if '&#' in author_string or '&amp;' in author_string:\n        author_string = unescape(author_string)\n    # remove html tags\n    author_string = AUTHOR_REMOVE_HTML.sub('', author_string)\n    # examine names\n    for author in AUTHOR_SPLIT.split(author_string):\n        author = trim(author)\n        # remove emoji\n        author = AUTHOR_EMOJI_REMOVE.sub('', author)\n        # remove @username\n        author = AUTHOR_TWITTER.sub('', author)\n        # replace special characters with space\n        author = trim(AUTHOR_REPLACE_JOIN.sub(' ', author))\n        author = AUTHOR_REMOVE_NICKNAME.sub('', author)\n        # remove special characters\n        author = AUTHOR_REMOVE_SPECIAL.sub('', author)\n        author = AUTHOR_PREFIX.sub('', author)\n        author = AUTHOR_REMOVE_NUMBERS.sub('', author)\n        author = AUTHOR_REMOVE_PREPOSITION.sub('', author)\n        # skip empty or improbably long strings\n        if len(author) == 0 or (\n            # simple heuristics, regex or vowel tests also possible\n            ' ' not in author and '-' not in author and len(author) >= 50\n            ):\n            continue\n        # title case\n        if not author[0].isupper() or sum(1 for c in author if c.isupper()) < 1:\n            author = author.title()\n        # safety checks\n        if author not in new_authors and (len(new_authors) == 0 or all(new_author not in author for new_author in new_authors)):\n            new_authors.append(author)\n    if len(new_authors) == 0:\n        return current_authors\n    return '; '.join(new_authors).strip('; ')\n\n\ndef uniquify_list(l):\n    \"\"\"\n    Remove duplicates from a list while keeping order in an efficient way.\n    Dictionaries preserve insertion order since Python 3.6.\n\n    https://www.peterbe.com/plog/fastest-way-to-uniquify-a-list-in-python-3.6\n    \"\"\"\n    return list(dict.fromkeys(l))\n\n\n@lru_cache(maxsize=1024)\ndef is_similar_domain(reference, new_string, threshold=0.5):\n    \"Return the similarity ratio between two short strings, here domain names.\"\n    if new_string != reference:\n        new_string = STRIP_EXTENSION.sub(\"\", new_string)\n        reference = STRIP_EXTENSION.sub(\"\", reference)\n        if SequenceMatcher(None, reference, new_string).ratio() < threshold:\n            return False\n    return True\n\n\ndef make_chunks(iterable, n):\n    \"\"\"\n    Chunk data into smaller pieces.\n    https://docs.python.org/3/library/itertools.html\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        chunk = tuple(islice(it, n))\n        if not chunk:\n            return\n        yield chunk\n    # Python 3.8+ with walrus operator\n    # while batch := tuple(islice(it, n)):\n    #    yield batch\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/corefunctions.rst.txt",
    "html": "Core functions\n==============\n\n.. contents:: Table of contents\n    :depth: 2\n    :local:\n    :backlinks: none\n\n\nExtraction\n----------\n\n``extract()``\n~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.extract\n\n``bare_extraction()``\n~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.bare_extraction\n\n``baseline()``\n~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.baseline\n\n``html2txt()``\n~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.html2txt\n\n``try_readability()``\n~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.external.try_readability\n\n``try_justext()``\n~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.external.try_justext\n\n``extract_metadata()``\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.extract_metadata\n\n``extract_comments()``\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.core.extract_comments\n\n\nLink discovery\n--------------\n\n``sitemap_search()``\n~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.sitemaps.sitemap_search\n\n``find_feed_urls()``\n~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.feeds.find_feed_urls\n\n``focused_crawler()``\n~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.spider.focused_crawler\n\nHelpers\n-------\n\n``fetch_url()``\n~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.fetch_url\n\n``decode_response()``\n~~~~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.utils.decode_response\n\n``load_html()``\n~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.load_html\n\n``sanitize()``\n~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.utils.sanitize\n\n``trim()``\n~~~~~~~~~~\n\n.. autofunction:: trafilatura.utils.trim\n\n\nXML processing\n--------------\n\n``xmltotxt()``\n~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.xml.xmltotxt\n\n``validate_tei()``\n~~~~~~~~~~~~~~~~~~\n\n.. autofunction:: trafilatura.xml.validate_tei\n\n"
  },
  {
    "title": "trafilatura.downloads — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/downloads.html#fetch_url",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura....\nSource code for trafilatura.downloads\n# pylint:disable-msg=E0611,I1101\n\"\"\"\nAll functions needed to steer and execute downloads of web documents.\n\"\"\"\n\n\nimport logging\nimport random\nfrom collections import namedtuple\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom time import sleep\n\nimport certifi\n\ntry:\n    import pycurl\n    CURL_SHARE = pycurl.CurlShare()\n    # available options:\n    # https://curl.se/libcurl/c/curl_share_setopt.html\n    CURL_SHARE.setopt(pycurl.SH_SHARE, pycurl.LOCK_DATA_DNS)\n    CURL_SHARE.setopt(pycurl.SH_SHARE, pycurl.LOCK_DATA_SSL_SESSION)\n    # not thread-safe\n    # CURL_SHARE.setopt(pycurl.SH_SHARE, pycurl.LOCK_DATA_CONNECT)\nexcept ImportError:\n    pycurl = None\n\nimport urllib3\nfrom courlan import UrlStore\nfrom courlan.network import redirection_test\n\nfrom . import __version__\nfrom .settings import DEFAULT_CONFIG\nfrom .utils import (URL_BLACKLIST_REGEX, decode_response, make_chunks,\n                    uniquify_list)\n\nNUM_CONNECTIONS = 50\nMAX_REDIRECTS = 2\n\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nHTTP_POOL = None\nNO_CERT_POOL = None\nRETRY_STRATEGY = None\n\nDEFAULT_HEADERS = urllib3.util.make_headers(accept_encoding=True)\nUSER_AGENT = 'trafilatura/' + __version__ + ' (+https://github.com/adbar/trafilatura)'\nDEFAULT_HEADERS['User-Agent'] = USER_AGENT\n\nLOGGER = logging.getLogger(__name__)\n\nRawResponse = namedtuple('RawResponse', ['data', 'status', 'url'])\n\n\n# caching throws an error\n# @lru_cache(maxsize=2)\ndef _parse_config(config):\n    'Read and extract HTTP header strings from the configuration file.'\n    # load a series of user-agents\n    myagents = config.get('DEFAULT', 'USER_AGENTS').strip() or None\n    if myagents is not None and myagents != '':\n        myagents = myagents.split(\"\\n\")\n    # https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies\n    # todo: support for several cookies?\n    mycookie = config.get('DEFAULT', 'COOKIE') or None\n    return myagents, mycookie\n\n\ndef _determine_headers(config, headers=None):\n    'Internal function to decide on user-agent string.'\n    if config != DEFAULT_CONFIG:\n        myagents, mycookie = _parse_config(config)\n        headers = {}\n        if myagents is not None:\n            rnumber = random.randint(0, len(myagents) - 1)\n            headers['User-Agent'] = myagents[rnumber]\n        if mycookie is not None:\n            headers['Cookie'] = mycookie\n    return headers or DEFAULT_HEADERS\n\n\ndef _send_request(url, no_ssl, config):\n    \"Internal function to robustly send a request (SSL or not) and return its result.\"\n    # customize headers\n    global HTTP_POOL, NO_CERT_POOL, RETRY_STRATEGY\n    if not RETRY_STRATEGY:\n        RETRY_STRATEGY = urllib3.util.Retry(\n            total=0,\n            redirect=MAX_REDIRECTS, # raise_on_redirect=False,\n            connect=0,\n            backoff_factor=config.getint('DEFAULT', 'DOWNLOAD_TIMEOUT')/2,\n            status_forcelist=[\n                429, 499, 500, 502, 503, 504, 509, 520, 521, 522, 523, 524, 525, 526, 527, 530, 598\n            ],\n            # unofficial: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#Unofficial_codes\n        )\n    try:\n        # TODO: read by streaming chunks (stream=True, iter_content=xx)\n        # so we can stop downloading as soon as MAX_FILE_SIZE is reached\n        if no_ssl is False:\n            # define pool\n            if not HTTP_POOL:\n                HTTP_POOL = urllib3.PoolManager(retries=RETRY_STRATEGY, timeout=config.getint('DEFAULT', 'DOWNLOAD_TIMEOUT'), ca_certs=certifi.where(), num_pools=NUM_CONNECTIONS)  # cert_reqs='CERT_REQUIRED'\n            # execute request\n            response = HTTP_POOL.request('GET', url, headers=_determine_headers(config))\n        else:\n            # define pool\n            if not NO_CERT_POOL:\n                NO_CERT_POOL = urllib3.PoolManager(retries=RETRY_STRATEGY, timeout=config.getint('DEFAULT', 'DOWNLOAD_TIMEOUT'), cert_reqs='CERT_NONE', num_pools=NUM_CONNECTIONS)\n            # execute request\n            response = NO_CERT_POOL.request('GET', url, headers=_determine_headers(config))\n    except urllib3.exceptions.SSLError:\n        LOGGER.warning('retrying after SSLError: %s', url)\n        return _send_request(url, True, config)\n    except Exception as err:\n        LOGGER.error('download error: %s %s', url, err)  # sys.exc_info()[0]\n    else:\n        # necessary for standardization\n        return RawResponse(response.data, response.status, response.geturl())\n    # catchall\n    return None\n\n\ndef _handle_response(url, response, decode, config):\n    'Internal function to run safety checks on response result.'\n    if response.status != 200:\n        LOGGER.error('not a 200 response: %s for URL %s', response.status, url)\n    elif response.data is None or len(response.data) < config.getint('DEFAULT', 'MIN_FILE_SIZE'):\n        LOGGER.error('too small/incorrect for URL %s', url)\n        # raise error instead?\n    elif len(response.data) > config.getint('DEFAULT', 'MAX_FILE_SIZE'):\n        LOGGER.error('too large: length %s for URL %s', len(response.data), url)\n        # raise error instead?\n    else:\n        return decode_response(response.data) if decode is True else response\n    # catchall\n    return None\n\n\n\n\n\n[docs]\n\ndef fetch_url(url, decode=True, no_ssl=False, config=DEFAULT_CONFIG):\n    \"\"\"Fetches page using urllib3 and decodes the response.\n\n    Args:\n        url: URL of the page to fetch.\n        decode: Decode response instead of returning urllib3 response object (boolean).\n        no_ssl: Don't try to establish a secure connection (to prevent SSLError).\n        config: Pass configuration values for output control.\n\n    Returns:\n        RawResponse object: data (headers + body), status (HTML code as string) and url\n        or None in case the result is invalid or there was a problem with the network.\n\n    \"\"\"\n    LOGGER.debug('sending request: %s', url)\n    if pycurl is None:\n        response = _send_request(url, no_ssl, config)\n    else:\n        response = _send_pycurl_request(url, no_ssl, config)\n    if response is not None and response != '':\n        return _handle_response(url, response, decode, config)\n        # return '' (useful do discard further processing?)\n        # return response\n    LOGGER.debug('request failed: %s', url)\n    return None\n\n\n\n\ndef _pycurl_is_live_page(url):\n    \"Send a basic HTTP HEAD request with pycurl.\"\n    # Initialize pycurl object\n    curl = pycurl.Curl()\n    # Set the URL and HTTP method (HEAD)\n    curl.setopt(pycurl.URL, url.encode('utf-8'))\n    curl.setopt(pycurl.CONNECTTIMEOUT, 10)\n    # no SSL verification\n    curl.setopt(pycurl.SSL_VERIFYPEER, 0)\n    curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n    # Set option to avoid getting the response body\n    curl.setopt(curl.NOBODY, True)\n    # Perform the request\n    try:\n        curl.perform()\n    except pycurl.error as err:\n        LOGGER.debug('pycurl HEAD error: %s %s', url, err)\n        return False\n    # Get the response code\n    page_exists = curl.getinfo(curl.RESPONSE_CODE) < 400\n    # Clean up\n    curl.close()\n    return page_exists\n\n\ndef _urllib3_is_live_page(url):\n    \"Use courlan redirection test (based on urllib3) to send a HEAD request.\"\n    try:\n        _ = redirection_test(url)\n    except Exception as err:\n        LOGGER.debug('urllib3 HEAD error: %s %s', url, err)\n        return False\n    return True\n\n\ndef is_live_page(url):\n    \"Send a HTTP HEAD request without taking anything else into account.\"\n    if pycurl is not None:\n        return _pycurl_is_live_page(url)\n    return _urllib3_is_live_page(url)\n\n\ndef add_to_compressed_dict(inputlist, blacklist=None, url_filter=None, url_store=None, compression=False, verbose=False):\n    '''Filter, convert input URLs and add them to domain-aware processing dictionary'''\n    if url_store is None:\n        url_store = UrlStore(\n                        compressed=compression,\n                        strict=False,\n                        verbose=verbose\n                    )\n\n    inputlist = uniquify_list(inputlist)\n\n    if blacklist:\n        inputlist = [u for u in inputlist if URL_BLACKLIST_REGEX.sub('', u) not in blacklist]\n\n    if url_filter:\n        inputlist = [u for u in inputlist if any(f in u for f in url_filter)]\n\n    url_store.add_urls(inputlist)\n    return url_store\n\n\ndef load_download_buffer(url_store, sleep_time=5):\n    '''Determine threading strategy and draw URLs respecting domain-based back-off rules.'''\n    bufferlist = []\n    while not bufferlist:\n        bufferlist = url_store.get_download_urls(timelimit=sleep_time)\n        # add emptiness test or sleep?\n        if not bufferlist:\n            if url_store.done is True:\n                break\n            sleep(sleep_time)\n    return bufferlist, url_store\n\n\ndef buffered_downloads(bufferlist, download_threads, decode=True):\n    '''Download queue consumer, single- or multi-threaded.'''\n    with ThreadPoolExecutor(max_workers=download_threads) as executor:\n        for chunk in make_chunks(bufferlist, 10000):\n            future_to_url = {executor.submit(fetch_url, url, decode): url for url in chunk}\n            for future in as_completed(future_to_url):\n                # url and download result\n                yield future_to_url[future], future.result()\n\n\ndef _send_pycurl_request(url, no_ssl, config):\n    '''Experimental function using libcurl and pycurl to speed up downloads'''\n    # https://github.com/pycurl/pycurl/blob/master/examples/retriever-multi.py\n\n    # init\n    # headerbytes = BytesIO()\n    headers = _determine_headers(config)\n    headerlist = ['Accept-Encoding: gzip, deflate', 'Accept: */*']\n    for header, content in headers.items():\n        headerlist.append(header + ': ' + content)\n\n    # prepare curl request\n    # https://curl.haxx.se/libcurl/c/curl_easy_setopt.html\n    curl = pycurl.Curl()\n    curl.setopt(pycurl.URL, url.encode('utf-8'))\n    # share data\n    curl.setopt(pycurl.SHARE, CURL_SHARE)\n    curl.setopt(pycurl.HTTPHEADER, headerlist)\n    # curl.setopt(pycurl.USERAGENT, '')\n    curl.setopt(pycurl.FOLLOWLOCATION, 1)\n    curl.setopt(pycurl.MAXREDIRS, MAX_REDIRECTS)\n    curl.setopt(pycurl.CONNECTTIMEOUT, config.getint('DEFAULT', 'DOWNLOAD_TIMEOUT'))\n    curl.setopt(pycurl.TIMEOUT, config.getint('DEFAULT', 'DOWNLOAD_TIMEOUT'))\n    curl.setopt(pycurl.NOSIGNAL, 1)\n    if no_ssl is True:\n        curl.setopt(pycurl.SSL_VERIFYPEER, 0)\n        curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n    else:\n        curl.setopt(pycurl.CAINFO, certifi.where())\n    curl.setopt(pycurl.MAXFILESIZE, config.getint('DEFAULT', 'MAX_FILE_SIZE'))\n    #curl.setopt(pycurl.HEADERFUNCTION, headerbytes.write)\n    #curl.setopt(pycurl.WRITEDATA, bufferbytes)\n    # TCP_FASTOPEN\n    # curl.setopt(pycurl.FAILONERROR, 1)\n    # curl.setopt(pycurl.ACCEPT_ENCODING, '')\n\n    # send request\n    try:\n        bufferbytes = curl.perform_rb()\n    except pycurl.error as err:\n        LOGGER.error('pycurl error: %s %s', url, err)\n        # retry in case of SSL-related error\n        # see https://curl.se/libcurl/c/libcurl-errors.html\n        # errmsg = curl.errstr_raw()\n        # additional error codes: 80, 90, 96, 98\n        if no_ssl is False and err.args[0] in (35, 54, 58, 59, 60, 64, 66, 77, 82, 83, 91):\n            LOGGER.debug('retrying after SSL error: %s %s', url, err)\n            return _send_pycurl_request(url, True, config)\n        # traceback.print_exc(file=sys.stderr)\n        # sys.stderr.flush()\n        return None\n\n    # https://github.com/pycurl/pycurl/blob/master/examples/quickstart/response_headers.py\n    #respheaders = dict()\n    #for header_line in headerbytes.getvalue().decode('iso-8859-1').splitlines(): # re.split(r'\\r?\\n',\n    #    # This will botch headers that are split on multiple lines...\n    #    if ':' not in header_line:\n    #        continue\n    #    # Break the header line into header name and value.\n    #    name, value = header_line.split(':', 1)\n    #    # Now we can actually record the header name and value.\n    #    respheaders[name.strip()] = value.strip() # name.strip().lower() ## TODO: check\n    # status\n    respcode = curl.getinfo(curl.RESPONSE_CODE)\n    # url\n    effective_url = curl.getinfo(curl.EFFECTIVE_URL)\n    # additional info\n    # ip_info = curl.getinfo(curl.PRIMARY_IP)\n\n    # tidy up\n    curl.close()\n    return RawResponse(bufferbytes, respcode, effective_url)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.feeds — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/feeds.html#find_feed_urls",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.feeds\nSource code for trafilatura.feeds\n\"\"\"\nExamining feeds and extracting links for further processing.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\nimport json\nimport logging\nimport re\nfrom itertools import islice\n\nfrom courlan import (check_url, clean_url, filter_urls, fix_relative_urls,\n                     get_hostinfo, validate_url)\n\nfrom .downloads import fetch_url\nfrom .settings import MAX_LINKS\nfrom .utils import is_similar_domain, load_html\n\nLOGGER = logging.getLogger(__name__)\n\nFEED_TYPES = {'application/atom+xml', 'application/json', 'application/rdf+xml', 'application/rss+xml', 'application/x.atom+xml', 'application/x-atom+xml', 'text/atom+xml', 'text/plain', 'text/rdf+xml', 'text/rss+xml', 'text/xml'}\nFEED_OPENING = re.compile(r'<(feed|rss|\\?xml)')\nLINK_ATTRS = re.compile(r'<link .*?href=\".+?\"')\nLINK_HREF = re.compile(r'href=\"(.+?)\"')\nLINK_ELEMENTS = re.compile(r'<link>(?:\\s*)(?:<!\\[CDATA\\[)?(.+?)(?:\\]\\]>)?(?:\\s*)</link>')\nBLACKLIST = re.compile(r'\\bcomments\\b')  # no comment feed\n\n\ndef handle_link_list(linklist, domainname, baseurl, target_lang=None):\n    '''Examine links to determine if they are valid and\n       lead to a web page'''\n    output_links = []\n    # sort and uniq\n    for item in sorted(set(linklist)):\n        # fix and check\n        link = fix_relative_urls(baseurl, item)\n        # control output for validity\n        checked = check_url(link, language=target_lang)\n        if checked is not None:\n            if not is_similar_domain(domainname, checked[1]) and not \"feed\" in link:\n                LOGGER.warning('Rejected, diverging domain names: %s %s', domainname, checked[1])\n            else:\n                output_links.append(checked[0])\n        # Feedburner/Google feeds\n        elif 'feedburner' in item or 'feedproxy' in item:\n            output_links.append(item)\n    return output_links\n\n\ndef extract_links(feed_string, domainname, baseurl, reference, target_lang=None):\n    '''Extract links from Atom and RSS feeds'''\n    feed_links = []\n    # check if it's a feed\n    if feed_string is None:\n        LOGGER.debug('Empty feed: %s', domainname)\n        return feed_links\n    feed_string = feed_string.strip()\n    # typical first and second lines absent\n    if not FEED_OPENING.match(feed_string) and not \\\n        ('<rss' in feed_string[:100] or '<feed' in feed_string[:100]):\n        # could be JSON\n        if feed_string.startswith('{'):\n            try:\n                feed_dict = json.loads(feed_string)\n                if 'items' in feed_dict:\n                    for item in feed_dict['items']:\n                        if 'url' in item:\n                            feed_links.append(item['url'])\n                        # fallback: https://www.jsonfeed.org/version/1.1/\n                        elif 'id' in item:\n                            feed_links.append(item['id'])\n            except json.decoder.JSONDecodeError:\n                LOGGER.debug('JSON decoding error: %s', domainname)\n        else:\n            LOGGER.debug('Possibly invalid feed: %s', domainname)\n        return feed_links\n    # could be Atom\n    if '<link ' in feed_string:\n        for link in (m[0] for m in islice(LINK_ATTRS.finditer(feed_string), MAX_LINKS)):\n            if 'atom+xml' in link or 'rel=\"self\"' in link:\n                continue\n            feedlink = LINK_HREF.search(link)[1]\n            #if '\"' in feedlink:\n            #    feedlink = feedlink.split('\"')[0]\n            feed_links.append(feedlink)\n    # could be RSS\n    elif '<link>' in feed_string:\n        feed_links.extend(\n            [m[1].strip() for m in islice(LINK_ELEMENTS.finditer(feed_string, re.DOTALL), MAX_LINKS)]\n        )\n\n    # refine\n    output_links = handle_link_list(feed_links, domainname, baseurl, target_lang)\n    output_links = [l for l in output_links if l != reference and l.count('/') > 2]\n    # log result\n    if feed_links:\n        LOGGER.debug('Links found: %s of which %s valid', len(feed_links), len(output_links))\n    else:\n        LOGGER.debug('Invalid feed for %s', domainname)\n    return output_links\n\n\ndef determine_feed(htmlstring, baseurl, reference):\n    '''Try to extract the feed URL from the home page.\n       Adapted from http://www.aaronsw.com/2002/feedfinder/'''\n    # parse the page to look for feeds\n    tree = load_html(htmlstring)\n    # safeguard\n    if tree is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', baseurl)\n        return []\n    feed_urls = []\n    for linkelem in tree.xpath('//link[@rel=\"alternate\"]'):\n        # discard elements without links\n        if 'href' not in linkelem.attrib:\n            continue\n        # most common case\n        if 'type' in linkelem.attrib and linkelem.get('type') in FEED_TYPES:\n            feed_urls.append(linkelem.get('href'))\n        # websites like geo.de\n        elif 'atom' in linkelem.get('href') or 'rss' in linkelem.get('href'):\n            feed_urls.append(linkelem.get('href'))\n    # backup\n    if not feed_urls:\n        for linkelem in tree.xpath('//a[@href]'):\n            if linkelem.get('href')[-4:].lower() in ('.rss', '.rdf', '.xml'):\n                feed_urls.append(linkelem.get('href'))\n            elif linkelem.get('href')[-5:].lower() == '.atom':\n                feed_urls.append(linkelem.get('href'))\n            elif 'atom' in linkelem.get('href') or 'rss' in linkelem.get('href'):\n                feed_urls.append(linkelem.get('href'))\n    # refine\n    output_urls = []\n    for link in sorted(set(feed_urls)):\n        link = fix_relative_urls(baseurl, link)\n        link = clean_url(link)\n        if link is None or link == reference or validate_url(link)[0] is False:\n            continue\n        if BLACKLIST.search(link):\n            continue\n        output_urls.append(link)\n    # log result\n    LOGGER.debug('Feed URLs found: %s of which %s valid', len(feed_urls), len(output_urls))\n    return output_urls\n\n\n\n\n\n[docs]\n\ndef find_feed_urls(url, target_lang=None):\n    \"\"\"Try to find feed URLs.\n\n    Args:\n        url: Webpage or feed URL as string.\n             Triggers URL-based filter if the webpage isn't a homepage.\n        target_lang: Define a language to filter URLs based on heuristics\n             (two-letter string, ISO 639-1 format).\n\n    Returns:\n        The extracted links as a list (sorted list of unique links).\n\n    \"\"\"\n    domainname, baseurl = get_hostinfo(url)\n    if domainname is None:\n        LOGGER.warning('Invalid URL: %s', url)\n        return []\n    urlfilter = None\n    downloaded = fetch_url(url)\n    if downloaded is not None:\n        # assume it's a feed\n        feed_links = extract_links(downloaded, domainname, baseurl, url, target_lang)\n        if len(feed_links) == 0:\n            # assume it's a web page\n            for feed in determine_feed(downloaded, baseurl, url):\n                feed_string = fetch_url(feed)\n                feed_links.extend(extract_links(feed_string, domainname, baseurl, url, target_lang))\n            # filter triggered, prepare it\n            if len(url) > len(baseurl) + 2:\n                urlfilter = url\n        # return links found\n        if len(feed_links) > 0:\n            feed_links = filter_urls(feed_links, urlfilter)\n            LOGGER.debug('%s feed links found for %s', len(feed_links), domainname)\n            return feed_links\n        LOGGER.debug('No usable feed links found: %s', url)\n    else:\n        LOGGER.error('Could not download web page: %s', url)\n        if url.strip('/') != baseurl:\n            return try_homepage(baseurl, target_lang)\n    # try alternative: Google News\n    if target_lang is not None:\n        downloaded = fetch_url(\n            f'https://news.google.com/rss/search?q=site:{baseurl}&hl={target_lang}&scoring=n&num=100'\n        )\n        if downloaded is not None:\n            feed_links = extract_links(downloaded, domainname, baseurl, url, target_lang)\n            feed_links = filter_urls(feed_links, urlfilter)\n            LOGGER.debug('%s Google news links found for %s', len(feed_links), domainname)\n            return feed_links\n    return []\n\n\n\n\ndef try_homepage(baseurl, target_lang):\n    '''Shift into reverse and try the homepage instead of the particular feed\n       page that was given as input.'''\n    LOGGER.debug('Probing homepage for feeds instead: %s', baseurl)\n    return find_feed_urls(baseurl, target_lang)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.spider — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/spider.html#focused_crawler",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.spider\nSource code for trafilatura.spider\n# pylint:disable-msg=E0611,E1101,I1101\n\"\"\"\nFunctions dedicated to website navigation and crawling/spidering.\n\"\"\"\n\nimport logging\nimport urllib.robotparser\nfrom time import sleep\n\nfrom courlan import (UrlStore, extract_links, fix_relative_urls, get_hostinfo,\n                     is_navigation_page, is_not_crawlable)\n\nfrom .core import baseline\nfrom .downloads import fetch_url\n# from .feeds import find_feed_urls # extract_links ad extract_feed_links\nfrom .settings import DEFAULT_CONFIG\nfrom .utils import decode_response, load_html\n\n# language detection\ntry:\n    import py3langid\n    LANGID_FLAG = True\nexcept ImportError:\n    LANGID_FLAG = False\n\nLOGGER = logging.getLogger(__name__)\n\nURL_STORE = UrlStore(compressed=False, strict=False)\n\n\ndef refresh_detection(htmlstring, homepage):\n    \"Check if there could be a redirection by meta-refresh tag.\"\n    if not '\"refresh\"' in htmlstring and not '\"REFRESH\"' in htmlstring:\n        return htmlstring, homepage\n\n    html_tree = load_html(htmlstring)\n    if html_tree is None:\n        return htmlstring, homepage\n\n    # test meta-refresh redirection\n    # https://stackoverflow.com/questions/2318446/how-to-follow-meta-refreshes-in-python\n    results =  html_tree.xpath('//meta[@http-equiv=\"refresh\"]/@content|//meta[@http-equiv=\"REFRESH\"]/@content')\n    if results and ';' in results[0]:\n        text = results[0].split(';')[1].strip().lower()\n        if text.startswith('url=') or text.startswith('URL='):\n            url2 = text[4:]\n            if not url2.startswith('http'):\n                # Relative URL, adapt\n                _, base_url = get_hostinfo(url2)\n                url2 = fix_relative_urls(base_url, url2)\n            # second fetch\n            newhtmlstring = fetch_url(url2)\n            if newhtmlstring is None:\n                logging.warning('failed redirect: %s', url2)\n                return None, None\n            #else:\n            htmlstring, homepage = newhtmlstring, url2\n            logging.info('successful redirect: %s', url2)\n        else:\n            logging.info('no redirect found: %s', homepage)\n    return htmlstring, homepage\n\n\ndef probe_alternative_homepage(homepage):\n    \"Check if the homepage is redirected and return appropriate values.\"\n    response = fetch_url(homepage, decode=False)\n    if response is None or response == '':\n        return None, None, None\n    # get redirected URL here?\n    if response.url not in (homepage, \"/\"):\n        logging.info('followed redirect: %s', response.url)\n        homepage = response.url\n    # decode response\n    htmlstring = decode_response(response.data)\n    # is there a meta-refresh on the page?\n    htmlstring, homepage = refresh_detection(htmlstring, homepage)\n    if homepage is None:  # malformed or malicious content\n        return None, None, None\n    logging.info('fetching homepage OK: %s', homepage)\n    _, base_url = get_hostinfo(homepage)\n    return htmlstring, homepage, base_url\n\n\ndef process_links(htmlstring, url=\"\", language=None, rules=None):\n    \"\"\"Examine the HTML code and process the retrieved internal links.\n       Extract and filter new internal links after an optional language check.\n       Store the links in todo-list while prioritizing the navigation ones.\"\"\"\n    links, links_priority = [], []\n    # optional language check: run baseline extraction + language identifier\n    if language is not None and LANGID_FLAG is True and htmlstring is not None:\n        _, text, _ = baseline(htmlstring)\n        result, _ = py3langid.classify(text)\n        if result != language:\n            return\n    # iterate through the links and filter them\n    for link in extract_links(pagecontent=htmlstring, url=url, external_bool=False, language=language, with_nav=True):\n        # check robots.txt rules\n        if rules is not None and not rules.can_fetch(\"*\", link):\n            continue\n        # sanity check\n        if is_not_crawlable(link):\n            continue\n        # store\n        if is_navigation_page(link):\n            links_priority.append(link)\n        else:\n            links.append(link)\n    URL_STORE.add_urls(urls=links, appendleft=links_priority)\n\n\ndef process_response(response, base_url, language, rules=None):\n    \"\"\"Convert urllib3 response object and extract links.\"\"\"\n    # add final document URL to known_links\n    if response is not None:\n        URL_STORE.add_urls([response.url], visited=True)\n        if response.data is not None and response.data != '':\n            # convert urllib3 response to string\n            htmlstring = decode_response(response.data)\n            # proceed to link extraction\n            process_links(htmlstring, base_url, language=language, rules=rules)\n\n\ndef init_crawl(homepage, todo, known_links, language=None, rules=None):\n    \"\"\"Start crawl by initializing variables and potentially examining the starting page.\"\"\"\n    # config=DEFAULT_CONFIG\n    _, base_url = get_hostinfo(homepage)\n    if base_url is None or len(base_url) < 1:\n        raise ValueError(f'cannot crawl homepage: {homepage}')\n    # TODO: just known or also visited?\n    if known_links is not None:\n        URL_STORE.add_urls(urls=known_links, visited=True)\n    i = 0\n    # fetch and parse robots.txt file if necessary\n    if rules is None:\n        rules = urllib.robotparser.RobotFileParser()\n        rules.set_url(base_url + '/robots.txt')\n        # exceptions happening here\n        try:\n            rules.read()\n        except Exception as exc:\n            LOGGER.error('cannot read robots.txt: %s', exc)\n            rules = None\n    URL_STORE.store_rules(base_url, rules)\n    # initialize crawl by visiting homepage if necessary\n    if todo is None:\n        URL_STORE.add_urls(urls=[homepage], visited=False)\n        _, known_num, i = crawl_page(i, base_url, lang=language, rules=rules, initial=True)\n    else:\n        known_num = len(URL_STORE.find_known_urls(base_url))\n    is_on = bool(URL_STORE.find_unvisited_urls(base_url))\n    return base_url, i, known_num, rules, is_on\n\n\ndef crawl_page(visited_num, base_url, lang=None, rules=None, initial=False):\n    \"\"\"Examine a webpage, extract navigation links and links.\"\"\"\n    # config=DEFAULT_CONFIG\n    if not URL_STORE.is_exhausted_domain(base_url):\n        url = URL_STORE.get_url(base_url)\n        visited_num += 1\n        if initial is True:\n            # probe and process homepage\n            htmlstring, homepage, base_url = probe_alternative_homepage(url)\n            if all((htmlstring, homepage, base_url)):\n                # add potentially \"new\" homepage\n                if homepage != url:\n                    URL_STORE.add_urls([homepage])\n                # extract links on homepage\n                process_links(htmlstring, url=url, language=lang, rules=rules)\n        else:\n            response = fetch_url(url, decode=False)\n            process_response(response, base_url, lang, rules=rules)\n    # optional backup of gathered pages without nav-pages ? ...\n    is_on = bool(URL_STORE.find_unvisited_urls(base_url))\n    known_num = len(URL_STORE.find_known_urls(base_url))\n    return is_on, known_num, visited_num\n\n\n\n\n\n[docs]\n\ndef focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=None, known_links=None, lang=None, config=DEFAULT_CONFIG, rules=None):\n    \"\"\"Basic crawler targeting pages of interest within a website.\n\n    Args:\n        homepage: URL of the page to first page to fetch, preferably the homepage of a website.\n        max_seen_urls: maximum number of pages to visit, stop iterations at this number or at the exhaustion of pages on the website, whichever comes first.\n        max_known_urls: stop if the total number of pages \"known\" exceeds this number.\n        todo: provide a previously generated list of pages to visit / crawl frontier, must be in collections.deque format.\n        known_links: provide a previously generated set of links.\n        lang: try to target links according to language heuristics.\n        config: use a different configuration (configparser format).\n        rules: provide politeness rules (urllib.robotparser.RobotFileParser() format).\n\n    Returns:\n        List of pages to visit, deque format, possibly empty if there are no further pages to visit.\n        Set of known links.\n\n    \"\"\"\n    base_url, i, known_num, rules, is_on = init_crawl(homepage, todo, known_links, language=lang, rules=rules)\n    # visit pages until a limit is reached\n    while is_on and i < max_seen_urls and known_num <= max_known_urls:\n        is_on, known_num, i = crawl_page(i, base_url, lang=lang, rules=rules)\n        sleep(URL_STORE.get_crawl_delay(base_url, default=config.getfloat('DEFAULT', 'SLEEP_TIME')))\n    todo = set(URL_STORE.find_unvisited_urls(base_url))\n    # refocus todo-list on URLs without navigation?\n    # [u for u in todo if not is_navigation_page(u)]\n    known_links = set(URL_STORE.find_known_urls(base_url))\n    return todo, known_links\n\n\n\n\ndef is_still_navigation(todo):\n    \"\"\"Probe if there are still navigation URLs in the queue.\"\"\"\n    return any(is_navigation_page(url) for url in todo)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.metadata — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/metadata.html#extract_metadata",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.metadata\nSource code for trafilatura.metadata\n\"\"\"\nModule bundling all functions needed to scrape metadata from webpages.\n\"\"\"\n\nimport json\nimport logging\nimport re\nfrom copy import deepcopy\n\nfrom courlan import extract_domain, get_base_url, normalize_url, validate_url\nfrom htmldate import find_date\nfrom lxml.html import tostring\n\nfrom .htmlprocessing import prune_unwanted_nodes\nfrom .json_metadata import (extract_json, extract_json_parse_error,\n                            normalize_json)\nfrom .metaxpaths import (author_discard_xpaths, author_xpaths,\n                         categories_xpaths, tags_xpaths, title_xpaths)\nfrom .utils import (line_processing, load_html, normalize_authors,\n                    normalize_tags, trim, unescape, uniquify_list)\n\nLOGGER = logging.getLogger(__name__)\nlogging.getLogger('htmldate').setLevel(logging.WARNING)\n\n\nclass Document:\n    \"Defines a class to store all necessary data and metadata fields for extracted information.\"\n    __slots__ = [\n    'title', 'author', 'url', 'hostname', 'description', 'sitename',\n    'date', 'categories', 'tags', 'fingerprint', 'id', 'license',\n    'body', 'comments', 'commentsbody', 'raw_text', 'text',\n    'language', 'image', 'pagetype'  # 'locale'?\n    ]\n    # consider dataclasses for Python 3.7+\n    def __init__(self):\n        for slot in self.__slots__:\n            setattr(self, slot, None)\n\n    def set_attributes(self, title, author, url, description, site_name, image, pagetype, tags):\n        \"Helper function to (re-)set a series of attributes.\"\n        if title:\n            self.title = title\n        if author:\n            self.author = author\n        if url:\n            self.url = url\n        if description:\n            self.description = description\n        if site_name:\n            self.sitename = site_name\n        if image:\n            self.image = image\n        if pagetype:\n            self.pagetype = pagetype\n        if tags:\n            self.tags = tags\n\n    def clean_and_trim(self):\n        \"Limit text length and trim the attributes.\"\n        for slot in self.__slots__:\n            value = getattr(self, slot)\n            if isinstance(value, str):\n                # length\n                if len(value) > 10000:\n                    new_value = value[:9999] + '…'\n                    setattr(self, slot, new_value)\n                    value = new_value\n                # HTML entities, remove spaces and control characters\n                value = line_processing(unescape(value))\n                setattr(self, slot, value)\n\n    def as_dict(self):\n        \"Convert the document to a dictionary.\"\n        return {\n            attr: getattr(self, attr)\n            for attr in self.__slots__\n            if hasattr(self, attr)\n        }\n\n\nHTMLDATE_CONFIG_FAST = {'extensive_search': False, 'original_date': True}\nHTMLDATE_CONFIG_EXTENSIVE = {'extensive_search': True, 'original_date': True}\n\nJSON_MINIFY = re.compile(r'(\"(?:\\\\\"|[^\"])*\")|\\s')\n\nHTMLTITLE_REGEX = re.compile(r'^(.+)?\\s+[–•·—|⁄*⋆~‹«<›»>:-]\\s+(.+)$')  # part without dots?\nHTML_STRIP_TAG = re.compile(r'(<!--.*?-->|<[^>]*>)')\n\nLICENSE_REGEX = re.compile(r'/(by-nc-nd|by-nc-sa|by-nc|by-nd|by-sa|by|zero)/([1-9]\\.[0-9])')\nTEXT_LICENSE_REGEX = re.compile(r'(cc|creative commons) (by-nc-nd|by-nc-sa|by-nc|by-nd|by-sa|by|zero) ?([1-9]\\.[0-9])?', re.I)\n\nMETANAME_AUTHOR = {\n    'article:author', 'atc-metaauthor', 'author', 'authors', 'byl', 'citation_author',\n    'creator', 'dc.creator', 'dc.creator.aut', 'dc:creator',\n    'dcterms.creator', 'dcterms.creator.aut', 'dcsext.author', 'parsely-author',\n    'rbauthors', 'sailthru.author', 'shareaholic:article_author_name'\n}  # questionable: twitter:creator\nMETANAME_DESCRIPTION = {\n    'dc.description', 'dc:description',\n    'dcterms.abstract', 'dcterms.description',\n    'description', 'sailthru.description', 'twitter:description'\n}\nMETANAME_PUBLISHER = {\n    'article:publisher', 'citation_journal_title', 'copyright',\n    'dc.publisher', 'dc:publisher', 'dcterms.publisher',\n    'publisher', 'sailthru.publisher', 'rbpubname', 'twitter:site'\n}  # questionable: citation_publisher\nMETANAME_TAG = {\n    'citation_keywords', 'dcterms.subject', 'keywords', 'parsely-tags',\n    'shareaholic:keywords', 'tags'\n}\nMETANAME_TITLE = {\n    'citation_title', 'dc.title', 'dcterms.title', 'fb_title',\n    'headline', 'parsely-title', 'sailthru.title', 'shareaholic:title',\n    'rbtitle', 'title', 'twitter:title'\n}\nMETANAME_URL = {\n    'rbmainurl', 'twitter:url'\n}\nMETANAME_IMAGE = {\n    'image', 'og:image', 'og:image:url', 'og:image:secure_url',\n    'twitter:image', 'twitter:image:src'\n}\nOG_AUTHOR = {'og:author', 'og:article:author'}\nPROPERTY_AUTHOR = {'author', 'article:author'}\nTWITTER_ATTRS = {'twitter:site', 'application-name'}\n\n# also interesting: article:section\n\nEXTRA_META = {'charset', 'http-equiv', 'property'}\n\n\ndef check_authors(authors, author_blacklist):\n    \"Check if the authors string correspond to expected values.\"\n    author_blacklist = {a.lower() for a in author_blacklist}\n    new_authors = [\n        author.strip()\n        for author in authors.split(';')\n        if author.strip().lower() not in author_blacklist\n    ]\n    if new_authors:\n        return '; '.join(new_authors).strip('; ')\n    return None\n\n\ndef extract_meta_json(tree, metadata):\n    '''Parse and extract metadata from JSON-LD data'''\n    for elem in tree.xpath('.//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if not elem.text:\n            continue\n        element_text = normalize_json(JSON_MINIFY.sub(r'\\1', elem.text))\n        try:\n            schema = json.loads(element_text)\n            metadata = extract_json(schema, metadata)\n        except json.JSONDecodeError:\n            metadata = extract_json_parse_error(element_text, metadata)\n    return metadata\n\n\ndef extract_opengraph(tree):\n    '''Search meta tags following the OpenGraph guidelines (https://ogp.me/)'''\n    title, author, url, description, site_name, image, pagetype = (None,) * 7\n    # detect OpenGraph schema\n    for elem in tree.xpath('.//head/meta[starts-with(@property, \"og:\")]'):\n        # safeguard\n        if not elem.get('content'):\n            continue\n        # site name\n        if elem.get('property') == 'og:site_name':\n            site_name = elem.get('content')\n        # blog title\n        elif elem.get('property') == 'og:title':\n            title = elem.get('content')\n        # orig URL\n        elif elem.get('property') == 'og:url':\n            if validate_url(elem.get('content'))[0] is True:\n                url = elem.get('content')\n        # description\n        elif elem.get('property') == 'og:description':\n            description = elem.get('content')\n        # og:author\n        elif elem.get('property') in OG_AUTHOR:\n            author = normalize_authors(None, elem.get('content'))\n        # image default\n        elif elem.get('property') == 'og:image':\n            image = elem.get('content')\n        # image url\n        elif elem.get('property') == 'og:image:url':\n            image = elem.get('content')\n        # image secure url\n        elif elem.get('property') == 'og:image:secure_url':\n            image = elem.get('content')\n        # og:type\n        elif elem.get('property') == 'og:type':\n            pagetype = elem.get('content')\n        # og:locale\n        # elif elem.get('property') == 'og:locale':\n        #    pagelocale = elem.get('content')\n    return title, author, url, description, site_name, image, pagetype\n\n\ndef examine_meta(tree):\n    '''Search meta tags for relevant information'''\n    metadata = Document()  # alt: Metadata()\n    # bootstrap from potential OpenGraph tags\n    title, author, url, description, site_name, image, pagetype = extract_opengraph(tree)\n    # test if all values not assigned in the following have already been assigned\n    if all((title, author, url, description, site_name, image)):\n        metadata.set_attributes(title, author, url, description, site_name, image, pagetype, None)  # tags\n        return metadata\n    tags, backup_sitename = [], None\n    # skim through meta tags\n    for elem in tree.iterfind('.//head/meta[@content]'):\n        # content\n        if not elem.get('content'):\n            continue\n        content_attr = HTML_STRIP_TAG.sub('', elem.get('content'))\n        # image info\n        # ...\n        # property\n        if 'property' in elem.attrib:\n            # no opengraph a second time\n            if elem.get('property').startswith('og:'):\n                continue\n            if elem.get('property') == 'article:tag':\n                tags.append(normalize_tags(content_attr))\n            elif elem.get('property') in PROPERTY_AUTHOR:\n                author = normalize_authors(author, content_attr)\n            elif elem.get('property') == 'article:publisher':\n                site_name = site_name or content_attr\n            elif elem.get('property') in METANAME_IMAGE:\n                image = image or content_attr\n        # name attribute\n        elif 'name' in elem.attrib:\n            name_attr = elem.get('name').lower()\n            # author\n            if name_attr in METANAME_AUTHOR:\n                author = normalize_authors(author, content_attr)\n            # title\n            elif name_attr in METANAME_TITLE:\n                title = title or content_attr\n            # description\n            elif name_attr in METANAME_DESCRIPTION:\n                description = description or content_attr\n            # site name\n            elif name_attr in METANAME_PUBLISHER:\n                site_name = site_name or content_attr\n            # twitter\n            elif name_attr in TWITTER_ATTRS or 'twitter:app:name' in elem.get('name'):\n                backup_sitename = content_attr\n            # url\n            elif name_attr == 'twitter:url':\n                if url is None and validate_url(content_attr)[0] is True:\n                    url = content_attr\n            # keywords\n            elif name_attr in METANAME_TAG:  # 'page-topic'\n                tags.append(normalize_tags(content_attr))\n        elif 'itemprop' in elem.attrib:\n            if elem.get('itemprop') == 'author':\n                author = normalize_authors(author, content_attr)\n            elif elem.get('itemprop') == 'description':\n                description = description or content_attr\n            elif elem.get('itemprop') == 'headline':\n                title = title or content_attr\n            # to verify:\n            # elif elem.get('itemprop') == 'name':\n            #    if title is None:\n            #        title = elem.get('content')\n        # other types\n        elif all(\n            key not in elem.attrib\n            for key in EXTRA_META\n        ):\n            LOGGER.debug('unknown attribute: %s',\n                         tostring(elem, pretty_print=False, encoding='unicode').strip())\n    # backups\n    if site_name is None and backup_sitename is not None:\n        site_name = backup_sitename\n    # copy\n    metadata.set_attributes(title, author, url, description, site_name, image, pagetype, tags)\n    return metadata\n\n\ndef extract_metainfo(tree, expressions, len_limit=200):\n    '''Extract meta information'''\n    # try all XPath expressions\n    for expression in expressions:\n        # examine all results\n        i = 0\n        for elem in tree.xpath(expression):\n            content = trim(' '.join(elem.itertext()))\n            if content and 2 < len(content) < len_limit:\n                return content\n            i += 1\n        if i > 1:\n            LOGGER.debug('more than one invalid result: %s %s', expression, i)\n    return None\n\n\ndef examine_title_element(tree):\n    '''Extract text segments out of main <title> element.'''\n    title, first, second = None, None, None\n    try:\n        title = trim(tree.xpath('.//head//title')[0].text_content())\n        mymatch = HTMLTITLE_REGEX.match(title)\n        if mymatch is not None:\n            first = mymatch[1] or None\n            second = mymatch[2] or None\n    except IndexError:\n        LOGGER.debug('no main title found')\n    return title, first, second\n\n\ndef extract_title(tree):\n    '''Extract the document title'''\n    # only one h1-element: take it\n    h1_results = tree.findall('.//h1')\n    if len(h1_results) == 1:\n        title = trim(h1_results[0].text_content())\n        if len(title) > 0:\n            return title\n    # extract using x-paths\n    title = extract_metainfo(tree, title_xpaths)\n    if title is not None:\n        return title\n    # extract using title tag\n    title, first, second = examine_title_element(tree)\n    if first is not None and '.' not in first:\n        return first\n    if second is not None and '.' not in second:\n        return second\n    # take first h1-title\n    if h1_results:\n        return h1_results[0].text_content()\n    # take first h2-title\n    try:\n        title = tree.xpath('.//h2')[0].text_content()\n    except IndexError:\n        LOGGER.debug('no h2 title found')\n    return title\n\n\ndef extract_author(tree):\n    '''Extract the document author(s)'''\n    subtree = prune_unwanted_nodes(deepcopy(tree), author_discard_xpaths)\n    author = extract_metainfo(subtree, author_xpaths, len_limit=120)\n    if author:\n        author = normalize_authors(None, author)\n    # copyright?\n    return author\n\n\ndef extract_url(tree, default_url=None):\n    '''Extract the URL from the canonical link'''\n    url = None\n    # https://www.tutorialrepublic.com/html-reference/html-base-tag.php\n    # try canonical link first\n    element = tree.find('.//head//link[@rel=\"canonical\"][@href]')\n    if element is not None:\n        url = element.attrib['href']\n    # try default language link\n    else:\n        for element in tree.iterfind('.//head//link[@rel=\"alternate\"][@hreflang]'):\n            if element.attrib['hreflang'] == 'x-default':\n                LOGGER.debug(tostring(element, pretty_print=False, encoding='unicode').strip())\n                url = element.attrib['href']\n    # add domain name if it's missing\n    if url is not None and url.startswith('/'):\n        for element in tree.iterfind('.//head//meta[@content]'):\n            if 'name' in element.attrib:\n                attrtype = element.attrib['name']\n            elif 'property' in element.attrib:\n                attrtype = element.attrib['property']\n            else:\n                continue\n            if attrtype.startswith('og:') or attrtype.startswith('twitter:'):\n                base_url = get_base_url(element.attrib['content'])\n                if base_url:\n                    # prepend URL\n                    url = base_url + url\n                    break\n    # sanity check: don't return invalid URLs\n    if url is not None:\n        validation_result, parsed_url = validate_url(url)\n        url = None if validation_result is False else normalize_url(parsed_url)\n    return url or default_url\n\n\ndef extract_sitename(tree):\n    '''Extract the name of a site from the main title (if it exists)'''\n    _, first, second = examine_title_element(tree)\n    if first is not None and '.' in first:\n        return first\n    if second is not None and '.' in second:\n        return second\n    return None\n\n\ndef extract_catstags(metatype, tree):\n    '''Find category and tag information'''\n    results = []\n    regexpr = '/' + metatype + '[s|ies]?/'\n    xpath_expression = categories_xpaths if metatype == 'category' else tags_xpaths\n    # search using custom expressions\n    for catexpr in xpath_expression:\n        results.extend(\n            elem.text_content()\n            for elem in tree.xpath(catexpr)\n            if re.search(regexpr, elem.attrib['href'])\n        )\n        if results:\n            break\n    # category fallback\n    if metatype == 'category' and not results:\n        for element in tree.xpath('.//head//meta[@property=\"article:section\" or contains(@name, \"subject\")][@content]'):\n            results.append(element.attrib['content'])\n        # optional: search through links\n        #if not results:\n        #    for elem in tree.xpath('.//a[@href]'):\n        #        search for 'category'\n    results = [line_processing(x) for x in results if x is not None]\n    return uniquify_list([x for x in results if x is not None])\n\n\ndef parse_license_element(element, strict=False):\n    '''Probe a link for identifiable free license cues.\n       Parse the href attribute first and then the link text.'''\n   # look for Creative Commons elements\n    match = LICENSE_REGEX.search(element.get('href'))\n    if match:\n        return 'CC ' + match[1].upper() + ' ' + match[2]\n    if element.text is not None:\n        # just return the anchor text without further ado\n        if strict is False:\n            return trim(element.text)\n        # else: check if it could be a CC license\n        match = TEXT_LICENSE_REGEX.search(element.text)\n        if match:\n            return match[0]\n    return None\n\n\ndef extract_license(tree):\n    '''Search the HTML code for license information and parse it.'''\n    result = None\n    # look for links labeled as license\n    for element in tree.findall('.//a[@rel=\"license\"][@href]'):\n        result = parse_license_element(element, strict=False)\n        if result is not None:\n            break\n    # probe footer elements for CC links\n    if result is None:\n        for element in tree.xpath(\n            './/footer//a[@href]|.//div[contains(@class, \"footer\") or contains(@id, \"footer\")]//a[@href]'\n        ):\n            result = parse_license_element(element, strict=True)\n            if result is not None:\n                break\n    return result\n\n\ndef extract_image(tree):\n    '''Search meta tags following the OpenGraph guidelines (https://ogp.me/)\n       and search meta tags with Twitter Image'''\n\n    for elem in tree.xpath('.//head/meta[@property=\"og:image\" or @property=\"og:image:url\"][@content]'):\n        return elem.get('content')\n\n    for elem in tree.xpath('.//head/meta[@property=\"twitter:image\" or @property=\"twitter:image:src\"][@content]'):\n        return elem.get('content')\n\n    return None\n\n\n\n\n\n[docs]\n\ndef extract_metadata(filecontent, default_url=None, date_config=None, fastmode=False, author_blacklist=None):\n    \"\"\"Main process for metadata extraction.\n\n    Args:\n        filecontent: HTML code as string.\n        default_url: Previously known URL of the downloaded document.\n        date_config: Provide extraction parameters to htmldate as dict().\n        author_blacklist: Provide a blacklist of Author Names as set() to filter out authors.\n\n    Returns:\n        A trafilatura.metadata.Document containing the extracted metadata information or None.\n        trafilatura.metadata.Document has .as_dict() method that will return a copy as a dict.\n    \"\"\"\n    # init\n    if author_blacklist is None:\n        author_blacklist = set()\n    # load contents\n    tree = load_html(filecontent)\n    if tree is None:\n        return None\n    # initialize dict and try to strip meta tags\n    metadata = examine_meta(tree)\n    # to check: remove it and replace with author_blacklist in test case\n    if metadata.author is not None and ' ' not in metadata.author:\n        metadata.author = None\n    # fix: try json-ld metadata and override\n    try:\n        metadata = extract_meta_json(tree, metadata)\n    # todo: fix bugs in json_metadata.py\n    except TypeError as err:\n        LOGGER.warning('error in JSON metadata extraction: %s', err)\n    # title\n    if metadata.title is None:\n        metadata.title = extract_title(tree)\n    # check author in blacklist\n    if metadata.author is not None and len(author_blacklist) > 0:\n        metadata.author = check_authors(metadata.author, author_blacklist)\n    # author\n    if metadata.author is None:\n        metadata.author = extract_author(tree)\n    # recheck author in blacklist\n    if metadata.author is not None and len(author_blacklist) > 0:\n        metadata.author = check_authors(metadata.author, author_blacklist)\n    # url\n    if metadata.url is None:\n        metadata.url = extract_url(tree, default_url)\n    # hostname\n    if metadata.url is not None:\n        metadata.hostname = extract_domain(metadata.url, fast=True)\n    # image\n    if metadata.image is None:\n        metadata.image = extract_image(tree)\n    # extract date with external module htmldate\n    if date_config is None:\n        # decide on fast mode\n        if fastmode is False:\n            date_config = HTMLDATE_CONFIG_EXTENSIVE\n        else:\n            date_config = HTMLDATE_CONFIG_FAST\n    date_config['url'] = metadata.url\n    metadata.date = find_date(tree, **date_config)\n    # sitename\n    if metadata.sitename is None:\n        metadata.sitename = extract_sitename(tree)\n    if metadata.sitename is not None:\n        # fix: take 1st element (['Westdeutscher Rundfunk'])\n        if isinstance(metadata.sitename, list):\n            metadata.sitename = metadata.sitename[0]\n        # hotfix: probably an error coming from json_metadata (#195)\n        elif isinstance(metadata.sitename, dict):\n            metadata.sitename = str(metadata.sitename)\n        if metadata.sitename.startswith('@'):\n            # scrap Twitter ID\n            metadata.sitename = re.sub(r'^@', '', metadata.sitename)\n        # capitalize\n        try:\n            if (\n                '.' not in metadata.sitename\n                and not metadata.sitename[0].isupper()\n            ):\n                metadata.sitename = metadata.sitename.title()\n        # fix for empty name\n        except IndexError as err:\n            LOGGER.warning('error in sitename extraction: %s', err)\n    # use URL\n    elif metadata.url:\n        mymatch = re.match(r'https?://(?:www\\.|w[0-9]+\\.)?([^/]+)', metadata.url)\n        if mymatch:\n            metadata.sitename = mymatch[1]\n    # categories\n    if not metadata.categories:\n        metadata.categories = extract_catstags('category', tree)\n    # tags\n    if not metadata.tags:\n        metadata.tags = extract_catstags('tag', tree)\n    # license\n    metadata.license = extract_license(tree)\n    # safety checks\n    metadata.clean_and_trim()\n    # return result\n    return metadata\n\n\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.sitemaps — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/sitemaps.html#sitemap_search",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.sitemaps\nSource code for trafilatura.sitemaps\n\"\"\"\nDeriving link info from sitemaps.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\n\nimport logging\nimport re\nfrom itertools import islice\nfrom typing import List, Optional\n\nfrom courlan import (clean_url, extract_domain, filter_urls, fix_relative_urls,\n                     get_hostinfo, lang_filter)\n\nfrom .downloads import fetch_url, is_live_page\nfrom .settings import MAX_LINKS, MAX_SITEMAPS_SEEN\nfrom .utils import is_similar_domain\n\n# import urllib.robotparser # Python >= 3.8\n# ROBOT_PARSER = urllib.robotparser.RobotFileParser()\n\n\n\n\n\nLOGGER = logging.getLogger(__name__)\n\nLINK_REGEX = re.compile(r'<loc>(?:<!\\[CDATA\\[)?(http.+?)(?:\\]\\]>)?</loc>')\nXHTML_REGEX = re.compile(r'<xhtml:link.+?>', re.DOTALL)\nHREFLANG_REGEX = re.compile(r'href=[\"\\'](.+?)[\"\\']')\nWHITELISTED_PLATFORMS = re.compile(r'(?:blogger|blogpost|ghost|hubspot|livejournal|medium|typepad|squarespace|tumblr|weebly|wix|wordpress)\\.')\n\nSITEMAP_FORMAT = re.compile(r'^.{0,5}<\\?xml|<sitemap|<urlset')\nDETECT_SITEMAP_LINK = re.compile(r'\\.xml(\\..{2,4})?$|\\.xml[?#]')\nDETECT_LINKS = re.compile(r'https?://[^\\s<\"]+')\nSCRUB_REGEX = re.compile(r'\\?.*$|#.*$')\nPOTENTIAL_SITEMAP = re.compile(r'\\.xml\\b')  # |\\bsitemap\\b\n\nGUESSES = ['sitemap.xml.gz', 'sitemap', 'sitemap_index.xml', 'sitemap_news.xml']\n\n\nclass SitemapObject:\n    \"Store all necessary information on sitemap download and processing.\"\n    __slots__ = [\"base_url\", \"content\", \"domain\", \"sitemap_url\", \"sitemap_urls\", \"target_lang\", \"urls\"]\n\n    def __init__(self, base_url: str, domain: str, sitemap_url: str, target_lang: Optional[str] = None) -> None:\n        self.base_url: str = base_url\n        self.content: str = \"\"\n        self.domain: str = domain\n        self.sitemap_url: str = sitemap_url\n        self.sitemap_urls: List[str] = []\n        self.target_lang: Optional[str] = target_lang\n        self.urls: List[str] = []\n\n    def fetch(self) -> None:\n        \"Fetch a sitemap over the network.\"\n        LOGGER.debug('fetching sitemap: %s', self.sitemap_url)\n        self.content = fetch_url(self.sitemap_url)\n\n    def handle_link(self, link: str) -> None:\n        \"\"\"Examine a link and determine if it's valid and if it leads to\n           a sitemap or a web page.\"\"\"\n        if link == self.sitemap_url:  # safety check\n            return\n        # fix, check, clean and normalize\n        link = fix_relative_urls(self.base_url, link)\n        link = clean_url(link, self.target_lang)\n\n        if link is None or not lang_filter(link, self.target_lang):\n            return\n\n        newdomain = extract_domain(link, fast=True)\n        if newdomain is None:\n            LOGGER.error(\"couldn't extract domain: %s\", link)\n            return\n\n        # don't take links from another domain and make an exception for main platforms\n        # also bypass: subdomains vs. domains\n        if not is_similar_domain(self.domain, newdomain) and not WHITELISTED_PLATFORMS.search(newdomain):\n            LOGGER.warning('link discarded, diverging domain names: %s %s', self.domain, newdomain)\n            return\n\n        if DETECT_SITEMAP_LINK.search(link):\n            self.sitemap_urls.append(link)\n        else:\n            self.urls.append(link)\n\n    def extract_sitemap_langlinks(self) -> None:\n        \"Extract links corresponding to a given target language.\"\n        if 'hreflang=' not in self.content:\n            return\n        # compile regex here for modularity and efficiency\n        lang_regex = re.compile(rf\"hreflang=[\\\"']({self.target_lang}.*?|x-default)[\\\"']\", re.DOTALL)\n        # extract\n        for attrs in (m[0] for m in islice(XHTML_REGEX.finditer(self.content), MAX_LINKS)):\n            if lang_regex.search(attrs):\n                lang_match = HREFLANG_REGEX.search(attrs)\n                if lang_match:\n                    self.handle_link(lang_match[1])\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n\n    def extract_sitemap_links(self) -> None:\n        \"Extract sitemap links and web page links from a sitemap file.\"\n        # extract\n        for match in (m[1] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            # process middle part of the match tuple\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n\n    def process(self) -> None:\n        \"Download a sitemap and extract the links it contains.\"\n        plausible = is_plausible_sitemap(self.sitemap_url, self.content)\n        # safeguard\n        if not plausible:\n            return\n        # try to extract links from TXT file\n        if not SITEMAP_FORMAT.match(self.content):\n            for match in (m[0] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n                self.handle_link(match)\n            return\n        # process XML sitemap\n        if self.target_lang is not None:\n            self.extract_sitemap_langlinks()\n            if self.sitemap_urls or self.urls:\n                return\n        self.extract_sitemap_links()\n\n\n\n\n\n[docs]\n\ndef sitemap_search(url: str, target_lang: Optional[str] = None) -> List[str]:\n    \"\"\"Look for sitemaps for the given URL and gather links.\n\n    Args:\n        url: Webpage or sitemap URL as string.\n             Triggers URL-based filter if the webpage isn't a homepage.\n        target_lang: Define a language to filter URLs based on heuristics\n             (two-letter string, ISO 639-1 format).\n\n    Returns:\n        The extracted links as a list (sorted list of unique links).\n\n    \"\"\"\n    domainname, baseurl = get_hostinfo(url)\n    if domainname is None:\n        LOGGER.warning('invalid URL: %s', url)\n        return []\n\n    if not is_live_page(baseurl):\n        LOGGER.warning('base URL unreachable, dropping sitemap: %s', url)\n        return []\n\n    urlfilter = None\n    if url.endswith(('.gz', 'sitemap', '.xml')):\n        sitemapurl = url\n    else:\n        sitemapurl = baseurl + '/sitemap.xml'\n        # filter triggered, prepare it\n        if len(url) > len(baseurl) + 2:\n            urlfilter = url\n\n    sitemap = SitemapObject(baseurl, domainname, sitemapurl, target_lang)\n    sitemap.fetch()\n    sitemap.process()\n\n    if not sitemap.sitemap_urls and sitemap.urls:\n        linklist = filter_urls(sitemap.urls, urlfilter)\n        LOGGER.debug('%s sitemap links found for %s', len(linklist), domainname)\n        return linklist\n\n    # try sitemaps in robots.txt file if nothing has been found\n    if not sitemap.sitemap_urls and not sitemap.urls:\n        sitemap.sitemap_urls = find_robots_sitemaps(baseurl)\n        # try additional URLs just in case\n        if not sitemap.sitemap_urls:\n            sitemap.sitemap_urls = [''.join([baseurl, '/', g]) for g in GUESSES]\n\n    # iterate through nested sitemaps and results\n    seen = {sitemapurl}\n    i = 1\n    while sitemap.sitemap_urls:\n        sitemap.sitemap_url = sitemap.sitemap_urls.pop()\n        sitemap.fetch()\n        sitemap.process()\n        # sanity check: keep track of visited sitemaps and exclude them\n        seen.add(sitemap.sitemap_url)\n        sitemap.sitemap_urls = [s for s in sitemap.sitemap_urls if s not in seen]\n        # counter and safeguard\n        i += 1\n        if i > MAX_SITEMAPS_SEEN:\n            break\n\n    sitemap.urls = filter_urls(sitemap.urls, urlfilter)\n    LOGGER.debug('%s sitemap links found for %s', len(sitemap.urls), domainname)\n    return sitemap.urls\n\n\n\n\ndef is_plausible_sitemap(url: str, contents: Optional[str]) -> bool:\n    '''Check if the sitemap corresponds to an expected format,\n       i.e. TXT or XML.'''\n    if contents is None:\n        return False\n\n    # strip query and fragments\n    url = SCRUB_REGEX.sub('', url)\n\n    # check content\n    if POTENTIAL_SITEMAP.search(url) and \\\n        (not isinstance(contents, str) or not SITEMAP_FORMAT.match(contents)) \\\n         or '<html' in contents[:150].lower():\n        LOGGER.warning('not a valid XML sitemap: %s', url)\n        return False\n\n    return True\n\n\ndef find_robots_sitemaps(baseurl: str) -> List[str]:\n    '''Guess the location of the robots.txt file and try to extract\n       sitemap URLs from it'''\n    robotstxt = fetch_url(baseurl + '/robots.txt')\n    return extract_robots_sitemaps(robotstxt, baseurl)\n\n\ndef extract_robots_sitemaps(robotstxt: str, baseurl: str) -> List[str]:\n    'Read a robots.txt file and find sitemap links.'\n    # sanity check on length (cause: redirections)\n    if robotstxt is None or len(robotstxt) > 10000:\n        return []\n    sitemapurls = []\n    # source: https://github.com/python/cpython/blob/3.8/Lib/urllib/robotparser.py\n    for line in robotstxt.splitlines():\n        # remove optional comment and strip line\n        i = line.find('#')\n        if i >= 0:\n            line = line[:i]\n        line = line.strip()\n        if not line:\n            continue\n        line = line.split(':', 1)\n        if len(line) == 2:\n            line[0] = line[0].strip().lower()\n            if line[0] == \"sitemap\":\n                # urllib.parse.unquote(line[1].strip())\n                candidate = fix_relative_urls(baseurl, line[1].strip())\n                sitemapurls.append(candidate)\n    LOGGER.debug('%s sitemaps found in robots.txt', len(sitemapurls))\n    return sitemapurls\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.external — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/external.html#try_readability",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.external\nSource code for trafilatura.external\n# pylint:disable-msg=E0611,I1101\n\"\"\"\nFunctions grounding on third-party software.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\n\nimport logging\nimport lzma\nfrom pathlib import Path\nfrom pickle import load as load_pickle\n\n# third-party\nfrom justext.core import (ParagraphMaker, classify_paragraphs, preprocessor,\n                          revise_paragraph_classification)\nfrom justext.utils import get_stoplist  # , get_stoplists\nfrom lxml.etree import Element, strip_tags\n\n# own\nfrom .htmlprocessing import convert_tags, prune_unwanted_nodes, tree_cleaning\nfrom .readability_lxml import Document as ReadabilityDocument  # fork\nfrom .settings import JUSTEXT_LANGUAGES\nfrom .utils import fromstring_bytes, trim\nfrom .xml import TEI_VALID_TAGS\nfrom .xpaths import PAYWALL_DISCARD_XPATH, REMOVE_COMMENTS_XPATH\n\nLOGGER = logging.getLogger(__name__)\n\nJT_STOPLIST = None\nJT_PICKLE = str(Path(__file__).parent / 'data/jt-stopwords-pickle.lzma')\n\nSANITIZED_XPATH = './/aside|.//audio|.//button|.//fieldset|.//figure|.//footer|.//iframe|.//input|.//label|.//link|.//nav|.//noindex|.//noscript|.//object|.//option|.//select|.//source|.//svg|.//time'\n\n\n\n\n\n[docs]\n\ndef try_readability(htmlinput):\n    '''Safety net: try with the generic algorithm readability'''\n    # defaults: min_text_length=25, retry_length=250\n    try:\n        doc = ReadabilityDocument(htmlinput, min_text_length=25, retry_length=250)\n        # force conversion to utf-8 (see #319)\n        return fromstring_bytes(doc.summary())\n    except Exception as err:\n        LOGGER.warning('readability_lxml failed: %s', err)\n        return Element('div')\n\n\n\n\ndef jt_stoplist_init():\n    'Retrieve and return the content of all JusText stoplists'\n    global JT_STOPLIST\n    with lzma.open(JT_PICKLE, 'rb') as picklefile:\n        JT_STOPLIST = load_pickle(picklefile)\n    # stoplist = set()\n    # for language in get_stoplists():\n    #     stoplist.update(get_stoplist(language))\n    # JT_STOPLIST = tuple(stoplist)\n    return JT_STOPLIST\n\n\ndef custom_justext(tree, stoplist):\n    'Customized version of JusText processing'\n    dom = preprocessor(tree)  # tree_cleaning(tree, True)\n    paragraphs = ParagraphMaker.make_paragraphs(dom)\n    classify_paragraphs(paragraphs, stoplist, 50, 200, 0.1, 0.2, 0.2, True)\n    revise_paragraph_classification(paragraphs, 200)\n    return paragraphs\n\n\n\n\n\n[docs]\n\ndef try_justext(tree, url, target_language):\n    '''Second safety net: try with the generic algorithm justext'''\n    # init\n    result_body = Element('body')\n    # determine language\n    if target_language is not None and target_language in JUSTEXT_LANGUAGES:\n        justext_stoplist = get_stoplist(JUSTEXT_LANGUAGES[target_language])\n    else:\n        justext_stoplist = JT_STOPLIST or jt_stoplist_init()\n    # extract\n    try:\n        paragraphs = custom_justext(tree, justext_stoplist)\n    except ValueError as err:  # not an XML element: HtmlComment\n        LOGGER.error('justext %s %s', err, url)\n        result_body = None\n    else:\n        for paragraph in [p for p in paragraphs if not p.is_boilerplate]:\n            #if duplicate_test(paragraph) is not True:\n            elem, elem.text = Element('p'), paragraph.text\n            result_body.append(elem)\n    return result_body\n\n\n\n\ndef justext_rescue(tree, url, target_language, postbody, len_text, text):\n    '''Try to use justext algorithm as a second fallback'''\n    result_bool = False\n    # additional cleaning\n    tree = prune_unwanted_nodes(tree, PAYWALL_DISCARD_XPATH)\n    tree = prune_unwanted_nodes(tree, REMOVE_COMMENTS_XPATH)\n    # proceed\n    temppost_algo = try_justext(tree, url, target_language)\n    if temppost_algo is not None:\n        temp_text = trim(' '.join(temppost_algo.itertext()))\n        len_algo = len(temp_text)\n        if len_algo > len_text:\n            postbody, text, len_text = temppost_algo, temp_text, len_algo\n            result_bool = True\n    return postbody, text, len_text, result_bool\n\n\ndef sanitize_tree(tree, options):\n    '''Convert and sanitize the output from the generic algorithm (post-processing)'''\n    # 1. clean\n    cleaned_tree = tree_cleaning(tree, options)\n    for elem in tree.findall(SANITIZED_XPATH):\n        elem.getparent().remove(elem)\n    if options.links is False:\n        strip_tags(cleaned_tree, 'a')\n    strip_tags(cleaned_tree, 'span')\n    # 2. convert\n    cleaned_tree = convert_tags(cleaned_tree, options)\n    for elem in cleaned_tree.iter('td', 'th', 'tr'):\n        # elem.text, elem.tail = trim(elem.text), trim(elem.tail)\n        # finish table conversion\n        if elem.tag == 'tr':\n            elem.tag = 'row'\n        elif elem.tag in ('td', 'th'):\n            if elem.tag == 'th':\n                elem.set('role', 'head')\n            elem.tag = 'cell'\n    # 3. sanitize\n    sanitization_list = [\n        tagname\n        for tagname in [element.tag for element in set(cleaned_tree.iter('*'))]\n        if tagname not in TEI_VALID_TAGS\n    ]\n    strip_tags(cleaned_tree, *sanitization_list)\n    # 4. return\n    text = trim(' '.join(cleaned_tree.itertext()))\n    return cleaned_tree, text, len(text)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "trafilatura.core — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/_modules/trafilatura/core.html#extract",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nModule code\ntrafilatura.core\nSource code for trafilatura.core\n# pylint:disable-msg=E0611,I1101\n\"\"\"\nModule bundling all functions needed to extract the text in a webpage.\n\"\"\"\n\n## This file is available from https://github.com/adbar/trafilatura\n## under GNU GPL v3 license\n\n\n# standard\nimport logging\nimport re  # import regex as re\nimport warnings\nfrom copy import deepcopy\n\nfrom lxml.etree import Element, SubElement, strip_elements, strip_tags\nfrom lxml.html import tostring\n\n# own\nfrom .external import (SANITIZED_XPATH, justext_rescue, sanitize_tree,\n                       try_readability)\nfrom .filters import (LANGID_FLAG, check_html_lang, duplicate_test,\n                      language_filter, text_chars_test)\nfrom .hashing import content_fingerprint\nfrom .htmlprocessing import (convert_tags, delete_by_link_density,\n                             handle_textnode, link_density_test_tables,\n                             process_node, prune_unwanted_nodes, tree_cleaning)\nfrom .metadata import Document, extract_metadata\nfrom .settings import DEFAULT_CONFIG, TAG_CATALOG, use_config\nfrom .utils import is_image_file, load_html, normalize_unicode, trim, txttocsv\nfrom .xml import (build_json_output, build_tei_output, build_xml_output,\n                  control_xml_output, remove_empty_elements, strip_double_tags,\n                  xmltotxt)\nfrom .xpaths import (BODY_XPATH, COMMENTS_DISCARD_XPATH, COMMENTS_XPATH,\n                     DISCARD_IMAGE_ELEMENTS, OVERALL_DISCARD_XPATH,\n                     PAYWALL_DISCARD_XPATH, PRECISION_DISCARD_XPATH,\n                     REMOVE_COMMENTS_XPATH, TEASER_DISCARD_XPATH)\n\nLOGGER = logging.getLogger(__name__)\n\nFORMATTING_PROTECTED = {'cell', 'head', 'hi', 'item', 'p', 'quote', 'td'}\nSPACING_PROTECTED = {'code', 'hi', 'ref'}\nP_FORMATTING = {'hi', 'ref'}\nTABLE_ELEMS = {'td', 'th'}\nTABLE_ALL = {'td', 'th', 'hi'}\nFORMATTING = {'hi', 'ref', 'span'}\nCODES_QUOTES = {'code', 'quote'}\nNOT_AT_THE_END = {'head', 'ref'}\n\nJSON_SEARCH = re.compile(r'\"articlebody\": *\"(.+?)(?<!\\\\)\"', re.I)\n\n\nclass Extractor:\n    \"Defines a class to store all extraction options.\"\n    __slots__ = [\n    'config', 'fast', 'precision', 'recall', 'comments',\n    'formatting', 'links', 'images', 'tables', 'dedup', 'lang',\n    ]\n    # consider dataclasses for Python 3.7+\n    def __init__(self, config, fast, precision, recall, comments,\n                 formatting, links, images, tables, deduplicate,\n                 target_language):\n        self.config = config\n        self.fast = fast\n        self.precision = precision\n        self.recall = recall\n        self.comments = comments\n        self.formatting = formatting\n        self.links = links\n        self.images = images\n        self.tables = tables\n        self.dedup = deduplicate\n        self.lang = target_language\n\n\ndef handle_titles(element, options):\n    '''Process head elements (titles)'''\n    if len(element) == 0:\n        # maybe needs attention?\n        # if element.tail and re.search(r'\\w', element.tail):\n        #    LOGGER.debug('tail in title, stripping: %s', element.tail)\n        #    element.tail = None\n        title = process_node(element, options)\n    # children\n    else:\n        title = deepcopy(element)\n        # list instead of element.iter('*')\n        # TODO: write tests for it and check\n        for child in list(element):\n            # if child.tag not in potential_tags:\n            #    LOGGER.debug('unexpected in title: %s %s %s', child.tag, child.text, child.tail)\n            #    continue\n            processed_child = handle_textnode(child, options, comments_fix=False)\n            if processed_child is not None:\n                title.append(processed_child)\n            child.tag = 'done'\n    if title is not None and text_chars_test(''.join(title.itertext())) is True:\n        return title\n    return None\n\n\ndef handle_formatting(element, options):\n    '''Process formatting elements (b, i, etc. converted to hi) found\n       outside of paragraphs'''\n    formatting = process_node(element, options)\n    if len(element) == 0 and formatting is None:\n        return None\n    # repair orphan elements\n    # if formatting is None:\n    #    formatting = Element(element.tag)\n    #     return None\n    # if len(element) > 0:\n    #    for child in element.iter('*'):\n    #        if child.tag not in potential_tags:\n    #            LOGGER.debug('unexpected in title: %s %s %s', child.tag, child.text, child.tail)\n    #            continue\n    #        processed_child = handle_textnode(child, options, comments_fix=False)\n    #        if processed_child is not None:\n    #            formatting.append(processed_child)\n    #        child.tag = 'done'\n    # if text_chars_test(element.text) is True:\n    #    processed_child.text = trim(element.text)\n    # if text_chars_test(element.tail) is True:\n    #    processed_child.tail = trim(element.tail)\n    # if len(element) == 0:\n    #    processed_element = process_node(element, options)\n    # children\n    # else:\n    #    processed_element = Element(element.tag)\n    #    processed_element.text, processed_element.tail = element.text, element.tail\n    #    for child in element.iter('*'):\n    #        processed_child = handle_textnode(child, options, comments_fix=False)\n    #        if processed_child is not None:\n    #            processed_element.append(processed_child)\n    #        child.tag = 'done'\n    # repair orphan elements\n    # shorter code but triggers warning:\n    # parent = element.getparent() or element.getprevious()\n    parent = element.getparent()\n    if parent is None:\n        parent = element.getprevious()\n    if parent is None or parent.tag not in FORMATTING_PROTECTED:\n        processed_element = Element('p')\n        processed_element.insert(0, formatting)\n    else:\n        processed_element = formatting\n    return processed_element\n\n\ndef handle_lists(element, options):\n    '''Process lists elements'''\n    processed_element = Element(element.tag)\n    if element.text is not None and element.text.strip():\n        newchildelem = SubElement(processed_element, \"item\")\n        newchildelem.text = element.text\n    # if element.tail is not None:\n    #    processed_element.tail = element.text\n    for child in element.iter('item'):\n        newchildelem = Element('item')\n        if len(child) == 0:\n            processed_child = process_node(child, options)\n            if processed_child is not None:\n                newchildelem.text = processed_child.text\n                if processed_child.tail is not None and processed_child.tail.strip():\n                    newchildelem.text += \" \" + processed_child.tail\n                processed_element.append(newchildelem)\n        else:\n            newchildelem.text = child.text\n            # proceed with iteration, fix for nested elements\n            for subelem in child.iterdescendants('*'):\n                # beware of nested lists\n                if subelem.tag == 'list':\n                    processed_subchild = handle_lists(subelem, options)\n                    if processed_subchild is not None:\n                        newchildelem.append(processed_subchild)\n                else:\n                    processed_subchild = handle_textnode(subelem, options, comments_fix=False)\n                    # add child element to processed_element\n                    if processed_subchild is not None:\n                        subchildelem = SubElement(newchildelem, processed_subchild.tag)\n                        subchildelem.text, subchildelem.tail = processed_subchild.text, processed_subchild.tail\n                        # set attributes\n                        for attr in subelem.attrib:\n                            subchildelem.set(attr, subelem.get(attr))\n                # strip_tags(newchildelem, 'item')\n                subelem.tag = 'done'\n            if child.tail is not None and child.tail.strip():\n                newchildelem_children = [el for el in newchildelem.getchildren() if el.tag != 'done']\n                if newchildelem_children:\n                    last_subchild = newchildelem_children[-1]\n                    if last_subchild.tail is None or not last_subchild.tail.strip():\n                        last_subchild.tail = child.tail\n                    else:\n                        last_subchild.tail += ' ' + child.tail\n        if newchildelem.text or len(newchildelem) > 0:\n            # set attribute\n            if child.get('rend') is not None:\n                newchildelem.set('rend', child.get('rend'))\n            processed_element.append(newchildelem)\n        child.tag = 'done'\n    element.tag = 'done'\n    # test if it has children and text. Avoid double tags??\n    if len(processed_element) > 0 and text_chars_test(''.join(processed_element.itertext())) is True:\n        # set attribute\n        if element.get('rend') is not None:\n            processed_element.set('rend', element.get('rend'))\n        return processed_element\n    return None\n\n\ndef is_code_block_element(element):\n    # pip\n    if element.get('lang') is not None or element.tag == 'code':\n        return True\n    # GitHub\n    parent = element.getparent()\n    if parent is not None and 'highlight' in parent.get('class', default=''):\n        return True\n    # highlightjs\n    code = element.find('code')\n    if code is not None and len(element.getchildren()) == 1:\n        return True\n    return False\n\n\ndef handle_code_blocks(element):\n    processed_element = deepcopy(element)\n    for child in element.iter('*'):\n        child.tag = 'done'\n    processed_element.tag = 'code'\n    return processed_element\n\n\ndef handle_quotes(element, options):\n    '''Process quotes elements'''\n    if is_code_block_element(element):\n        return handle_code_blocks(element)\n\n    processed_element = Element(element.tag)\n    for child in element.iter('*'):\n        processed_child = process_node(child, options)  # handle_textnode(child, comments_fix=True)\n        if processed_child is not None:\n            newsub = SubElement(processed_element, child.tag)\n            newsub.text, newsub.tail = processed_child.text, processed_child.tail\n        child.tag = 'done'\n    if len(processed_element) > 0 and text_chars_test(''.join(processed_element.itertext())) is True:\n        # avoid double/nested tags\n        strip_tags(processed_element, 'quote')\n        return processed_element\n    return None\n\n\ndef handle_other_elements(element, potential_tags, options):\n    '''Handle diverse or unknown elements in the scope of relevant tags'''\n    # handle w3schools code\n    if element.tag == 'div' and 'w3-code' in element.get('class', default=''):\n        return handle_code_blocks(element)\n    # delete unwanted\n    if element.tag not in potential_tags:\n        if element.tag != 'done':\n            LOGGER.debug('discarding element: %s %s', element.tag, element.text)\n        return None\n    if element.tag == 'div':\n        # make a copy and prune it in case it contains sub-elements handled on their own?\n        # divcopy = deepcopy(element)\n        processed_element = handle_textnode(element, options, comments_fix=False)\n        if processed_element is not None and text_chars_test(processed_element.text) is True:\n            processed_element.attrib.clear()\n            # small div-correction # could be moved elsewhere\n            if processed_element.tag == 'div':\n                processed_element.tag = 'p'\n            # insert\n            return processed_element\n    else:\n        LOGGER.debug('unexpected element seen: %s %s', element.tag, element.text)\n    return None\n\n\ndef handle_paragraphs(element, potential_tags, options):\n    '''Process paragraphs (p) elements along with their children,\n       trim and clean the content'''\n    element.attrib.clear()\n    # strip_tags(element, 'p') # change in precision due to spaces?\n    # no children\n    if len(element) == 0:\n        processed_element = process_node(element, options)\n        if processed_element is not None:\n            return processed_element\n        return None\n    # children\n    processed_element = Element(element.tag)\n    for child in element.iter('*'):\n        if child.tag not in potential_tags and child.tag != 'done':\n            LOGGER.debug('unexpected in p: %s %s %s', child.tag, child.text, child.tail)\n            continue\n        # spacing = child.tag in SPACING_PROTECTED  # todo: outputformat.startswith('xml')?\n        # todo: act on spacing here?\n        processed_child = handle_textnode(child, options, comments_fix=False, preserve_spaces=True)\n        if processed_child is not None:\n            # todo: needing attention!\n            if processed_child.tag == 'p':\n                LOGGER.debug('extra p within p: %s %s %s', processed_child.tag, processed_child.text,\n                             processed_child.tail)\n                if processed_element.text:\n                    processed_element.text += ' ' + processed_child.text\n                else:\n                    processed_element.text = processed_child.text\n                continue\n            # handle formatting\n            newsub = Element(child.tag)\n            if processed_child.tag in P_FORMATTING:\n                # check depth and clean\n                if len(processed_child) > 0:\n                    for item in processed_child:  # children are lists\n                        if text_chars_test(item.text) is True:\n                            item.text = ' ' + item.text\n                        strip_tags(processed_child, item.tag)\n                # correct attributes\n                if child.tag == 'hi':\n                    newsub.set('rend', child.get('rend'))\n                elif child.tag == 'ref':\n                    if child.get('target') is not None:\n                        newsub.set('target', child.get('target'))\n            # handle line breaks\n            # elif processed_child.tag == 'lb':\n            #    try:\n            #        processed_child.tail = process_node(child, options).tail\n            #    except AttributeError:  # no text\n            #        pass\n            # prepare text\n            # todo: to be moved to handle_textnode()\n            # if text_chars_test(processed_child.text) is False:\n            #    processed_child.text = ''\n            # if text_chars_test(processed_child.tail) is False:\n            #    processed_child.tail = ''\n            # if there are already children\n            # if len(processed_element) > 0:\n            #    if text_chars_test(processed_child.tail) is True:\n            #        newsub.tail = processed_child.text + processed_child.tail\n            #    else:\n            #        newsub.tail = processed_child.text\n            newsub.text, newsub.tail = processed_child.text, processed_child.tail\n            processed_element.append(newsub)\n        child.tag = 'done'\n    # finish\n    if len(processed_element) > 0:\n        # clean trailing lb-elements\n        if (\n                processed_element[-1].tag == 'lb'\n                and processed_element[-1].tail is None\n        ):\n            processed_element[-1].getparent().remove(processed_element[-1])\n        return processed_element\n    if processed_element.text:\n        return processed_element\n    LOGGER.debug('discarding p-child: %s', tostring(processed_element))\n    return None\n\n\ndef define_cell_type(element):\n    '''Determine cell element type and mint new element'''\n    # define tag\n    cell_element = Element('cell')\n    if element.tag == 'th':\n        cell_element.set('role', 'head')\n    return cell_element\n\n\ndef handle_table(table_elem, potential_tags, options):\n    '''Process single table element'''\n    newtable = Element('table')\n    newrow = Element('row')\n    # strip these structural elements\n    strip_tags(table_elem, 'thead', 'tbody', 'tfoot')\n    # explore sub-elements\n    for subelement in table_elem.iterdescendants():\n        if subelement.tag == 'tr':\n            # process existing row\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        elif subelement.tag in TABLE_ELEMS:\n            newchildelem = define_cell_type(subelement)\n            # process\n            if len(subelement) == 0:\n                processed_cell = process_node(subelement, options)\n                if processed_cell is not None:\n                    newchildelem.text, newchildelem.tail = processed_cell.text, processed_cell.tail\n            else:\n                # proceed with iteration, fix for nested elements\n                newchildelem.text, newchildelem.tail = subelement.text, subelement.tail\n                subelement.tag = \"done\"\n                for child in subelement.iterdescendants():\n                    if child.tag in TABLE_ALL:\n                        # todo: define attributes properly\n                        if child.tag in TABLE_ELEMS:\n                            # subcell_elem = define_cell_type(subelement)\n                            child.tag = 'cell'\n                        processed_subchild = handle_textnode(child, options, preserve_spaces=True, comments_fix=True)\n                    # todo: lists in table cells\n                    else:\n                        # subcell_elem = Element(child.tag)\n                        processed_subchild = handle_textelem(child, potential_tags.union(['div']), options)\n                    # add child element to processed_element\n                    if processed_subchild is not None:\n                        subchildelem = SubElement(newchildelem, processed_subchild.tag)\n                        subchildelem.text, subchildelem.tail = processed_subchild.text, processed_subchild.tail\n                    child.tag = 'done'\n            # add to tree\n            if newchildelem.text or len(newchildelem) > 0:\n                newrow.append(newchildelem)\n        # beware of nested tables\n        elif subelement.tag == 'table':\n            break\n        # cleanup\n        subelement.tag = 'done'\n    # end of processing\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    if len(newtable) > 0:\n        return newtable\n    return None\n\n\ndef handle_image(element):\n    '''Process image element'''\n    # image source\n    processed_element = Element(element.tag)\n    if is_image_file(element.get('data-src')):\n        processed_element.set('src', element.get('data-src'))\n    elif is_image_file(element.get('src')):\n        processed_element.set('src', element.get('src'))\n    else:\n        # take the first corresponding attribute\n        for attr in element.attrib:\n            if attr.startswith('data-src') and is_image_file(element.get(attr)):\n                processed_element.set('src', element.get(attr))\n                break\n    # additional data\n    if element.get('alt') is not None:\n        processed_element.set('alt', element.get('alt'))\n    if element.get('title') is not None:\n        processed_element.set('title', element.get('title'))\n    # don't return empty elements or elements without source, just None\n    if len(processed_element.attrib) == 0 or not processed_element.get('src'):\n        return None\n    # post-processing: URLs\n    url = processed_element.get('src')\n    processed_element.set('src', re.sub(r'^//', 'http://', url))\n    return processed_element\n\n\ndef handle_textelem(element, potential_tags, options):\n    '''Process text element and determine how to deal with its content'''\n    new_element = None\n    # bypass: nested elements\n    if element.tag == 'list':\n        new_element = handle_lists(element, options)\n    elif element.tag in CODES_QUOTES:\n        new_element = handle_quotes(element, options)\n    elif element.tag == 'head':\n        new_element = handle_titles(element, options)\n    elif element.tag == 'p':\n        new_element = handle_paragraphs(element, potential_tags, options)\n    elif element.tag == 'lb':\n        if text_chars_test(element.tail) is True:\n            element = process_node(element, options)\n            if element is not None:\n                new_element = Element('p')\n                new_element.text = element.tail\n    elif element.tag in FORMATTING:\n        new_element = handle_formatting(element, options)  # process_node(element, options)\n    elif element.tag == 'table' and 'table' in potential_tags:\n        new_element = handle_table(element, potential_tags, options)\n    elif element.tag == 'graphic' and 'graphic' in potential_tags:\n        new_element = handle_image(element)\n    else:\n        # other elements (div, ??, ??)\n        new_element = handle_other_elements(element, potential_tags, options)\n    return new_element\n\n\ndef recover_wild_text(tree, result_body, options, potential_tags=TAG_CATALOG):\n    '''Look for all previously unconsidered wild elements, including outside of the determined\n       frame and throughout the document to recover potentially missing text parts'''\n    LOGGER.debug('Recovering wild text elements')\n    search_expr = './/blockquote|.//code|.//p|.//pre|.//q|.//quote|.//table|.//div[contains(@class, \\'w3-code\\')]'\n    if options.recall is True:\n        potential_tags.update(['div', 'lb'])\n        search_expr += '|.//div|.//lb|.//list'\n    # prune\n    search_tree = prune_unwanted_sections(tree, potential_tags, options)\n    # decide if links are preserved\n    if 'ref' not in potential_tags:\n        strip_tags(search_tree, 'a', 'ref', 'span')\n    else:\n        strip_tags(search_tree, 'span')\n    subelems = search_tree.xpath(search_expr)\n    result_body.extend(filter(lambda x: x is not None, (handle_textelem(e, potential_tags, options)\n                       for e in subelems)))\n    return result_body\n\n\ndef prune_unwanted_sections(tree, potential_tags, options):\n    'Rule-based deletion of targeted document sections'\n    # prune the rest\n    tree = prune_unwanted_nodes(tree, OVERALL_DISCARD_XPATH, with_backup=True)\n    tree = prune_unwanted_nodes(tree, PAYWALL_DISCARD_XPATH)\n    # decide if images are preserved\n    if 'graphic' not in potential_tags:\n        tree = prune_unwanted_nodes(tree, DISCARD_IMAGE_ELEMENTS)\n    # balance precision/recall\n    if options.recall is False:\n        tree = prune_unwanted_nodes(tree, TEASER_DISCARD_XPATH)\n        if options.precision is True:\n            tree = prune_unwanted_nodes(tree, PRECISION_DISCARD_XPATH)\n    # remove elements by link density\n    tree = delete_by_link_density(tree, 'div', backtracking=True, favor_precision=options.precision)\n    tree = delete_by_link_density(tree, 'list', backtracking=False, favor_precision=options.precision)\n    tree = delete_by_link_density(tree, 'p', backtracking=False, favor_precision=options.precision)\n    # also filter fw/head, table and quote elements?\n    if options.precision is True:\n        # delete trailing titles\n        while len(tree) > 0 and (tree[-1].tag == 'head'):\n            tree[-1].getparent().remove(tree[-1])\n        tree = delete_by_link_density(tree, 'head', backtracking=False)  # favor_precision=options.precision\n        tree = delete_by_link_density(tree, 'quote', backtracking=False)  # favor_precision=options.precision\n    return tree\n\n\ndef extract_content(tree, options):\n    '''Find the main content of a page using a set of XPath expressions,\n       then extract relevant elements, strip them of unwanted subparts and\n       convert them'''\n    # backup\n    backup_tree = deepcopy(tree)\n    # init\n    result_body = Element('body')\n    potential_tags = set(TAG_CATALOG)\n    if options.tables is True:\n        potential_tags.update(['table', 'td', 'th', 'tr'])\n    if options.images is True:\n        potential_tags.add('graphic')\n    if options.links is True:\n        potential_tags.add('ref')\n    # iterate\n    for expr in BODY_XPATH:\n        # select tree if the expression has been found\n        try:\n            subtree = tree.xpath(expr)[0]\n        except IndexError:\n            continue\n        # prune the subtree\n        subtree = prune_unwanted_sections(subtree, potential_tags, options)\n        # second pass?\n        # subtree = delete_by_link_density(subtree, 'list', backtracking=False, favor_precision=options.precision)\n        if 'table' in potential_tags or options.precision is True:\n            for elem in subtree.iter('table'):\n                if link_density_test_tables(elem) is True:\n                    elem.getparent().remove(elem)\n        # skip if empty tree\n        if len(subtree) == 0:\n            continue\n        # no paragraphs containing text, or not enough\n        ptest = subtree.xpath('//p//text()')\n        if options.recall is True:\n            factor = 5\n        elif options.precision is True:\n            factor = 1\n        else:\n            factor = 3\n        if not ptest or len(''.join(ptest)) < options.config.getint('DEFAULT', 'MIN_EXTRACTED_SIZE') * factor:\n            potential_tags.add('div')\n        # polish list of potential tags\n        if 'ref' not in potential_tags:\n            strip_tags(subtree, 'ref')\n        if 'span' not in potential_tags:\n            strip_tags(subtree, 'span')\n        LOGGER.debug(sorted(potential_tags))\n        # proper extraction\n        subelems = subtree.xpath('.//*')\n        # e.g. only lb-elems in a div\n        if {e.tag for e in subelems} == {'lb'}:\n            subelems = [subtree]\n        # extract content\n        result_body.extend(filter(lambda x: x is not None, (handle_textelem(e, potential_tags, options) for e in subelems)))\n        # remove trailing titles\n        while len(result_body) > 0 and (result_body[-1].tag in NOT_AT_THE_END):\n            result_body[-1].getparent().remove(result_body[-1])\n        # exit the loop if the result has children\n        if len(result_body) > 1:\n            LOGGER.debug(expr)\n            break\n    temp_text = ' '.join(result_body.itertext()).strip()\n    # try parsing wild <p> elements if nothing found or text too short\n    # todo: test precision and recall settings here\n    if len(result_body) == 0 or len(temp_text) < options.config.getint('DEFAULT', 'MIN_EXTRACTED_SIZE'):\n        result_body = recover_wild_text(backup_tree, result_body, options, potential_tags)\n        temp_text = ' '.join(result_body.itertext()).strip()\n    # filter output\n    strip_elements(result_body, 'done')\n    strip_tags(result_body, 'div')\n    # return\n    return result_body, temp_text, len(temp_text)\n\n\ndef process_comments_node(elem, potential_tags, options):\n    '''Process comment node and determine how to deal with its content'''\n    if elem.tag in potential_tags:\n        # print(elem.tag, elem.text_content())\n        processed_element = handle_textnode(elem, options, comments_fix=True)\n        # test length and remove\n        if processed_element is not None:  # and processed_element.text not in COMMENTS_BLACKLIST:\n            processed_element.attrib.clear()\n            # if textfilter(elem) is True:  # ^Pingback\n            #    return None\n            return processed_element\n    return None\n\n\n\n\n\n[docs]\n\ndef extract_comments(tree, options):\n    '''Try and extract comments out of potential sections in the HTML'''\n    comments_body = Element('body')\n    # define iteration strategy\n    potential_tags = set(TAG_CATALOG)  # 'span'\n    # potential_tags.add('div') trouble with <div class=\"comment-author meta\">\n    for expr in COMMENTS_XPATH:\n        # select tree if the expression has been found\n        subtree = tree.xpath(expr)\n        if not subtree:\n            continue\n        subtree = subtree[0]\n        # prune\n        subtree = prune_unwanted_nodes(subtree, COMMENTS_DISCARD_XPATH)\n        # todo: unified stripping function, taking include_links into account\n        strip_tags(subtree, 'a', 'ref', 'span')\n        # extract content\n        # for elem in subtree.xpath('.//*'):\n        #    processed_elem = process_comments_node(elem, potential_tags)\n        #    if processed_elem is not None:\n        #        comments_body.append(processed_elem)\n        # processed_elems = (process_comments_node(elem, potential_tags, options) for elem in\n        #                    subtree.xpath('.//*'))\n        comments_body.extend(filter(lambda x: x is not None, (process_comments_node(e, potential_tags, options) for e in subtree.xpath('.//*'))))\n        # control\n        if len(comments_body) > 0:  # if it has children\n            LOGGER.debug(expr)\n            # remove corresponding subtree\n            subtree.getparent().remove(subtree)\n            break\n    # lengths\n    temp_comments = ' '.join(comments_body.itertext()).strip()\n    return comments_body, temp_comments, len(temp_comments), tree\n\n\n\n\ndef compare_extraction(tree, backup_tree, url, body, text, len_text, options):\n    '''Decide whether to choose own or external extraction\n       based on a series of heuristics'''\n    min_target_length = options.config.getint('DEFAULT', 'MIN_EXTRACTED_SIZE')\n    # bypass for recall\n    if options.recall is True and len_text > min_target_length * 10:\n        return body, text, len_text\n    algo_flag, jt_result = False, False\n    # prior cleaning\n    backup_tree = prune_unwanted_nodes(backup_tree, PAYWALL_DISCARD_XPATH)\n    if options.precision is True:\n        backup_tree = prune_unwanted_nodes(backup_tree, OVERALL_DISCARD_XPATH)\n    # try with readability\n    temppost_algo = try_readability(backup_tree)\n    # unicode fix necessary on certain systems (#331)\n    algo_text = trim(tostring(temppost_algo, method='text', encoding='utf-8').decode('utf-8'))\n    len_algo = len(algo_text)\n    # compare\n    LOGGER.debug('extracted length: %s (algorithm) %s (extraction)', len_algo, len_text)\n    # conditions to use alternative algorithms\n    if len_algo in (0, len_text):\n        algo_flag = False\n    elif len_text == 0 and len_algo > 0:\n        algo_flag = True\n    elif len_text > 2 * len_algo:\n        algo_flag = False\n    elif len_algo > 2 * len_text:\n        algo_flag = True\n    # borderline cases\n    elif not body.xpath('.//p//text()') and len_algo > min_target_length * 2:\n        algo_flag = True\n    elif len(body.findall('.//table')) > len(body.findall('.//p')) and len_algo > min_target_length * 2:\n        algo_flag = True\n    # https://github.com/adbar/trafilatura/issues/354\n    elif options.recall is True and not body.xpath('.//head') and temppost_algo.xpath('.//h2|.//h3|.//h4') and len_algo > len_text:\n        algo_flag = True\n    else:\n        LOGGER.debug('extraction values: %s %s for %s', len_text, len_algo, url)\n        algo_flag = False\n    # apply decision\n    if algo_flag:\n        body, text, len_text = temppost_algo, algo_text, len_algo\n        LOGGER.debug('using generic algorithm: %s', url)\n    else:\n        LOGGER.debug('using custom extraction: %s', url)\n    # override faulty extraction: try with justext\n    if body.xpath(SANITIZED_XPATH) or len_text < min_target_length:  # body.find(...)\n    # or options.recall is True ?\n        LOGGER.debug('unclean document triggering justext examination: %s', url)\n        # tree = prune_unwanted_sections(tree, {}, options)\n        body2, text2, len_text2, jt_result = justext_rescue(tree, url, options.lang, body, 0, '')\n        # prevent too short documents from replacing the main text\n        if jt_result is True and not len_text > 4*len_text2:  # threshold could be adjusted\n            LOGGER.debug('using justext, length: %s', len_text2)\n            body, text, len_text = body2, text2, len_text2\n    # post-processing: remove unwanted sections\n    if algo_flag is True and jt_result is False:\n        body, text, len_text = sanitize_tree(body, options)\n    return body, text, len_text\n\n\n\n\n\n[docs]\n\ndef baseline(filecontent):\n    \"\"\"Use baseline extraction function targeting text paragraphs and/or JSON metadata.\n\n    Args:\n        filecontent: HTML code as binary string or string.\n\n    Returns:\n        A LXML <body> element containing the extracted paragraphs,\n        the main text as string, and its length as integer.\n\n    \"\"\"\n    tree = load_html(filecontent)\n    postbody = Element('body')\n    if tree is None:\n        return postbody, '', 0\n    # scrape from json text\n    for elem in tree.iterfind('.//script[@type=\"application/ld+json\"]'):\n        if elem.text and '\"article' in elem.text:\n            mymatch = JSON_SEARCH.search(elem.text)\n            if mymatch:\n                elem = SubElement(postbody, 'p')\n                elem.text = trim(mymatch[1].replace('\\\\\"', '\"'))\n                return postbody, elem.text, len(elem.text)\n    # basic tree cleaning\n    for elem in tree.xpath('.//aside|.//footer|.//script|.//style'):\n        elem.getparent().remove(elem)\n    # scrape from article tag\n    article_elem = tree.find('.//article')\n    if article_elem is not None:\n        temp_text = trim(article_elem.text_content())\n        if len(temp_text) > 100:\n            elem = SubElement(postbody, 'p')\n            elem.text = temp_text\n            return postbody, temp_text, len(temp_text)\n    # scrape from text paragraphs\n    results = set()\n    for element in tree.iter('blockquote', 'code', 'p', 'pre', 'q', 'quote'):\n        entry = element.text_content()\n        if entry not in results:\n            elem = SubElement(postbody, 'p')\n            elem.text = entry\n            results.add(entry)\n    temp_text = trim('\\n'.join(postbody.itertext()))\n    if len(temp_text) > 100:\n        return postbody, temp_text, len(temp_text)\n    # default strategy: clean the tree and take everything\n    postbody = Element('body')\n    body_elem = tree.find('.//body')\n    if body_elem is not None:\n        # elem.text = trim(body_elem.text_content())\n        text = '\\n'.join([trim(e) for e in body_elem.itertext()])\n        if len(text) > 100:\n            elem = SubElement(postbody, 'p')\n            elem.text = text\n            return postbody, text, len(text)\n    # new fallback\n    text = html2txt(tree)\n    elem = SubElement(postbody, 'p')\n    elem.text = text\n    return postbody, text, len(text)\n\n\n    # old: return postbody, '', 0\n\n\n\n\n\n[docs]\n\ndef html2txt(content):\n    \"\"\"Run basic html2txt on a document.\n\n    Args:\n        content: HTML document as string or LXML element.\n\n    Returns:\n        The extracted text in the form of a string or an empty string.\n\n    \"\"\"\n    tree = load_html(content)\n    if tree is None:\n        return ''\n    return ' '.join(tree.text_content().split()).strip()\n\n\n\n\ndef determine_returnstring(document, output_format, include_formatting, tei_validation):\n    '''Convert XML tree to chosen format, clean the result and output it as a string'''\n    # XML (TEI) steps\n    if 'xml' in output_format:\n        # last cleaning\n        for element in document.body.iter('*'):\n            if element.tag != 'graphic' and len(element) == 0 and not element.text and not element.tail:\n                parent = element.getparent()\n                # do not remove elements inside <code> to preserve formatting\n                if parent is not None and parent.tag != 'code':\n                    parent.remove(element)\n        # build output trees\n        strip_double_tags(document.body)\n        remove_empty_elements(document.body)\n        if output_format == 'xml':\n            output = build_xml_output(document)\n        elif output_format == 'xmltei':\n            output = build_tei_output(document)\n        # can be improved\n        returnstring = control_xml_output(output, output_format, tei_validation, document)\n    # CSV\n    elif output_format == 'csv':\n        posttext = xmltotxt(document.body, include_formatting)\n        if document.commentsbody is not None:\n            commentstext = xmltotxt(document.commentsbody, include_formatting)\n        else:\n            commentstext = ''\n        returnstring = txttocsv(posttext, commentstext, document)\n    # JSON\n    elif output_format == 'json':\n        returnstring = build_json_output(document)\n    # TXT\n    else:\n        returnstring = xmltotxt(document.body, include_formatting)\n        if document.commentsbody is not None:\n            comments_text = xmltotxt(document.commentsbody, include_formatting)\n            returnstring = f\"{returnstring}\\n{comments_text}\".strip()\n    # normalize Unicode format (defaults to NFC)\n    return normalize_unicode(returnstring)\n\n\n\n\n\n[docs]\n\ndef bare_extraction(filecontent, url=None, no_fallback=False,  # fast=False,\n                    favor_precision=False, favor_recall=False,\n                    include_comments=True, output_format='python', target_language=None,\n                    include_tables=True, include_images=False, include_formatting=False,\n                    include_links=False, deduplicate=False,\n                    date_extraction_params=None,\n                    only_with_metadata=False, with_metadata=False,\n                    max_tree_size=None, url_blacklist=None, author_blacklist=None,\n                    as_dict=True, config=DEFAULT_CONFIG):\n    \"\"\"Internal function for text extraction returning bare Python variables.\n\n    Args:\n        filecontent: HTML code as string.\n        url: URL of the webpage.\n        no_fallback: Use faster heuristics and skip backup extraction.\n        favor_precision: prefer less text but correct extraction.\n        favor_recall: prefer more text even when unsure.\n        include_comments: Extract comments along with the main text.\n        output_format: Define an output format, Python being the default\n            and the interest of this internal function.\n            Other values: \"txt\", \"csv\", \"json\", \"xml\", or \"xmltei\".\n        target_language: Define a language to discard invalid documents (ISO 639-1 format).\n        include_tables: Take into account information within the HTML <table> element.\n        include_images: Take images into account (experimental).\n        include_formatting: Keep structural elements related to formatting\n            (present in XML format, converted to markdown otherwise).\n        include_links: Keep links along with their targets (experimental).\n        deduplicate: Remove duplicate segments and documents.\n        date_extraction_params: Provide extraction parameters to htmldate as dict().\n        only_with_metadata: Only keep documents featuring all essential metadata\n            (date, title, url).\n        max_tree_size: Discard documents with too many elements.\n        url_blacklist: Provide a blacklist of URLs as set() to filter out documents.\n        author_blacklist: Provide a blacklist of Author Names as set() to filter out authors.\n        as_dict: Legacy option, return a dictionary instead of a class with attributes.\n        config: Directly provide a configparser configuration.\n\n    Returns:\n        A Python dict() containing all the extracted information or None.\n\n    Raises:\n        ValueError: Extraction problem.\n    \"\"\"\n    # init\n    if url_blacklist is None:\n        url_blacklist = set()\n\n    # deprecation warnings\n    if with_metadata is True:\n        only_with_metadata = with_metadata\n        warnings.warn(\n            '\"with_metadata\" will be deprecated in a future version, use \"only_with_metadata instead\"',\n            PendingDeprecationWarning\n        )\n    #if no_fallback is True:\n    #    fast = no_fallback\n        #warnings.warn(\n        #    '\"no_fallback\" will be deprecated in a future version, use \"fast\" instead',\n        #    PendingDeprecationWarning\n        #)\n\n    # load data\n    try:\n        tree = load_html(filecontent)\n        if tree is None:\n            LOGGER.error('empty HTML tree for URL %s', url)\n            raise ValueError\n\n        # quick and dirty HTML lang check\n        if target_language is not None and (no_fallback is True or LANGID_FLAG is False):\n            if check_html_lang(tree, target_language) is False:\n                LOGGER.error('wrong HTML meta language for URL %s', url)\n                raise ValueError\n\n        # extract metadata if necessary\n        if output_format != 'txt':\n            document = extract_metadata(tree, url, date_extraction_params, no_fallback, author_blacklist)\n            # cut short if extracted URL in blacklist\n            if document.url in url_blacklist:\n                LOGGER.warning('blacklisted URL: %s', url)\n                raise ValueError\n            # cut short if core elements are missing\n            if only_with_metadata is True and any(\n                    x is None for x in\n                    [document.date, document.title, document.url]\n            ):\n                LOGGER.error('no metadata for URL %s', url)\n                raise ValueError\n        else:\n            document = Document()\n\n        # regroup extraction options\n        options = Extractor(config, no_fallback, favor_precision, favor_recall,\n                            include_comments, include_formatting, include_links,\n                            include_images, include_tables, deduplicate,\n                            target_language)\n\n        # backup (or not) for further processing\n        tree_backup_1 = deepcopy(tree) if no_fallback is False else None\n        tree_backup_2 = deepcopy(tree)\n\n        # clean + use LXML cleaner\n        cleaned_tree = tree_cleaning(tree, options)\n        cleaned_tree_backup = deepcopy(cleaned_tree)\n\n        # convert tags, the rest does not work without conversion\n        cleaned_tree = convert_tags(cleaned_tree, options, url or document.url)\n\n        # comments first, then remove\n        if include_comments is True:\n            commentsbody, temp_comments, len_comments, cleaned_tree = extract_comments(cleaned_tree, options)\n        else:\n            commentsbody, temp_comments, len_comments = None, '', 0\n        if favor_precision is True:\n            cleaned_tree = prune_unwanted_nodes(cleaned_tree, REMOVE_COMMENTS_XPATH)\n\n        # extract content\n        postbody, temp_text, len_text = extract_content(cleaned_tree, options)\n\n        # compare if necessary\n        if no_fallback is False:\n            postbody, temp_text, len_text = compare_extraction(cleaned_tree_backup, tree_backup_1, url, postbody, temp_text, len_text, options)\n        # add baseline as additional fallback\n        # rescue: try to use original/dirty tree # and favor_precision is False=?\n        if len_text < config.getint('DEFAULT', 'MIN_EXTRACTED_SIZE'):\n            postbody, temp_text, len_text = baseline(tree_backup_2)\n            LOGGER.debug('non-clean extracted length: %s (extraction)', len_text)\n\n        # tree size sanity check\n        if max_tree_size is not None:\n            # strip tags\n            if len(postbody) > max_tree_size:\n                LOGGER.debug('output tree too long: %s', len(postbody))\n                strip_tags(postbody, 'hi')\n            # still too long, raise an error\n            if len(postbody) > max_tree_size:\n                LOGGER.debug('output tree too long: %s, discarding file', len(postbody))\n                raise ValueError\n        # size checks\n        if len_comments < config.getint('DEFAULT', 'MIN_EXTRACTED_COMM_SIZE'):\n            LOGGER.debug('not enough comments %s', url)\n        if len_text < config.getint('DEFAULT', 'MIN_OUTPUT_SIZE') and len_comments < config.getint('DEFAULT',\n                                                                                                   'MIN_OUTPUT_COMM_SIZE'):\n            LOGGER.debug('text and comments not long enough: %s %s', len_text, len_comments)\n            raise ValueError\n\n        # check duplicates at body level\n        if deduplicate is True and duplicate_test(postbody, config) is True:\n            LOGGER.debug('discarding duplicate document for URL %s', url)\n            raise ValueError\n\n        # sanity check on language\n        if target_language is not None:\n            is_not_target_lang, document = language_filter(temp_text, temp_comments, target_language, document)\n            if is_not_target_lang is True:\n                LOGGER.debug('wrong language for URL %s', url)\n                raise ValueError\n\n    except (TypeError, ValueError):\n        LOGGER.warning('discarding data for url: %s', url)  # document.url , record_id\n        return None\n\n    # special case: python variables\n    if output_format == 'python':\n        document.text = xmltotxt(postbody, include_formatting)\n        if include_comments is True:\n            document.comments = xmltotxt(commentsbody, include_formatting)\n    else:\n        document.raw_text, document.body, document.commentsbody = temp_text, postbody, commentsbody\n    if as_dict is True:\n        document = {slot: getattr(document, slot, None) for slot in document.__slots__}\n    return document\n\n\n\n\ndef timeout_handler(signum, frame):\n    '''Raise a timeout exception to handle rare malicious files'''\n    raise RuntimeError('unusual file processing time, aborting')\n\n\n\n\n\n[docs]\n\ndef extract(filecontent, url=None, record_id=None, no_fallback=False,\n            favor_precision=False, favor_recall=False,\n            include_comments=True, output_format='txt',\n            tei_validation=False, target_language=None,\n            include_tables=True, include_images=False, include_formatting=False,\n            include_links=False, deduplicate=False,\n            date_extraction_params=None,\n            only_with_metadata=False, with_metadata=False,\n            max_tree_size=None, url_blacklist=None, author_blacklist=None,\n            settingsfile=None, config=DEFAULT_CONFIG,\n            **kwargs):\n    \"\"\"Main function exposed by the package:\n       Wrapper for text extraction and conversion to chosen output format.\n\n    Args:\n        filecontent: HTML code as string.\n        url: URL of the webpage.\n        record_id: Add an ID to the metadata.\n        no_fallback: Skip the backup extraction with readability-lxml and justext.\n        favor_precision: prefer less text but correct extraction.\n        favor_recall: when unsure, prefer more text.\n        include_comments: Extract comments along with the main text.\n        output_format: Define an output format:\n            'txt', 'csv', 'json', 'xml', or 'xmltei'.\n        tei_validation: Validate the XML-TEI output with respect to the TEI standard.\n        target_language: Define a language to discard invalid documents (ISO 639-1 format).\n        include_tables: Take into account information within the HTML <table> element.\n        include_images: Take images into account (experimental).\n        include_formatting: Keep structural elements related to formatting\n            (only valuable if output_format is set to XML).\n        include_links: Keep links along with their targets (experimental).\n        deduplicate: Remove duplicate segments and documents.\n        date_extraction_params: Provide extraction parameters to htmldate as dict().\n        only_with_metadata: Only keep documents featuring all essential metadata\n            (date, title, url).\n        max_tree_size: Discard documents with too many elements.\n        url_blacklist: Provide a blacklist of URLs as set() to filter out documents.\n        author_blacklist: Provide a blacklist of Author Names as set() to filter out authors.\n        settingsfile: Use a configuration file to override the standard settings.\n        config: Directly provide a configparser configuration.\n\n    Returns:\n        A string in the desired format or None.\n\n    \"\"\"\n    # older, deprecated functions\n    if kwargs and any([\n        # output formats\n            'csv_output' in kwargs,\n            'json_output' in kwargs,\n            'tei_output' in kwargs,\n            'xml_output' in kwargs\n        ]):\n        raise NameError(\n            'Deprecated argument: use output_format instead, e.g. output_format=\"xml\"'\n            )\n        # todo: add with_metadata later\n\n    # configuration init\n    config = use_config(settingsfile, config)\n\n    # extraction\n    try:\n        document = bare_extraction(\n            filecontent, url=url, no_fallback=no_fallback,\n            favor_precision=favor_precision, favor_recall=favor_recall,\n            include_comments=include_comments, output_format=output_format,\n            target_language=target_language, include_tables=include_tables,\n            include_images=include_images,\n            include_formatting=include_formatting, include_links=include_links,\n            deduplicate=deduplicate,\n            date_extraction_params=date_extraction_params,\n            only_with_metadata=only_with_metadata, with_metadata=with_metadata,\n            max_tree_size=max_tree_size, url_blacklist=url_blacklist,\n            author_blacklist=author_blacklist,\n            as_dict=False, config=config,\n        )\n    except RuntimeError:\n        LOGGER.error('Processing timeout for %s', url)\n        document = None\n\n    # post-processing\n    if document is None:\n        return None\n    if output_format != 'txt':\n        # add record ID to metadata\n        document.id = record_id\n        # calculate fingerprint\n        document.fingerprint = content_fingerprint(str(document.title) + \" \" + document.raw_text)\n\n    # return\n    return determine_returnstring(document, output_format, include_formatting, tei_validation)\n\n\n\n\n# for legacy and backwards compatibility\ndef process_record(filecontent, url=None, record_id=None, no_fallback=False,\n                   include_comments=True, target_language=None,\n                   include_tables=True):\n    \"Legacy extraction function, now deprecated.\"\n    # deprecation warning\n    warnings.warn(\n        \"process_record() is deprecated, use extract() instead\",\n        DeprecationWarning\n    )\n    return extract(filecontent, url=url, record_id=record_id, no_fallback=no_fallback,\n                   include_comments=include_comments, target_language=target_language,\n                   include_tables=include_tables)\n\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Search - trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/search.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nSearch\n⌘\n+\nK\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/evaluation.rst.txt",
    "html": "Evaluation\n==========\n\n.. meta::\n    :description lang=en:\n        This benchmark tests how Python tools work on extraction of text from HTML code. Trafilatura\n        performs significantly better than the other comparable libraries in internal and external\n        evaluations.\n\n\nAlthough text is ubiquitous on the Web, extracting information from web pages can prove to be difficult. Should the tooling be adapted to particular news outlets or blogs that are targeted (which often amounts to the development of web scraping tools) or should the extraction be as generic as possible to provide opportunistic ways of gathering information?\n\nThe extraction focuses on the main content, which is usually the part displayed centrally, without the left or right bars, the header or the footer, but including potential titles and (optionally) comments. This task is also known as web scraping, boilerplate removal, DOM-based content extraction, main content identification, or web page cleaning.\n\n\nAlternatives\n------------\n\nAlthough a few corresponding Python packages are not actively maintained the following alternatives exist.\n\nThese packages keep the structure intact but do not focus on main text extraction:\n\n- `html2text <https://github.com/Alir3z4/html2text>`_ converts HTML pages to Markup language\n- `html_text <https://github.com/TeamHG-Memex/html-text>`_ converts HTML code to plain text\n- `inscriptis <https://github.com/weblyzard/inscriptis>`_ converts HTML to text with a particular emphasis on nested tables\n\nThese packages focus on main text extraction:\n\n- `boilerpy3 <https://github.com/jmriebold/BoilerPy3>`_ is a Python version of the boilerpipe algorithm for boilerplate removal and fulltext extraction\n- *dragnet* is not maintained anymore, it is provided for reference only (in older evaluations)\n- `goose3 <https://github.com/goose3/goose3>`_ can extract information for embedded content but doesn't preserve markup\n- `jusText <https://github.com/miso-belica/jusText>`_ is designed to preserve mainly text containing full sentences along with some markup, it has been explicitly developed to create linguistic resources\n- `newspaper3k <https://github.com/codelucas/newspaper>`_ is mostly geared towards newspaper texts, provides additional functions but no structured text or comment extraction\n- `news-please <https://github.com/fhamborg/news-please>`_ is a news crawler that extracts structured information\n- `readability-lxml <https://github.com/buriy/python-readability>`_ cleans the page and preserves some markup\n- `readabilipy <https://github.com/alan-turing-institute/ReadabiliPy>`_ contains a Python wrapper for Mozilla's Node.js package, as well as article extraction routines written in pure Python\n- `trafilatura <https://github.com/adbar/trafilatura>`_ is the library documented here, several options are tested regarding main text extraction only, without metadata or comments\n\nThe tools are compared to the raw page source and to a meaningful baseline consisting of extracting the raw text contained in the JSON article element or in a combination of paragraph, code and quote elements.\n\n\nDescription\n-----------\n\n**Test set**: The experiments below are run on a collection of documents which are either typical for Internet articles (news outlets, blogs). Some are harder to process due to mixed content (lists, tables) or not fully valid HTML code. They are selected from `large collections of web pages in German <https://www.dwds.de/d/k-web>`_, for the sake of completeness documents in other languages are added (notably English, French, other European languages, Chinese and Arabic, about 20-30% of the total).\n\n**Evaluation**: Decisive document segments are singled out which are not statistically representative but very significant in the perspective of working with the texts, most notably left/right columns, additional header, author or footer information such as imprints or addresses, as well as affiliated and social network links, in short boilerplate. Raw text segments are expected which is also a way to evaluate the quality of HTML extraction in itself.\n\n**Time**: The execution time is provided as an indication. As the baseline extraction is simple and fast, it is used for the benchmark. Certain packages are noticeably slower than the rest: *goose3* and *newspaper*, while *news-please*'s execution time isn't comparable because of operations unrelated to text extraction. ReadabiliPy is very slow for unclear reasons.\n\n**Errors**: The *boilerpy3*, *newspaper3k*, and *readabilipy* modules do not work without errors on every HTML file in the test set, probably because of malformed HTML, encoding or parsing bugs. These errors are ignored in order to complete the benchmark.\n\n**Results**: The baseline beats a few systems, showing its interest. *justext* is highly configurable and tweaking its configuration (as it is done here) can lead to better performance than its generic settings. *goose3* is the most precise algorithm, albeit at a significant cost in terms of recall. The packages focusing on raw text extraction *html_text* and *inscriptis* are roughly comparable and achieve the best recall as they try to extract all the text. Rule-based approaches such as *trafilatura*'s obtain balanced results despite a lack of precision. Combined with an algorithmic approach they perform significantly better than the other tested solutions.\n\n**Roadmap**: Further evaluations will be run, including additional tools and languages. Comment extraction still has to be evaluated, although most libraries don not offer this functionality.\n\nThe evaluation script is available on the project repository: `tests/README.rst <https://github.com/adbar/trafilatura/blob/master/tests/>`_. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the *tests* directory.\n\n\nResults (2022-05-18)\n--------------------\n\n=============================== =========  ========== ========= ========= ======\n750 documents, 2236 text & 2250 boilerplate segments, Python 3.8\n--------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Diff.\n=============================== =========  ========== ========= ========= ======\n*raw HTML*                      0.527      0.874      0.546     0.658     0\nhtml2text 2020.1.16             0.486      0.709      0.481     0.577     7.6x\nhtml_text 0.5.2                 0.529      **0.958**  0.554     0.682     2.2x\ninscriptis 2.2.0 (html to txt)  0.534      **0.959**  0.563     0.686     3.5x\nnewspaper3k 0.2.8               0.895      0.593      0.762     0.713     12x\njustext 3.0.0 (custom)          0.865      0.650      0.775     0.742     5.2x\nboilerpy3 1.0.6 (article mode)  0.814      0.744      0.787     0.777     4.1x\n*baseline (text markup)*        0.757      0.827      0.781     0.790     **1x**\ngoose3 3.1.9                    **0.934**  0.690      0.821     0.793     22x\nreadability-lxml 0.8.1          0.891      0.729      0.820     0.801     5.8x\nnews-please 1.5.22              0.898      0.734      0.826     0.808     61x\nreadabilipy 0.2.0               0.877      0.870      0.874     0.874     248x\ntrafilatura 1.2.2 (fast)        0.914      0.886      0.902     0.900     4.8x\ntrafilatura 1.2.2 (precision)   **0.932**  0.874      0.905     0.902     9.4x\ntrafilatura 1.2.2 (standard)    0.914      0.904      **0.910** **0.909** 7.1x\n=============================== =========  ========== ========= ========= ======\n\n\nExternal evaluations\n--------------------\n\n- Trafilatura is the most efficient open-source library in *ScrapingHub*'s `article extraction benchmark <https://github.com/scrapinghub/article-extraction-benchmark>`_.\n- Best overall tool according to GaÃ«l Lejeune & Adrien Barbaresi, `Bien choisir son outil d'extraction de contenu Ã  partir du Web <https://hal.archives-ouvertes.fr/hal-02768510v3/document>`_ (2020, PDF, in French).\n- Comparison on a small `sample of Polish news texts and forums <https://github.com/tsolewski/Text_extraction_comparison_PL>`_.\n\n\nOlder results\n-------------\n\n\nOlder results (2021-06-07)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n=============================== =========  ========== ========= ========= ======\n500 documents, 1487 text and 1496 boilerplate segments\n--------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Diff.\n=============================== =========  ========== ========= ========= ======\n*raw HTML*                      0.527      0.878      0.547     0.659     0\nhtml2text 2020.1.16             0.488      0.714      0.484     0.580     8.9x\nhtml_text 0.5.2                 0.526      **0.958**  0.548     0.679     1.9x\ninscriptis 1.1 (html to txt)    0.531      **0.958**  0.556     0.683     2.4x\njustext 2.2.0 (custom)          0.870      0.584      0.749     0.699     6.1x\nnewspaper3k 0.2.8               0.921      0.574      0.763     0.708     12.9x\nboilerpy3 1.0.2 (article mode)  0.851      0.696      0.788     0.766     4.8x\ngoose3 3.1.9                    **0.950**  0.644      0.806     0.767     18.8x\n*baseline (text markup)*        0.746      0.804      0.766     0.774     **1x**\ndragnet 2.0.4                   0.906      0.689      0.810     0.783     3.1x\nreadability-lxml 0.8.1          0.917      0.716      0.826     0.804     5.9x\nnews-please 1.5.21              0.924      0.718      0.830     0.808     60x\ntrafilatura 0.8.2 (fast)        0.925      0.868      0.899     0.896     3.9x\ntrafilatura 0.8.2               0.934      **0.890**  **0.914** **0.912** 8.4x\n=============================== =========  ========== ========= ========= ======\n\n\n\nOlder results (2020-11-06)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n=============================== =========  ========== ========= ========= ======\n500 documents, 1487 text and 1496 boilerplate segments\n--------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Diff.\n=============================== =========  ========== ========= ========= ======\n*raw HTML*                      0.527      0.878      0.547     0.659     0\nhtml2text 2020.1.16             0.488      0.714      0.484     0.580     8.9x\nhtml_text 0.5.2                 0.526      **0.958**  0.548     0.679     1.9x\ninscriptis 1.1 (html to txt)    0.531      **0.958**  0.556     0.683     2.4x\njustext 2.2.0 (tweaked)         0.870      0.584      0.749     0.699     6.1x\nnewspaper3k 0.2.8               0.921      0.574      0.763     0.708     12.9x\ngoose3 3.1.6                    **0.950**  0.629      0.799     0.757     19.0x\nboilerpy3 1.0.2 (article mode)  0.851      0.696      0.788     0.766     4.8x\n*baseline (text markup)*        0.746      0.804      0.766     0.774     **1x**\ndragnet 2.0.4                   0.906      0.689      0.810     0.783     3.1x\nreadability-lxml 0.8.1          0.917      0.716      0.826     0.804     5.9x\nnews-please 1.5.13              0.923      0.711      0.827     0.804     184x\ntrafilatura 0.6.0               0.924      0.849      0.890     0.885     3.9x\ntrafilatura 0.6.0 (+ fallbacks) 0.933      **0.877**  **0.907** **0.904** 8.4x\n=============================== =========  ========== ========= ========= ======\n\n\n\nOlder results (2020-07-16)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n=============================== =========  ========== ========= ========= ======\n400 documents, 1186 text and 1198 boilerplate segments\n--------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Diff.\n=============================== =========  ========== ========= ========= ======\n*raw HTML*                      0.524      0.879      0.543     0.657     0\nhtml2text 2020.1.16             0.485      0.718      0.480     0.579     8.4x\nhtml_text 0.5.1                 0.521      0.962      0.542     0.676     1.8x\ninscriptis 1.0 (html to txt)    0.527      **0.965**  0.551     0.681     1.9x\nnewspaper3k 0.2.8               0.916      0.577      0.763     0.708     11.8x\njustext 2.2.0 (tweaked)         0.867      0.651      0.777     0.744     4.9x\ngoose3 3.1.6                    **0.953**  0.635      0.803     0.762     17.3x\n*baseline (text markup)*        0.738      0.804      0.760     0.770     **1x**\nboilerpy3 1.0.2 (article mode)  0.847      0.711      0.792     0.773     4.4x\ndragnet 2.0.4                   0.906      0.704      0.816     0.792     2.8x\nreadability-lxml 0.8.1          0.913      0.739      0.835     0.817     5.4x\nnews-please 1.4.25              0.918      0.739      0.837     0.819     56.4x\ntrafilatura 0.5.1               0.927      0.854      0.894     0.889     3.1x\ntrafilatura 0.5.1 (+ fallbacks) 0.933      0.885      **0.911** **0.908** 6.8x\n=============================== =========  ========== ========= ========= ======\n\n\nOlder results (2020-03-19)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n=============================== =========  ========== ========= ========= =====\n300 documents, 869 text and 878 boilerplate segments\n-------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Time\n=============================== =========  ========== ========= ========= =====\n*raw HTML*                      0.519      0.885      0.535     0.654     0\n*baseline (text markup)*        0.726      0.776      0.742     0.750     1.14 \nhtml2text 2020.1.16             0.499      0.787      0.501     0.611     11.00\ninscriptis 1.0 (html to txt)    0.521      **0.962**  0.541     0.676     2.47\njustext 2.2.0 (German stoplist) 0.849      0.529      0.719     0.652     6.37\nnewspaper 0.2.8                 0.923      0.591      0.772     0.721     14.80\ngoose3 3.1.6                    **0.957**  0.640      0.807     0.767     21.54\nboilerpy3 1.0.2 (article mode)  0.841      0.734      0.799     0.784     5.65\ndragnet 2.0.4                   0.909      0.722      0.825     0.804     3.64\nreadability-lxml 0.7.1          0.928      0.743      0.844     0.826     6.59\nnews-please 1.4.25              0.926      0.747      0.844     0.827     70.81\ntrafilatura 0.3.1 (rule-based)  0.901      0.831      0.871     0.865     5.43\ntrafilatura 0.3.1 (+ justext)   0.897      0.868      0.884     0.882     6.97\ntrafilatura 0.4                 0.914      0.869      0.894     0.891     4.87\ntrafilatura 0.4 (+ fallback)    0.925      0.904      **0.916** **0.914** 9.94\n=============================== =========  ========== ========= ========= =====\n\n\nOlder results (2020-01-29)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n=============================== =========  ========== ========= ========= =====\n100 documents, 266 text and 294 boilerplate segments\n-------------------------------------------------------------------------------\nPython Package                  Precision  Recall     Accuracy  F-Score   Time\n=============================== =========  ========== ========= ========= =====\n*raw HTML*                      0.492      0.902      0.511     0.637     0\ninscriptis 1.0 (html to txt)    0.504      **0.989**  0.532     0.668     0.87\njustext 2.2.0 (German stoplist) 0.886      0.553      0.754     0.681     2.22\ngoose3 3.1.6                    **0.935**  0.594      0.787     0.726     7.64\nnewspaper 0.2.8                 0.920      0.609      0.789     0.733     5.34\nboilerpy3 1.0.2 (default mode)  0.767      0.756      0.775     0.761     1.89\ndragnet 2.0.4                   0.904      0.673      0.811     0.772     1.25\nreadability-lxml 0.7.1          0.894      0.699      0.818     0.785     2.34\nnews-please 1.4.25              0.900      0.714      0.827     0.797     22.99\ntrafilatura 0.3.1 (rule-based)  0.872      0.895      0.887     0.883     1.87\ntrafilatura 0.3.1 (+ justext)   0.889      0.936      **0.914** **0.912** 2.19\n=============================== =========  ========== ========= ========= =====\n"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/tutorials.rst.txt",
    "html": "Tutorials\n=========\n\n\n.. toctree::\n   :maxdepth: 2\n\n   tutorial0\n   tutorial1\n   tutorial2\n   tutorial-epsilla\n   tutorial-dwds\n\n\n\nBlog posts\n^^^^^^^^^^\n\n- `Extracting the main text content from web pages using Python <https://adrien.barbaresi.eu/blog/trafilatura-main-text-content-python.html>`_\n- `Validating TEI-XML documents with Python <https://adrien.barbaresi.eu/blog/validating-tei-xml-python.html>`_\n- `Evaluating scraping and text extraction tools for Python <https://adrien.barbaresi.eu/blog/evaluating-text-extraction-python.html>`_\n- `Filtering links to gather texts on the web <https://adrien.barbaresi.eu/blog/link-filtering-courlan-python.html>`_\n- `Using sitemaps to crawl websites on the command-line <https://adrien.barbaresi.eu/blog/using-sitemaps-crawl-websites.html>`_\n- `Using RSS and Atom feeds to collect web pages with Python <https://adrien.barbaresi.eu/blog/using-feeds-text-extraction-python.html>`_\n- `Web scraping with R: Text and metadata extraction <https://adrien.barbaresi.eu/blog/web-scraping-text-metadata-r.html>`_\n- `Web scraping with Trafilatura just got faster <https://adrien.barbaresi.eu/blog/web-scraping-trafilatura-faster.html>`_\n\n\nVideos\n^^^^^^\n\nYoutube playlist\n    `Web scraping how-tos and tutorials <https://www.youtube.com/watch?v=8GkiOM17t0Q&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci>`_.\n\n\n.. raw:: html\n\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/rEOoItpzlVw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n\nExternal resources\n^^^^^^^^^^^^^^^^^^\n\n- `GLAM-Workbench <https://github.com/GLAM-Workbench>`_\n   - `Harvesting collections of text from archived web pages <https://github.com/GLAM-Workbench/web-archives/blob/master/getting_text_from_web_pages.ipynb>`_\n   - `Compare two versions of an archived web page <https://github.com/GLAM-Workbench/web-archives/blob/master/show_diffs.ipynb>`_\n\n\n- `User Ethics & Legal Concerns <https://melaniewalsh.github.io/Intro-Cultural-Analytics/Data-Collection/User-Ethics-Legal-Concerns.html>`_\n- `Download von Web-Daten <https://www.bubenhofer.com/korpuslinguistik/kurs/index.php?id=eigenes_wwwdownload.html>`_ & `Daten aufbereiten und verwalten <https://www.bubenhofer.com/korpuslinguistik/kurs/index.php?id=eigenes_aufbereitenXML.html>`_ (Tutorials in German by Noah Bubenhofer)\n\n"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/usage.rst.txt",
    "html": "Usage\n=====\n\n\n.. toctree::\n   :maxdepth: 2\n\n   quickstart\n   usage-python\n   usage-cli\n   usage-r\n   usage-gui\n   downloads\n   crawls\n   settings\n   troubleshooting\n   url-management\n"
  },
  {
    "title": "404 Not Found | Read the Docs",
    "url": "https://trafilatura.readthedocs.io/en/latest/py-modindex.html",
    "html": "Read the Docs\n404 Not Found\n\nYou are browsing the documentation of trafilatura. The documentation page you are looking for was not found.\n\nDocumentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page.\n\nTry searching?\nAre you the project owner?\n\nHere are some tips to address 404 errors:\n\nUse your own custom 404 page: Read more »\nCreate redirects when you move contents: Read more »\nStay Updated\nBlog\n\nSign up for our newsletter to get our latest blog updates delivered to your inbox weekly.\n\nEmail\nSubscribe\nResources\nTutorial\nTeam\nDocumentation\nGoing Ad-free\nSite Support\nSite Status\nCompany\nJobs\nAdvertise with Us\nRead the Docs for Business\nBranding & Media Kit\nPrivacy Policy\nTerms of Service\n \n\n© Copyright 2023, Read the Docs, Inc & contributors\n\nVersion 10.11.0\n\nAWS\nCloud Computing\n \nCloudflare\nDNS & SSL\n \nSentry\nMonitoring\n \nElastic\nSearch\n \nNew Relic\nPerformance\n \nPagerDuty\nMonitoring"
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/index.rst.txt",
    "html": "A Python package & command-line tool to gather text on the Web\n==============================================================\n\n.. meta::\n    :description lang=en:\n        Trafilatura is a Python package and command-line tool designed to gather text on the Web. Its main applications are web crawling, downloads, scraping, and extraction of main texts, comments and metadata.\n\n\n.. image:: https://img.shields.io/pypi/v/trafilatura.svg\n    :target: https://pypi.python.org/pypi/trafilatura\n    :alt: Python package\n\n.. image:: https://img.shields.io/pypi/pyversions/trafilatura.svg\n    :target: https://pypi.python.org/pypi/trafilatura\n    :alt: Python versions\n\n.. image:: https://img.shields.io/codecov/c/github/adbar/trafilatura.svg\n    :target: https://codecov.io/gh/adbar/trafilatura\n    :alt: Code Coverage\n\n.. image:: https://static.pepy.tech/badge/trafilatura/month\n    :target: https://pepy.tech/project/trafilatura\n    :alt: Downloads\n\n.. image:: https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue\n    :target: https://aclanthology.org/2021.acl-demo.15/\n    :alt: Reference DOI: 10.18653/v1/2021.acl-demo.15\n\n|\n\n.. image:: trafilatura-demo.gif\n    :alt: Demo as GIF image\n    :align: center\n    :width: 85%\n    :target: https://trafilatura.readthedocs.org/\n\n\nDescription\n-----------\n\nTrafilatura is a **Python package and command-line tool** designed to gather text on the Web. It includes discovery, extraction and text processing components. Its main applications are **web crawling, downloads, scraping, and extraction** of main texts, metadata and comments. It aims at staying **handy and modular**: no database is required, the output can be converted to various commonly used formats.\n\nGoing from raw HTML to essential parts can alleviate many problems related to text quality, first by avoiding the **noise caused by recurring elements** (headers, footers, links/blogroll etc.) and second by including information such as author and date in order to **make sense of the data**. The extractor tries to strike a balance between limiting noise (precision) and including all valid parts (recall). It also has to be **robust and reasonably fast**, it runs in production on millions of documents.\n\nThis tool can be **useful for quantitative research** in corpus linguistics, natural language processing, computational social science and beyond: it is relevant to anyone interested in data science, information extraction, text mining, and scraping-intensive use cases like search engine optimization, business analytics or information security.\n\n\nFeatures\n~~~~~~~~\n\n- Web crawling and text discovery:\n   - Focused crawling and politeness rules\n   - Support for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)\n   - URL management (blacklists, filtering and de-duplication)\n- Seamless and parallel processing, online and offline:\n   - URLs, HTML files or parsed HTML trees usable as input\n   - Efficient and polite processing of download queues\n   - Conversion of previously downloaded files\n- Robust and efficient extraction:\n   - Main text (with LXML, common patterns and generic algorithms: jusText, fork of readability-lxml)\n   - Metadata (title, author, date, site name, categories and tags)\n   - Formatting and structural elements: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting\n   - Comments (if applicable)\n- Output formats:\n   - Text (minimal formatting or Markdown)\n   - CSV (with metadata, `tab-separated values <https://en.wikipedia.org/wiki/Tab-separated_values>`_)\n   - JSON (with metadata)\n   - XML (with metadata, text formatting and page structure) and `TEI-XML <https://tei-c.org/>`_\n- Optional add-ons:\n   - Language detection on extracted content\n   - Graphical user interface (GUI)\n   - Speed optimizations\n\n\nEvaluation and alternatives\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor detailed results see the `benchmark <evaluation.html>`_ and `evaluation script <https://github.com/adbar/trafilatura/blob/master/tests/comparison.py>`_. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the *tests* directory.\n\nOther evaluations:\n^^^^^^^^^^^^^^^^^^\n\n- Most efficient open-source library in *ScrapingHub*'s `article extraction benchmark <https://github.com/scrapinghub/article-extraction-benchmark>`_\n- Best overall tool according to Gaël Lejeune & Adrien Barbaresi, `Bien choisir son outil d'extraction de contenu à partir du Web <https://hal.archives-ouvertes.fr/hal-02768510v3/document>`_ (2020, PDF, French)\n\n\nIn a nutshell\n-------------\n\nPrimary installation method is with a Python package manager: ``pip install trafilatura``. See `installation documentation <installation.html>`_.\n\nWith Python:\n\n.. code-block:: python\n\n    >>> import trafilatura\n    >>> downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n    >>> trafilatura.extract(downloaded)\n    # outputs main content and comments as plain text ...\n\nOn the command-line:\n\n.. code-block:: bash\n\n    $ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n    # outputs main content and comments as plain text ...\n\nFor more information please refer to `usage documentation <usage.html>`_ and `tutorials <tutorials.html>`_.\n\n\n.. raw:: html\n\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/rEOoItpzlVw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n\nLicense\n-------\n\n*Trafilatura* is distributed under the `GNU General Public License v3.0 <https://github.com/adbar/trafilatura/blob/master/LICENSE>`_. If you wish to redistribute this library but feel bounded by the license conditions please try interacting `at arms length <https://www.gnu.org/licenses/gpl-faq.html#GPLInProprietarySystem>`_, `multi-licensing <https://en.wikipedia.org/wiki/Multi-licensing>`_ with `compatible licenses <https://en.wikipedia.org/wiki/GNU_General_Public_License#Compatibility_and_multi-licensing>`_, or `contacting me <https://github.com/adbar/trafilatura#author>`_.\n\nSee also `GPL and free software licensing: What's in it for business? <https://web.archive.org/web/20230127221311/https://www.techrepublic.com/article/gpl-and-free-software-licensing-whats-in-it-for-business/>`_\n\n\n\nContext\n-------\n\n\nThese documentation pages also provide information on `concepts behind data collection <background.html>`_ as well as practical tips on how to gather web texts (see `tutorials <tutorials.html>`_).\n\n\n\nContributing\n~~~~~~~~~~~~\n\nContributions are welcome! See `CONTRIBUTING.md <https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md>`_ for more information. Bug reports can be filed on the `dedicated page <https://github.com/adbar/trafilatura/issues>`_.\n\nMany thanks to the `contributors <https://github.com/adbar/trafilatura/graphs/contributors>`_ who submitted features and bugfixes!\n\n\nRoadmap\n~~~~~~~\n\nFor planned enhancements and relevant milestones see `issues page <https://github.com/adbar/trafilatura/milestones>`_.\n\n\nAuthor\n~~~~~~\n\nThis effort is part of methods to derive information from web documents in order to build `text databases for research <https://www.dwds.de/d/k-web>`_ (chiefly linguistic analysis and natural language processing). Extracting and pre-processing web texts to the exacting standards of scientific research presents a substantial challenge for those who conduct such research. Web corpus construction involves numerous design decisions, and this software package can help facilitate text data collection and enhance corpus quality.\n\n\n- Barbaresi, A. `Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction <https://aclanthology.org/2021.acl-demo.15/>`_, Proceedings of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131.\n-  Barbaresi, A. \"`Generic Web Content Extraction with Open-Source Software <https://hal.archives-ouvertes.fr/hal-02447264/document>`_\", Proceedings of KONVENS 2019, Kaleidoscope Abstracts, 2019.\n-  Barbaresi, A. \"`Efficient construction of metadata-enhanced web corpora <https://hal.archives-ouvertes.fr/hal-01371704v2/document>`_\", Proceedings of the `10th Web as Corpus Workshop (WAC-X) <https://www.sigwac.org.uk/wiki/WAC-X>`_, 2016.\n\n\n.. image:: https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue\n    :target: https://aclanthology.org/2021.acl-demo.15/\n    :alt: Reference DOI: 10.18653/v1/2021.acl-demo.15\n\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3460969.svg\n   :target: https://doi.org/10.5281/zenodo.3460969\n   :alt: Zenodo archive DOI: 10.5281/zenodo.3460969\n\n\n.. code-block:: shell\n\n    @inproceedings{barbaresi-2021-trafilatura,\n      title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n      author = \"Barbaresi, Adrien\",\n      booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n      pages = \"122--131\",\n      publisher = \"Association for Computational Linguistics\",\n      url = \"https://aclanthology.org/2021.acl-demo.15\",\n      year = 2021,\n    }\n\n\nYou can contact me via my `contact page <https://adrien.barbaresi.eu/>`_ or on `GitHub <https://github.com/adbar>`_.\n\n\nSoftware ecosystem\n~~~~~~~~~~~~~~~~~~\n\n\n.. image:: software-ecosystem.png\n    :alt: Software ecosystem \n    :align: center\n    :width: 65%\n\n\n*Trafilatura*: `Italian word <https://en.wiktionary.org/wiki/trafilatura>`_ for `wire drawing <https://en.wikipedia.org/wiki/Wire_drawing>`_.\n\n`Known uses of the software <used-by.html>`_.\n\nCorresponding posts on `Bits of Language <https://adrien.barbaresi.eu/blog/tag/trafilatura.html>`_ (blog).\n\n\n\nFurther documentation\n=====================\n\n.. toctree::\n   :maxdepth: 2\n\n   installation\n   usage\n   tutorials\n   evaluation\n   corefunctions\n   used-by\n   background\n\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n"
  },
  {
    "title": "Index — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/genindex.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nIndex\nB | D | E | F | H | L | S | T | V | X\nB\nbare_extraction() (in module trafilatura)\n\t\nbaseline() (in module trafilatura)\nD\ndecode_response() (in module trafilatura.utils)\nE\nextract() (in module trafilatura)\n\t\nextract_comments() (in module trafilatura.core)\nextract_metadata() (in module trafilatura)\nF\nfetch_url() (in module trafilatura)\n\t\nfind_feed_urls() (in module trafilatura.feeds)\nfocused_crawler() (in module trafilatura.spider)\nH\nhtml2txt() (in module trafilatura)\nL\nload_html() (in module trafilatura)\nS\nsanitize() (in module trafilatura.utils)\n\t\nsitemap_search() (in module trafilatura.sitemaps)\nT\ntrim() (in module trafilatura.utils)\n\t\ntry_justext() (in module trafilatura.external)\ntry_readability() (in module trafilatura.external)\nV\nvalidate_tei() (in module trafilatura.xml)\nX\nxmltotxt() (in module trafilatura.xml)\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Working with corpus data — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/corpus-data.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nHighly targeted ads for devs Reach your exact developer niche with ML powered ads\nAd by EthicalAds   ·   ℹ️\nv: latest\nBackground\nWorking...\nWorking with corpus data\n\nAfter gathering texts from the Web, what to do next? This page lists options to work with output generated by Trafilatura.\n\nGeneric solutions in Python\nData science\nLoad the input into the data analysis library Pandas:\n\nread_csv\n\nread_json\n\nNatural language processing\n\nFor a first hand approach to NLP pipelines, see Textblob or the Natural Language Toolkit (NTLK).\n\nAccessible tutorials:\n\nPart-of-Speech Tagging\n\nTF-IDF with Scikit-Learn\n\nSpecific tools:\n\nTopic modeling, including word2vec models: Gensim tutorials\n\nScattertext is a tool for finding distinguishing terms in corpora, and presenting them in an interactive scatter plot.\n\nFormats and software used in corpus linguistics\n\nInput/Output formats: TXT, XML and XML-TEI are quite frequent in corpus linguistics.\n\nHan., N.-R. (2022). “Transforming Data”, The Open Handbook of Linguistic Data.\n\nThe XML and XML-TEI formats\n\nSee A Gentle Introduction to XML or the Python package xmltodict which provide ways to directly read the files and work with the data as if it were in JSON format.\n\nCorpus analysis tools\n\nAntconc is expected to work with TXT files\n\nCorpusExplorer supports CSV, TXT and various XML formats\n\nCorpus Workbench (CWB) uses verticalized texts whose origin can be in TXT or XML format\n\nLancsBox support various formats, notably TXT & XML\n\nTXM (textometry platform) can take TXT, XML & XML-TEI files as input\n\nVoyant support various formats, notably TXT, XML & XML-TEI\n\nWmatrix can work with TXT and XML\n\nWordSmith supports TXT and XML\n\nFurther corpus analysis software can be found on corpus-analysis.com.\n\nGeneric NLP solutions\n\nFor natural language processing see this list of open-source/off-the-shelf NLP tools for German and further lists for other languages.\n\nPrevious\n\nFinding sources for web corpora\n\n On this page\nGeneric solutions in Python\nData science\nNatural language processing\nFormats and software used in corpus linguistics\nGeneric NLP solutions\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Compendium: Web texts in linguistics and humanities — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/compendium.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nHighly targeted ads for devs Reach your exact developer niche with ML powered ads\nAd by EthicalAds   ·   ℹ️\nv: latest\nBackground\nCompendium:...\nCompendium: Web texts in linguistics and humanities\n\nA compendium is a concise collection of information pertaining to a body of knowledge.\n\nWeb corpora as scientific objects\n\nIn linguistics, a text corpus (plural corpora) is a language resource consisting of a structured set of texts. Nowadays, corpora are mostly electronically stored and processed. By nature, web corpora are digital by default. They can be considered to be a “collection of linguistic data that are directly observable” and as such “naturalistic data”, that is data “intended to be reflective of actual language use” (Good 2022).\n\n“A corpus is simply described as a large body of linguistic evidence typically composed of attested language use. […] The term corpus should properly only be applied to a well-organized collection of data.” (McEnery 2003, p. 449)\n\nHow well defined corpora should be, depends on research tradition and practices. Web corpora are the heirs of established corpora and they mostly undergo the same construction process, with necessary adapations and novelties (see Barbaresi 2015, chapter 1).\n\nFor Baroni & Ueyama (2006), corpora bring “actual usage evidence”, “collections of language samples produced in natural contexts and without experimental interference”. They distinguish four main cases where such evidence is needed: theoretical and applied linguistic questions, simulations of language acquisition, lexicography, and a large number of tasks in natural language processing.\n\n“First, [Internet corpora] can be searched using linguistic criteria, at least with respect to lemmas and POS tags […]. Second, it is possible to produce sets of collocates […]. It is also possible to obtain other statistical data from Internet corpora, such as word and n-gram frequencies.” (Sharoff 2006)\n\nCorpus types and resulting methods\n\n“Documentary and corpus data are generally likely to be usable to support a wide range of investigations across more than one linguistic subfield, though data of such kinds could also be collected to serve a fairly narrow purpose depending on the research practices adopted.” (Good 2022)\n\nMajor fault lines:\n\nGeneral vs. specialized corpora (Gries & Newman 2014)\n\nGeneral-purpose vs. special-purpose (Baroni & Ueyama 2006), ad hoc vs. general-purpose web corpora (Barbaresi 2015)\n\n“Miniweb” vs. domain specific web corpora (WebCorp LSE typology)\n\nCorpus construction methods:\nTailor-made corpus\n\nuse of already known sources only\n\nPartial control and/or “corpus seeds” (focused crawling)\n\nknown corpus seeds & use of criteria to restrict the search space (using structural properties like domains or content-aware criteria like topics)\n\nuse of search engines via focused searches or BootCaT method (Baroni & Bernardini 2004), see blog post Replicating the BootCat method to build web corpora from search engines\n\n“Open-end” approach & automated web browsing (web crawling, broad crawls)\n\nneeds “crawling seeds”, hops from page to page and from one website to another\n\nGeneral-purpose corpora\n\nGeneral-purpose corpora are supposed to encompass a large amount of texts and a gamut of text types and text genres. Their significance arises from it, which can make them representative in some way. Corpus designers usually rely on the fact that potential small irregularities are going to be smoothed out by the sheer number of texts, so that empirical findings in the corpus are expected to be statistically relevant all the same. The goal for linguists is to get a statistical perspective on norms.\n\nSuch corpora can also strive to be representative of a genre or of a particular source, in the case of web corpus, like a Mini web, because the Web is too large to be completely retrieved and stored in a database (see Tanguy 2013).\n\nThey are often found at dedicated research institutions, as the building and maintenance is costly in time and resources. In the case of web corpora, this involves first an extensive web crawling phase, using mostly breadth-first techniques. Second, the text pre-processed. Meaning that a selection of resources of the documents or relevant extracts. Finally, loaded into corpus tool, which in that case, mostly involves tailored database applications.\n\nSpecialized corpora\n\nOn the second hand, there are specialized corpora which focus on a particular genre or or a particular source. They can be opportunistic in nature but they mostly involve prior knowledge of the contents and also a certain amount of control over what comes into the corpus. Contrarily to open ended-corpora, the goal for linguists is to get a better coverage of particular linguistic settings or phenomena:\n\nThe purpose of focused web corpora is to complement existing collections, as they allow for better coverage of specific written text types and genres, especially the language evolution seen through the lens of user-generated content, which gives access to a number of variants, socio- and idiolects, for example in the case of blogs (Barbaresi 2019).\n\nCorpus building comprises three phases:\n\nFirst, the texts are discovered and listed.\n\nThen they are downloaded, possibly using web crawling techniques which are not as extensive as in the other case since it is mainly about fetching and processing.\n\nFinally a processed version is stored, which is in itself the linguistic corpus. It can be indexed by a corpus-query tool or be made available using standardized formats used by the research Community such as XML or XML TEI.\n\nHint\n\nFor more information, see the tutorial Gathering a custom web corpus\n\nIn-between\n\nThe distinction is not always clear-cut, web corpora can target a particular set of web pages while keeping a more generalist approach:\n\n“Manually selecting, crawling and cleaning particular web sites with large and good-enough-quality textual content.” (Spoustová & Spousta 2012, see also Review of the Czech Internet corpus)\n\n“One possibility to address the difficulty to find good sources is to avoid “wide crawling” but instead to bind the crawler to manually identified Web domains which are updated on a regular basis and which offer textual data of good quality (this can be seen as ‘vertical crawling’ as opposed to ‘horizontal’ or ‘wide crawling’).” “Do not sort a list of retrieved documents, but retrieve a list of sorted documents.” (Fairon 2006)\n\nMonitor corpora\n\nTerm coined by Sinclair (1982) and proposal motivated by increasing text availability over time, with the following expectations (Clear 1987):\n\nMore information about language\n\nBetter statistical foundation\n\nA diachronic perspective\n\nMore effective use of computer facilities\n\nDoes that imply that a monitor corpus is a sliding window? Opinions may differ:\n\n“It would be absurd for entymologists studying beetles to keep in the laboratory every single beetle that has ever been looked at; if they want to see more beetles then there are plenty of them in the wild. So it is with words: only the rare and interesting specimens need to be kept in matchboxes.” (Clear 1987)\n\nCorpus size\n\n“Corpora that have been used for lexicographical purposes – looking at the whole language – have, perhaps by necessity, always been created to be as large as possible. However, the need for smaller corpora – looking at specific areas of the language – has been recognised, especially in relation to teaching and use in the language classroom.” (Nelson 2010)\n\nMost agree on the fact that quality and adequateness of the data relatively to the research objectives are at least as important as the sheer size.\n\nCorpus construction steps\nPreliminary questions\n\nNelson (2010) formulates the following questions which should be answered before building a corpus:\n\n“Why build a new corpus?” / “What do you want to achieve?” / “What will it be used for?”\n\n“Will you stick to [the] plan rigidly, or will you take texts from where you can readily get them?”\n\nWorking like search engines\n\nIn a seminal article, Adam Kilgarriff sums up issues related to the “low-entry-cost way to use the Web” which commercial search engines represent. He shows how an alternative can be developed within the academic community.\n\n“An alternative is to work like the search engines, downloading and indexing substantial proportions of the World Wide Web, but to do so transparently, giving reliable figures, and supporting language researchers’ queries.”\n\n“The process involves crawling, downloading, ’cleaning’ and de-duplicating the data, then linguistically annotating it and loading it into a corpus query tool.” (Kilgarriff 2007)\n\nBased on these steps, three distinct phases can be distinguished:\n\nWeb crawling determines the range and the general contents of a web corpus\n\nData pre-processing impacts all the other steps downstream see following section\n\nLinguistic annotation and query tools give profile to the data, they can make certain features noticeable while blurring others (Anthony 2013)\n\nHandling of steps (1) & (2) is the primary motivation behind the development of the Trafilatura software package.\n\nHint\n\nFor more information on the article mentioned above see the blog post “Googleology is bad science”: Anatomy of a web corpus infrastructure.\n\nIn case a search engine is used to find documents, pre- and post-processing phases may differ.\n\n“A large reference corpus for an arbitrary language can be collected in six steps: word selection, query generation, downloading from the Internet, post-processing, composition assessment and comparison of word lists (the last two steps are optional).” (Sharoff 2006)\n\nN.B.: unlike in BootCat corpora, in this approach words from the general lexicon as used as seeds.\n\nIn the case of a corpus based on web feeds, four steps can be distinguished: feed discovery, validation, scheduling and crawling (Minocha et al. 2013).\n\nCrawling and download\n\nIf web pages are to be discovered, an automatic way of navigating the Web is needed, that is web crawling. It is possible to crawl a given website or the Web as a whole.\n\n“A crawler starts off with the URL for an initial page \n𝑃\n0\n. It retrieves \n𝑃\n0\n, extracts any URLs in it, and adds them to a queue of URLs to be scanned. Then the crawler gets URLs from the queue (in some order), and repeats the process.” (Cho et al. 1998)\n\nSuitable texts for inclusion into corpora are not evenly distributed across the Internet. Pre-selecting documents according to certain criteria can be crucial. This concerns both content discovery and corpus creation for which URLs are the most practical hint (see Finding sources for web corpora).\n\n“It is important for the crawler to visit “important” pages first, so that the fraction of the Web that is visited […] is more meaningful.” (Cho et al. 1998)\n\nThe corpus construction strategy usually follows from the chosen corpus type, one can decide to retrieve a whole website or just targeted URLs.\n\n“Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.” (Edwards et al. 2001)\n\nCertain indicators can be applied while scouting the Web and potentially affect the course of events, such as language identification in order to keep the crawl language-focused (Jauhiainen et al. 2020).\n\nCrawling efficiency is usually understood as a “yield rate”, i.e. as a measure of residual corpus size vs. computational power invested.\n\nHint\n\nSee documentation on crawls and downloads as well as the “brain” for web crawling tasks: the library courlan.\n\nWeb scraping and data cleaning\n\nWhile web crawling focuses on hopping from page to page, retrieving the content and collecting especially links and potentially other information, web scraping describes the automatic extraction of targeted information on particular sections of a page.\n\nIn the context of web corpora, one’s interest resides in finding texts and relevant metadata. Web scraping thus implies to download web pages (or to open locally stored ones) and to strip them of all unnecessary content in order to obtain a clean document which can be passed to linguistic analysis tools.\n\n“Cleaning is a low-level, unglamorous task, yet crucial: The better it is done, the better the outcomes. All further layers of linguistic processing depend on the cleanliness of the data.” (Kilgarriff 2007)\n\nWhen confronted with web pages, the main issues affecting the content can be summarized as follows:\n\nHow do we detect and get rid of navigation bars, headers, footers, etc.?\n\nHow do we identify metadata, paragraphs and other structural information?\n\nHow do we produce output in a standard form suitable for further processing?\n\nOn site level, recurring elements are called boilerplate. Removing them allow for avoiding hundreds of occurrences of phrases like “back to the main page” or “Copyright 2022 (site name)”.\n\nPreserving some elements of the page structure can be useful to distinguish main text, quotes and comments. Authorship definitely is meaningful in a humanities context. Metadata such as the page title or the publication date are also quite relevant.\n\nOptional step: further post-processing (notably removal of near duplicates).\n\nFor concrete steps see usage.\n\nPost hoc evaluation\n\nFor practical reasons, web corpus construction partly relies on the assumption that “the Web is a space in which resources are identified by Uniform Resource Identifiers (URIs).” (Berners-Lee et al., 2006) The Web is however changing faster than the researchers’ ability to observe it (Hendler et al., 2008), and a constant problem faced by web resources resides in meta-information and categorization.\n\nThe actual contents of a web corpus can only be listed with certainty once the corpus is complete. In addition to the potential lack of information concerning the metadata of the texts, there is a lack of information regarding the content, whose adequacy, focus and quality has to be assessed in a post hoc evaluation (Baroni et al., 2009).\n\nThat is why web texts can and should be further examined and prepared for inclusion into a linguistic corpus. The gathered documents should be controlled at least on a sample basis. Ideally, the corpus should undergo a qualitative screening, examination using quantitative criteria is easier to handle using machines, be it with statistical indicators (such as text length, frequent n-grams) or with content-based heuristics (for example using metadata or text analysis). Language identification is also best performed on clean text.\n\nNote\n\nFurther text quality criteria are discussed in Schäfer & Bildhauer (2013) chapter 5, and Barbaresi (2015) chapter 2.\n\nA different series of questions arise when randomly searching for text on the Internet: What is a text? When does input stop to be a text? What should be included in the corpus? Sometimes the results are bounded by certain texts types (like classified ads) or by the toolchain (with scraps of text coming from the tools). See the challenges and indicators described in Kilgarriff & Suchomel (2013), Schäfer et al. (2013). For work on corpus description in terms of genres see Sharoff (2018).\n\nQuality assessments can also be made in the context of distributional semantics, for example by creating vectors of co-occurring lemmas (Versley & Panchenko 2012). Evert (2015) lists the following approaches: statistical properties, corpus comparison, use as training data in NLP, and linguistic evaluation (e.g. frequency comparisons).\n\nMethodological issues\nLimited time or ressources\n\nCreating a corpus means finding a “balance academic integrity with practical realities, accuracy with expediency and size with efficiency” (Nelson 2010).\n\nData sparsity\n\nThe above deals with texts published in the form of web pages. There are also a number of platforms and social networks which sadly cannot be comprehensively studied without the agreement of the company running them. It is although possible to gather data on a smaller scale (Barbaresi 2016).\n\nThe Web constantly evolves and hyperlinks cannot be expected to remain stable in time. Page display is also affected by technological or commercial evolutions, for example prominent news outlets may disappear behind pay walls. See the Wikipedia page on link rot for ideas on how to prevent it (chiefly using clean URLs and web archives).\n\nSuperfluous data\n\n“Web spam is a large and growing problem for web corpus builders.” (Kilgarriff & Suchomel 2013)\n\n“In contrast to the traditional or search engine definitions of web spam, the corpus use point of view is not concerned with intentions of spam producers or the justification of the search engine optimisation of a web page. A text corpus built for NLP or linguistics purpose should contain coherent and consistent, meaningful, natural and authentic sentences in the target language. Only texts created by spamming techniques breaking those properties should be detected and avoided.” (Suchomel 2020)\n\nTechnicalities\n\nTechnical problems are mostly related to communications over the network and text processing. For smaller projects running from a single computer, bandwidth and RAM are two main limitations. For larger projects, the capacity to scale crawling and processing operations across servers is paramount. This notably includes the capacity to control when web servers are contacted, to what extent web pages can be processed on the fly, and how the resulting corpus data is stored and eventually indexed.\n\nAccess and ownership\n\nThe growing digitization of text production and processing leaves us with remaining leagal issues:\n\n“The speed of technological advance has left us with an ethical and legal confusion over the ownership of information, which is hindering the acquisition of text.” (Clear 1987)\n\nIn the USA, the Van Buren v. United States case has been used to defend the right to scrape publicly available web pages, i.e. without bypassing authentication or going off-limits.\n\nIn addition, sharing URL lists rather than full documents can alleviate copyright issues raised by access to web corpora:\n\n“Given that the list of links in an Internet corpus does not give the impression that websites it points to are the same as the URL list itself nor compete with services provided by respective websites, a corpus in the form of lists of URLs is not subject to copyright restrictions.” (Sharoff 2006)\n\nReferences\n\nAnthony, L. (2013). A critical look at software tools in corpus linguistics. Linguistic Research, 30(2), 141-161.\n\nBarbaresi, A. (2015). Ad hoc and general-purpose corpus construction from web sources (Doctoral dissertation, ENS Lyon).\n\nBarbaresi, A. (2016). Collection and indexing of tweets with a geographical focus. In Proceedings of the 4th Workshop on Challenges in the Management of Large Corpora (CMLC-4), pp. 24-27.\n\nBarbaresi, A. (2019). The Vast and the Focused: On the need for thematic web and blog corpora. In Proceedings of the 7th Workshop on Challenges in the Management of Large Corpora (CMLC-7), Leibniz-Institut für Deutsche Sprache, pp. 29-32.\n\nBaroni, M., & Bernardini, S. (2004). BootCaT: Bootstrapping Corpora and Terms from the Web. In Proceedings of LREC 2004, pp. 1313-1316.\n\nBaroni, M., & Ueyama, M. (2006). Building general- and special-purpose corpora by Web crawling. In Proceedings of the 13th NIJL international symposium, Language Corpora: Their compilation and application (pp. 31–40).\n\nBaroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. (2009). The WaCky Wide Web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3), 209-226.\n\nBerners-Lee, T., Hall, W., & Hendler, J. A. (2006). A Framework for Web Science. Foundations and Trends in Web Science, 1, 1, 1–130.\n\nCho, J., Garcia-Molina, H., & Page, L. (1998). Efficient crawling through URL ordering. Computer networks and ISDN systems, 30(1-7), 161–172.\n\nClear, J. (1987). Trawling the language: monitor corpora. ZURILEX Proceedings. Tübingen: Francke.\n\nEdwards, J., McCurley, K. S., and Tomlin, J. A. (2001). “An adaptive model for optimizing performance of an incremental web crawler”. In Proceedings of the 10th international conference on World Wide Web - WWW ‘01, pp. 106–113.\n\nEvert, S. (2015). An NLP approach to the evaluation of Web Corpora. Evaluation, (1/64).\n\nFairon, C. (2006). Corporator: A tool for creating RSS-based specialized corpora. In Proceedings of the 2nd International Workshop on Web as Corpus.\n\nGood, J. (2022). “The Scope of Linguistic Data”, In The Open Handbook of Linguistic Data Management, MIT Press, 27-47.\n\nGries, S. T., & Newman, J. (2014). Creating and using corpora. In Research methods in linguistics, Podesva, R.J., & Sharma, D. (eds.), 257-287.\n\nJauhiainen, H., Jauhiainen, T., & Lindén, K. (2020). Building web corpora for minority languages. In Proceedings of the 12th Web as Corpus Workshop, pp. 23-32.\n\nKilgarriff, A. (2007). Googleology is bad science. Computational linguistics, 33(1), 147-151.\n\nKilgarriff, A. and Suchomel, V. (2013) “Web Spam”. In: Proceedings of the 8th Web as Corpus Workshop (WAC-8), Corpus Linguistics 2013, pp. 46–52.\n\nMcEnery, T. (2003). Corpus Linguistics. In R. Mitkov (Ed.), The Oxford Handbook of Computational Linguistics (pp. 448–463). Oxford University Press.\n\nMinocha, A., Reddy, S., & Kilgarriff, A. (2014). Feed Corpus: an ever growing up-to-date corpus. Proceedings of the 8th Web as Corpus Workshop, pp. 1-4, ACL SIGWAC.\n\nNelson, M. (2010). Building a written corpus. The Routledge Handbook of Corpus Linguistics, 53-65.\n\nSchäfer, R., Barbaresi, A., & Bildhauer, F. (2013). The Good, the Bad, and the Hazy: Design Decisions in Web Corpus Construction. In 8th Web as Corpus Workshop, pp. 7-15, ACL SIGWAC.\n\nSchäfer, R., & Bildhauer, F. (2013). Web Corpus Construction. Morgan & Claypool.\n\nSharoff, S. (2006). Open-source corpora: Using the net to fish for linguistic data. International journal of corpus linguistics, 11(4), 435-462.\n\nSharoff, S. (2018). Functional text dimensions for the annotation of web corpora. Corpora, 13(1), 65-95.\n\nSinclair, J. (1982). Reflections on computer corpora in English language research. Computer corpora in English language research, 1-6.\n\nSpoustová, J., & Spousta, M. (2012). A High-Quality Web Corpus of Czech. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pp. 311-315.\n\nSuchomel, V. (2020). Better Web Corpora For Corpus Linguistics And NLP (Doctoral dissertation, PhD thesis, Masaryk University).\n\nTanguy, L. (2013). La ruée linguistique vers le Web. Texto! Textes et Cultures, 18(4).\n\nVersley, Y., & Panchenko, Y. (2012). Not just bigger: Towards better-quality Web corpora. In Proceedings of the seventh Web as Corpus Workshop (WAC7), pp. 44-52.\n\nPrevious\n\nBackground\n\nNext\n\nFinding sources for web corpora\n\n On this page\nWeb corpora as scientific objects\nCorpus types and resulting methods\nCorpus construction steps\nMethodological issues\nReferences\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Tutorial: DWDS-Korpusdaten reproduzieren — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorial-dwds.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nHighly targeted ads for devs Reach your exact developer niche with ML powered ads\nAd by EthicalAds   ·   ℹ️\nv: latest\nTutorials\nTutorial:...\nTutorial: DWDS-Korpusdaten reproduzieren\nZiel\n\nDie Korpusdaten des Digitalen Wörterbuchs der deutschen Sprache sind mithilfe eines externen Tools reproduzierbar. Anhand dieser Anleitung können Sie Daten unabhängig vom DWDS-Projekt selbst zusammenstellen, speichern und verwalten. Die dafür benötigten Tools sind frei verfügbar und erfordern keine fortgeschrittenen IT-Kenntnisse.\n\nDas grundsätzliche Problem besteht darin, dass die Originalsammlung selbst nicht uneingeschränkt kopiert und weitergegeben werden darf. Die Nutzung des DWDS-Portals als Suchmaschine ist aber möglich, ebenso wie das Tätigen eigenhändiger Downloads, um die Sammlung zu replizieren. Auf diesem Wege muss kein Zugang zu den Rohdaten gewährt und keine unmittelbare Kopie erzeugt werden.\n\nVon einer Abfrage zur Einsicht der Quellen\n\nNote\n\nUm die meisten Webkorpora des DWDS online abfragen zu können, ist eine kostenlose Anmeldung notwendig: Jede/r kann sich beim DWDS-Portal registrieren oder anmelden.\n\nErgebnisse zur weiteren Nutzung exportieren\n\nMithilfe der Export-Funktion können Links aus den Trefferlisten zur Basis eines eigenen Korpus oder Subkorpus werden.\n\nIn jedem DWDS-Korpus können Sie die Ergebnisse einer Abfrage exportieren. So können Trefferlisten aus der DWDS-Plattform weiter bearbeitet und ausgewertet werden. Wenn Sie auf den blauen Knopf „Treffer exportieren“ klicken, haben Sie die Wahl zwischen mehreren Formaten.\n\nTrefferliste im DWDS-Portal und Knopf „Treffer exportieren“\n\nCSV- oder TSV-Dateien können von der frei verfügbaren Software LibreOffice Calc sowie von Microsoft Excel oder Apple Numbers geöffnet werden. Die Quellen (URLs) werden in einer Spalte aufgelistet und können dann als getrennte Liste anderswo gespeichert werden.\n\nKontextmenü „Treffer exportieren“ und Wahl des Ausgabeformats\n\nVon einem Webkorpus zu URL-Listen\n\nAlternativ können Sie mit dieser besonderen Art der Abfrage URLs in gebündelter Form im TSV-Format exportieren. Damit kommen Sie zu einer Liste von Quellen, die zur weiteren Nutzung auch heruntergeladen werden kann.\n\nIn dieser Form gilt das nur für Webkorpora. Im Übrigen ist es auch möglich, Informationen aus unterschiedlichen Metadatenfeldern zu zählen, siehe die entsprechende Dokuseite.\n\nTrefferliste nach Quelle sortiert\n\nNachdem Sie die CSV- oder TSV-Datei mit der Tabellenkalkulationssoftware Ihrer Wahl geöffnet haben, können Sie die URL-Spalte auswählen und in einer TXT-Datei kopieren, die Sie als Eingabe für Trafilatura verwenden werden (siehe unten).\n\nInteresse und Gestaltungsmöglichkeiten\n\nAnhand von solchen URL-Listen haben Sie zwei Möglichkeiten, Sie können:\n\nKorpusdaten reproduzieren, sofern die Seiten noch verfügbar oder in Archiven zu finden sind;\n\nEin maßgeschneidertes Korpus auf der Basis einer DWDS-Abfrage zusammenstellen.\n\nSo wird die DWDS-Plattform zu einer Art Meta-Suchmaschine. Der Vorteil besteht darin, dass Sie nicht von dem wilden Web abhängig sind, sondern in allgemeinen oder thematischen Sammlungen suchen, die hinsichtlich ihrer Relevanz geprüft worden sind. Außerdem wird die Datenmenge dadurch übersichtlicher.\n\nHint\n\nHier finden Sie eine Liste der Webkorpora auf der DWDS-Plattform.\n\nBei größeren Webkorpora ist die Filterung hinsichtlich der Relevanz und der Textqualität meistens quantitativer Natur, siehe Barbaresi 2015 (Diss.) Kapitel 4 für Details. Im Übrigen haben wir das Schlimmste aus dem Web manuell ausgegrenzt.\n\nDownload und Verarbeitung der Daten\n\nFür die eigenhändige Zusammenstellung von Korpusdaten brauchen Sie:\n\nGrundkenntnisse im Umgang mit Python, R oder der Kommandozeile (siehe Hinweise zur Nutzung der Kommandozeile oder diese Einführung in die Kommandozeile auf Deutsch);\n\nEine aktuelle Version der Software Trafilatura, siehe Installation.\n\nIm Grunde geben Sie Links (URLs) in der Form einer Liste ein und erhalten als Ausgabe eine Reihe von Dateien als TXT, CSV oder XML. Für weitere Informationen können Sie die folgende Anleitung sowie diese Dokumentationsseiten auf Englisch lesen:\n\nTutorial zum Korpusaufbau;\n\nDownload und Verarbeitung mit Python, R, auf der Kommandozeile oder mit einer graphischen Oberfläche.\n\nMehrfach vorhandene URLs in der Eingabeliste werden automatisch dedupliziert und die Reihenfolge der Downloads wird optimiert, Sie müssen diese Schritte also nicht selber durchführen.\n\nFalls sich die betroffenen Webseiten in der Zeit zwischen der DWDS-Archivierung und Ihrem Download nicht geändert haben, erhalten Sie genau die gleichen Daten. Alternativ können Sie eine archivierte Version der Seiten verwenden, sofern sie in Internet Archiven zu finden sind.\n\nBeispiel: Wie kann man die Seiten herunterladen, speichern und konvertieren?\n\nHier ist eine Empfehlung für die Kommandozeile, die eine Datei namens linkliste.txt liest (eine URL pro Zeile).\n\nDiese Linkliste kann zunächst gefiltert werden, um deutschsprachige, inhaltsreiche Webseiten zu bevorzugen. Der dafür nötige Softwareteil, courlan wird mit Trafilatura installiert:\n\ncourlan --language de --strict --inputfile linkliste-roh.txt --outputfile linkliste-gefiltert.txt\n\nDie Ausgabe von Trafilatura erfolgt auf zweierlei Weise: die extrahierten Texte (TXT-Format) im Verzeichnis ausgabe und eine Kopie der heruntergeladenen Webseiten unter html-quellen (zur Archivierung und ggf. erneuten Verarbeitung):\n\ntrafilatura --input-file linkliste.txt --outputdir ausgabe/ --backup-dir html-quellen/\n\nSo werden TXT-Dateien ohne Metadaten ausgegeben. Wenn Sie --csv, --json, --xml oder --xmltei hinzufügen, werden Metadaten einbezogen und das entsprechende Format für die Ausgabe bestimmt. Zusätzliche Optionen sind verfügbar, siehe die passenden Dokumentationsseiten.\n\nFür bis zu einige Tausend URLs gelingt dieses Verfahren problemlos von einem Laptop aus, für mehr URLs kann ein Server notwendig sein, vor allem um lange Wartezeiten zu handhaben (zunächst werden die Seiten nämlich heruntergeladen).\n\nPrevious\n\nText embedding\n\nNext\n\nEvaluation\n\n On this page\nZiel\nVon einer Abfrage zur Einsicht der Quellen\nInteresse und Gestaltungsmöglichkeiten\nDownload und Verarbeitung der Daten\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Finding sources for web corpora — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/sources.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nHighly targeted ads for devs Reach your exact developer niche with ML powered ads\nAd by EthicalAds   ·   ℹ️\nv: latest\nBackground\nFinding...\nFinding sources for web corpora\nFrom link lists to web corpora\nURLs and corpora\n\nGetting a list of web pages to start from is essential in order to build document collections. The latter are often called web corpora by linguists.\n\nThe former lists of links (also known as URL lists) can be used in two different ways: first, to build a corpus straight from the pages they link to, and second, to start web crawls and hopefully discover other relevant pages. On the one hand, corpus sources are restricted to a fixed list, and on the other hand, one looks opportunistically for more content without knowing everything in advance.\n\nThe issue with the sources\n\nThe question of web corpus sources does not stop there. Indeed, one does not necessarily know where to look for interesting websites, i.e. “seeds” to start from.\n\nThe answers to this issue that are frequently found in the literature are twofold: either one initiates a web crawling phase with a (small or large) list of websites, or one uses already existing link collections. Note that both strategies can also complement each other and be used alternatively during different corpus construction phases.\n\nAs with “traditional” corpora, web corpora can either focus on a given range of websites and topics, or be merely language-minded and opportunistically take all kinds of possible texts into account. In the latter case, using diverse sources for URL seeds could ensure there is no potentially unknown bias.\n\nExisting resources\nCorpora\n\nURL lists from corpus linguistic projects can be a starting ground to derive information from, either to recreate existing corpora or to re-crawl the websites and find new content. If the websites do not exist anymore, the links can still be useful as the corresponding web pages can be retrieved from web archives.\n\nSources for the Internet Corpora of the Leeds Centre for Translation Studies\n\nLink data sets of the COW project\n\nURL directories\n\nOverview of the Web archiving community\n\nlazynlp list of sources\n\nDMOZ (now an archive) and Wikipedia work quite well as primary sources:\n\nQualification of URLs extracted from DMOZ and Wikipedia (PhD thesis section)\n\nSearching for URLs\n\nThe Common Crawl is a good place to start looking for already known URLs, and possibly for the corresponding pages stored by the project. So is the Internet Archive (with a different focus):\n\nCommonCrawl index\n\ncdx_toolkit (toolkit for CDX indices such as Common Crawl and the Internet Archive’s Wayback Machine) & Python example\n\nPython script to extract all URLs known by the Internet Archive for a given domain\n\nRelated info: before retrieving them later, storing web documents in Internet archives can be fruitful, see for instance the tool archivenow.\n\nWith particular filters, one may look for specific kinds of sources as well, here is for instance a regular expression targeting feeds, as used in a study on web syndication feeds:\n\n\"(<link[^>]*(?:\\s(?:type=[\\\"']?(application\\/rss\\+xml|application\\/atom\\+xml|application\\/rss|application\\/atom|application\\/rdf\\+xml|application\\/rdf|text\\/rss\\+xml|text\\/atom\\+xml|text\\/rss|text\\/atom|text\\/rdf\\+xml|text\\/rdf|text\\/xml|application\\/xml)[\\\"']?|rel=[\\\"']?(?:alternate)[\\\"']?))[^>]*>)\"\n\n\nDiscovering feeds on social networks can also be used for corpus construction (Minocha et al. 2013).\n\nSearch engines\n\nThe BootCat approach (Baroni & Bernardini 2004) uses randomly generated search engines queries and gathers the links in the results (seed URLs). The queries consist of several randomly combined word seeds.\n\nHere is how to make this method work in a modular way:\n\nFirst, you need a list of words in the target language(s). For German see for instance the DWDS list.\n\nThen, draw random word tuples, e.g. with Python:\n\n>>> import random\n# use the list gathered in (1)\n>>> wordlist = ['word1', 'word2', 'word3', 'word4']  # and so on\n# draw 3 random words from the list\n>>> selection = random.sample(wordlist, k=3)\n\n\nGet URL results from search engines for the random tuples. Here are examples of Python modules to query search engines: search-engine-parser and GoogleScraper.\n\nOne of the main drawbacks of the BootCaT method is that it is not stable in time, both search engines and scraper modules may not work as intended anymore. In that case it would be necessary to look for alternatives, look for concepts like “SERP” and “search engines scraping”.\n\nDownload and process the link list with Trafilatura, see usage.\n\nHint\n\nFor more information, see the corresponding blog post: Replicating the BootCat method to build web corpora from search engines.\n\nSelecting random documents from the Web\n\nA model for web texts is described along with some experiments in the PhD thesis preceding the work on this library. Here are criteria you could use:\n\nGeneral text form, line and sentences lengths, etc.\n\nProportion of discourse and temporal markers\n\nFor more see Indicators for intrinsic quality assessment (section of PhD thesis).\n\nSee also the blog post What is good enough to become part of a web corpus?\n\nSocial networks\n\nSeries of surface scrapers that crawl the networks without even logging in, thus circumventing the API restrictions. Development of such software solutions is fast-paced, so no links will be listed here at the moment.\n\nPreviously collected tweet IDs can be “hydrated”, i.e. retrieved from Twitter in bulk. see for instance:\n\nTwitter datasets for research and archiving\n\nSearch GitHub for Tweet IDs\n\nLinks can be extracted from tweets with a regular expression such as re.findall(r'https?://[^ ]+'). They probably need to be resolved first to get actual link targets and not just shortened URLs (like t.co/…).\n\nFor further ideas from previous projects see references below.\n\nRemarks\n\nFor relatively small and focused corpora, human supervision is key. It is advisable to keep an eye on all steps of corpus construction.\n\nA crawling method using diverse seeds for corpus building can yield better results and notably ensure better randomness in a population of web documents (see Henzinger et al. 2000).\n\nScreening and refining the lists of URLs you use for your projects can also enhance corpus quality, see for example the implementation details in the papers mentioned below as well as the filtering tool courlan included with Trafilatura.\n\nThe following blog posts give more insights on aspects of web corpus construction:\n\nChallenges in web corpus construction for low-resource languages\n\nFinding viable seed URLs for web corpora\n\nReferences\n\nBarbaresi, A. (2014). Finding viable seed URLs for web corpora: a scouting approach and comparative study of available sources. In 9th Web as Corpus Workshop (WaC-9), 14th Conference of the European Chapter of the Association for Computational Linguistics (pp. 1-8).\n\nBarbaresi, A. (2015). Ad hoc and general-purpose corpus construction from web sources (Doctoral dissertation, ENS Lyon).\n\nBarbaresi, A. (2016). Collection and indexing of tweets with a geographical focus. In Proceedings of CMLC workshop, 10th International Conference on Language Resources and Evaluation (LREC 2016), pp. 24-27.\n\nBaroni, M., & Bernardini, S. (2004). BootCaT: Bootstrapping Corpora and Terms from the Web. In Proceedings of LREC 2004 (pp. 1313-1316).\n\nBerners-Lee, T., Hall, W., & Hendler, J. A. (2006). A framework for web science. Found. Trends Web Sci. 1, 1, 1–130.\n\nBlombach, A., Dykes, N., Heinrich, P., Kabashi, B., & Proisl, T. (2020). A corpus of German Reddit exchanges (GeRedE). In Proceedings of the 12th Language Resources and Evaluation Conference (pp. 6310-6316).\n\nHenzinger, M. R., Heydon, A., Mitzenmacher, M., & Najork, M. (2000). On near-uniform URL sampling. Computer Networks, 33(1-6), 295-308.\n\nJauhiainen, H., Jauhiainen, T., & Lindén, K. (2020). Building web corpora for minority languages. In Proceedings of the 12th Web as Corpus Workshop (pp. 23-32).\n\nMinocha, A., Reddy, S., & Kilgarriff, A. (2014). Feed Corpus: an ever growing up-to-date corpus. Proceedings of the 8th Web as Corpus Workshop, pp. 1-4, ACL SIGWAC.\n\nSchäfer, R., Barbaresi, A., & Bildhauer, F. (2014). Focused web corpus crawling. In Proceedings of the 9th Web as Corpus workshop (WAC-9), pp. 9-15.\n\nPrevious\n\nCompendium: Web texts in linguistics and humanities\n\nNext\n\nWorking with corpus data\n\n On this page\nFrom link lists to web corpora\nURLs and corpora\nThe issue with the sources\nExisting resources\nSearch engines\nSelecting random documents from the Web\nSocial networks\nRemarks\nReferences\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Text embedding — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorial-epsilla.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nTutorials\nText embedding\nText embedding\nWhy perform text embedding with crawled data?\n\nIf you are doing natural language research, you may want to perform text embeddings on text crawled with Trafilatura.\n\nText embedding involves converting text into numerical vectors, and is commonly used for\n\nSearch (rank results by a query string)\n\nClustering (group text strings by similarity)\n\nAnomaly detection (identify outliers)\n\nIn this tutorial, we will show you how to perform text embedding on results from Trafilatura. We will use Epsilla, an open source vector database for storing and searching vector embeddings. It is 10x faster than regular vector databases for vector operations.\n\nNote\n\nFor a hands-on version of this tutorial, try out the Colab Notebook.\n\nSetup Epsilla\n\nIn this tutorial, we will run an Epsilla databse server. You can start one locally with a Docker image.\n\n$ docker pull epsilla/vectordb\n$ docker run --pull=always -d -p 8888:8888 epsilla/vectordb\n\n\nSee Epsilla documentation for a full quick start guide.\n\nWe need to install the database client. You can do this with pip:\n\n$ pip install -U pyepsilla\n\n\nWe will also install langchain to use the open source BGE embedding model.\n\n$ pip install -U langchain sentence_transformers\n\n\nWe can now connect to the demo server.\n\nfrom pyepsilla import vectordb\nclient = vectordb.Client(\n    # replace with a production server if not running a local docker container\n    host='localhost',\n    port='8888'\n)\n\nstatus_code, response = client.load_db(\n    db_name=\"TrafilaturaDB\",\n    db_path=\"/tmp/trafilatura_store\"\n)\nprint(response)\n\nclient.use_db(db_name=\"TrafilaturaDB\")\n\n# creates a table called Trafilatura\nclient.drop_table('Trafilatura')\nclient.create_table(\n  table_name=\"Trafilatura\",\n  table_fields=[\n    {\"name\": \"ID\", \"dataType\": \"INT\"},\n    {\"name\": \"Doc\", \"dataType\": \"STRING\"},\n    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 384}\n  ]\n)\n\nCrawl project homepages and store their vector embeddings in Epsilla\n\nSuppose we want to find the most relevant open source project based on a query string.\n\nWe will first crawl the homepage of many projects and store their vector embeddings in Epsilla.\n\n# import Trafilatura and embedding model\nfrom trafilatura import fetch_url, extract\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\n\nmodel_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {'device': 'cpu'}\nencode_kwargs = {'normalize_embeddings': False}\n\nhf = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n\n# download the homepages from a few open source projects\nurls = [\n    'https://www.tensorflow.org/',\n    'https://pytorch.org/',\n    'https://react.dev/',\n]\nresults = [extract(fetch_url(url)) for url in urls]\n\n# get the embedding vector and store it in Epsilla\nembeddings = [hf.embed_query(result) for result in results]\nrecords = [\n    {\"ID\": idx, \"Doc\": results[idx], \"Embedding\": embeddings[idx]}\n    for idx in range(len(results))\n]\nclient.insert(\n   table_name=\"Trafilatura\",\n   records=records\n)\n\n\nNow the vector embeddings are stored in Epsilla. In the next section, we will perform a vector search.\n\nPerform vector search\n\nWe have stored the homepages of PyTorch, TensorFlow and React in the database. We can now perform a vector search to find the most relevant project based on a query string.\n\nquery = \"A modern frontend library\"\nquery_embedding = hf.embed_query(query)\nstatus_code, response = client.query(\n    table_name=\"Trafilatura\",\n    query_field=\"Embedding\",\n    query_vector=query_embedding,\n    limit=1\n)\nprint(response)\n\n\nYou will see the returned response is React! That is the correct answer. React is a modern frontend library, but PyTorch and Tensorflow are not.\n\nPrevious\n\nTutorial: Validation of TEI files\n\nNext\n\nTutorial: DWDS-Korpusdaten reproduzieren\n\n On this page\nWhy perform text embedding with crawled data?\nSetup Epsilla\nCrawl project homepages and store their vector embeddings in Epsilla\nPerform vector search\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Tutorial: Validation of TEI files — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorial2.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nTutorials\nTutorial:...\nTutorial: Validation of TEI files\n\nTrafilatura can produce and validate XML documents according to the guidelines of the Text Encoding Initiative (XML-TEI).\n\nProducing TEI files\n\nIn Python:\n\n# load the necessary components\nfrom trafilatura import fetch_url, extract\n\n# download a file\ndownloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n\n# extract information as XML TEI and validate the result\nresult = extract(downloaded, output_format='xmltei', tei_validation=True)\n\n\nFrom the command line:\n\ntrafilatura --xmltei --validate --URL \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\nValidating existing files\n\nThe following code returns True if a document is valid and outputs a message related to the first error impeding validation otherwise:\n\n# load the necessary components\nfrom lxml import etree\nfrom trafilatura.xml import validate_tei\n\n# open a file and parse it\nmytree = etree.parse('document-name.xml')\n\n# validate it\nvalidate_tei(mytree)\n# returns True or an error message\n\n\nFor more information please refer to this blog post: Validating TEI-XML documents with Python\n\nPrevious\n\nTutorial: From a list of links to a frequency list\n\nNext\n\nText embedding\n\n On this page\nProducing TEI files\nValidating existing files\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Tutorial: From a list of links to a frequency list — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorial1.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nTutorials\nTutorial:...\nTutorial: From a list of links to a frequency list\nGet your system up and running\n\nInstallation: see dedicated page\n\nEnsure you have installed the latest version: pip install -U trafilatura\n\nAdditional software for this tutorial: pip install -U SoMaJo\n\nThe following consists of command-line instructions. For an introduction see the page on command-line usage.\n\nProcess a list of links\n\nFor the collection and filtering of links see this tutorial and this blog post.\n\nTwo major options are necessary here:\n\n-i or --input-file to select an input list to read links from\n\n-o or --output-dir to define a directory to eventually store the results\n\nThe input list will be read sequentially, and only lines beginning with a valid URL will be read; any other information contained in the file will be discarded.\n\nThe output directory can be created on demand, but it has to be writable.\n\n$ trafilatura -i list.txt -o txtfiles       # output as raw text\n$ trafilatura --xml -i list.txt -o xmlfiles # output in XML format\n\n\nThe second instruction creates a collection of XML files which can be edited with a basic text editor or a full-fledged text-editing package or IDE such as Atom.\n\nBuild frequency lists\nStep-by-step\nTokenization\n\nThe SoMaJo tokenizer splits text into words and sentences. It works with Python and gets good results when applied to texts in German and English.\n\nAssuming the output directory you are working with is called txtfiles:\n\n# concatenate all files\n$ cat txtfiles/*.txt > txtfiles/all.txt\n# output all tokens\n$ somajo-tokenizer txtfiles/all.txt > tokens.txt\n# sort the tokens by decreasing frequency and output up to 10 most frequent tokens\n$ sort tokens.txt | uniq -c | sort -nrk1 | head -10\n\nFiltering words\n# further filtering: remove punctuation, delete empty lines and lowercase strings\n$ < tokens.txt sed -e \"s/[[:punct:]]//g\" -e \"/^$/d\" -e \"s/.*/\\L\\0/\" > tokens-filtered.txt\n# display most frequent tokens\n$ < tokens-filtered.txt sort | uniq -c | sort -nrk1 | head -20\n# store frequency information in a CSV-file\n$ < tokens.txt sort | uniq -c | sort -nrk1 | sed -e \"s|^ *||g\" -e  \"s| |\\t|\" > txtfiles/frequencies.csv\n\n\nFurther filtering steps:\n\nwith a list of stopwords: egrep -vixFf stopwords.txt\n\nalternative to convert to lower case: uconv -x lower\n\nCollocations and multi-word units\n# word bigrams\n$ < tokens-filtered.txt tr \"\\n\" \" \" | awk '{for (i=1; i<NF; i++) print $i, $(i+1)}' | sort | uniq -c | sort -nrk1 | head -20\n# word trigrams\n$ < tokens-filtered.txt tr \"\\n\" \" \" | awk '{for (i=1; i<NF; i++) print $i, $(i+1), $(i+2)}' | sort | uniq -c | sort -nrk1 | head -20\n\nFurther information\n\nUnix™ for Poets (count and sort words, compute ngram statistics, make a Concordance)\n\nWord analysis and N-grams\n\nN-Grams with NLTK and collocations howto\n\nAnalyzing Documents with Term Frequency - Inverse Document Frequency (tf-idf), both a corpus exploration method and a pre-processing step for many other text-mining measures and models\n\nAdditional information for XML files\n\nAssuming the output directory you are working with is called xmlfiles:\n\n# tokenize a file\n$ somajo-tokenizer --xml xmlfiles/filename.xml\n# remove tags\n$ somajo-tokenizer --xml xmlfiles/filename.xml | sed -e \"s|</*.*>||g\" -e \"/^$/d\"\n# continue with the steps above...\n\n\nPrevious\n\nTutorial: Gathering a custom web corpus\n\nNext\n\nTutorial: Validation of TEI files\n\n On this page\nGet your system up and running\nProcess a list of links\nBuild frequency lists\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Tutorial: Gathering a custom web corpus — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorial0.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nTutorials\nTutorial:...\nTutorial: Gathering a custom web corpus\nGet your system up and running\n\nInstallation: see dedicated page\n\nEnsure that you have installed the latest version: pip install -U trafilatura (or pip3)\n\nNote\n\nThe following consists of command-line instructions.\n\nFor an introduction to and more information on this topic see the documentation page on command-line usage.\n\nContent discovery\nWeb sources\n\nSources used by Trafilatura can consist of previously known or listed web pages. Currently, functions to discover content within a website are available. Other methods include sifting through Wikipedia, social networks, or using lists of links gathered by other projects.\n\nHint\n\nPlease refer to the tutorial page on sources for detailed information.\n\nFinding subpages within a website\n\nIn order to gather web documents it can be useful to download the portions of a website programmatically, mostly to save time and resources. The retrieval and download of documents within a website is often called web crawling or web spidering. Web crawlers usually discover pages from links within the site and from other sites. Trafilatura supports three different ways to gather further links:\n\nSitemaps\n\nWeb feeds (Atom and RSS)\n\nWeb crawling (see the corresponding documentation page)\n\nA comprehensive overview of the available documents can be obtained faster and more efficiently using the first two methods than by systematically extracting and following links within a website.\n\nThe formats supported are all machine-readable rather than human-readable they can also be used to automatically transfer information from one website to another without any human intervention. However, link inspection and filtering prior to systematic download is recommended to avoid undesired content or overstreching computing resources.\n\nIn addition, trafilatura includes support for multilingual and multinational sitemaps. For example, a site can target English language users through links like http://www.example.com/en/… and German language users through http://www.example.com/de/….\n\nSitemaps\n\nA sitemap is a file that lists the visible or whitelisted URLs for a given site, the main goal being to reveal where machines can look for content. Web crawlers usually discover pages from links within the site and from other sites, following a series of rules and protocols. Sitemaps supplement this data to allow crawlers that support Sitemaps to pick up all URLs in the Sitemap and learn about those URLs using the associated metadata.\n\nThe sitemaps protocol primarily allows webmasters to inform search engines about pages on their sites that are available for crawling. Crawlers can use it to pick up all URLs in the sitemap and learn about those URLs using the associated metadata. Sitemaps follow the XML format, so each sitemap is or should be a valid XML file.\n\nSitemaps are particularly useful by large or complex websites since they are made so that machines can more intelligently crawl the site. This particularly true if there is a chance to overlook some of the new or recently updated content, for example because some areas of the website are not available through the browsable interface, or when websites have a huge number of pages that are isolated or not well linked together.\n\nFeeds\n\nA web feed (or news feed) is a data format used for providing users with frequently updated content. This process is also called web syndication, meaning a form of syndication in which content is made available from one website to other sites.\n\nMost commonly, feeds are made available to provide either summaries or full renditions of a website’s recently added content. The term may also describe other kinds of content licensing for reuse. The kinds of content delivered by a web feed are typically HTML (webpage content) or links to webpages and other kinds of digital media. Many news websites, weblogs, schools, and podcasters operate web feeds. The feed icon is commonly used to indicate that a web feed is available.\n\nTrafilatura supports XML-based feeds with the two common formats Atom and RSS.\n\nGathering links\n\nNote\n\nThe following examples use the command-line interface. For more information on the usage with Python please refer to this blog post: Using RSS and Atom feeds to collect web pages with Python.\n\nFeatures\n\nLinks can be gathered straight from the homepage (using heuristics) or using a particular URL if it is already known\n\nThe --list option is useful to list URLs prior to processing\n\nLinks discovery can start from an input file (-i) containing a list of sources which will then be processed in parallel\n\nThe following examples return lists of links. If --list is absent the pages that have been found are directly retrieved, processed, and returned in the chosen output format (default: TXT and standard output).\n\nNote\n\nPlease refer to the CLI documentation on link discovery for detailed information.\n\nIn a nutshell\n\nThe --sitemap option followed by a homepage or a XML sitemap will search for sitemaps links:\n\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list\n\nThe --feed option followed by a homepage or a feed URL will search for feed links:\n\n$ trafilatura --feed \"https://www.dwds.de/\" --list\n\nThe --crawl option will try to discover internal links by hopping from page to page\n\nFor more information on sitemap use and filters for lists of links see this blog post: Using sitemaps to crawl websites.\n\nLink filtering\n\nNote\n\nFor more information see also these blog posts:\n\nFiltering links to gather texts on the web\n\nAn easy way to save time and resources: content-aware URL filtering.\n\nFiltering with coURLan\n\nIt is better to examine a list of URLs for content adequacy, most notably to make download and extraction more efficient by removing unwanted and redundant content. The courlan software package is installed along with trafilatura. It separates the wheat from the chaff by focusing on non-spam text-rich HTML pages, and can be used on the command-line:\n\n$ courlan --inputfile raw-linklist.txt --outputfile filtered-linklist.txt\n\nCustom filtering\n\nURL lists can be filtered manually or with grep, a command-line utility to search text data which operates on line-level and returns either matching or non-matching lines.\n\nMatching relevant links: grep \"/article/\" mylist.txt > filtered-list.txt\n\nExclusion criteria: grep -v \"/video/\" mylist.txt > filtered-list.txt\n\nFor further filters in grep, see grep tutorial.\n\nOther relevant utilities include sort and shuf:\n\n# sort the links and make sure they are unique\nsort -u myfile.txt > myfile-sorted.txt\n# alternatives to shuffle the URLs\nsort -R myfile.txt > myfile-random.txt\nshuf myfile.txt > myfile-random.txt\n\n\nTo draw a random sample of a list of URLs head or tail come in handy after a random sorting:\n\n$ shuf myfile.txt | head -100 > myfile-random-sample.txt\n\n\nTrafilatura automatically sorts the input list to optimize the download order and make sure the input URLs are unique; it is not mandatory to perform these steps by yourself.\n\nProcess a list of links\nSeamless download and processing\n\nTwo major command line arguments are necessary here:\n\n-i or --input-file to select an input list to read links from\n\n-o or --output-dir to define a directory to eventually store the results\n\nAn additional argument can be useful in this context:\n\n--backup-dir in order to keep a copy of downloaded pages\n\nThe input list will be read sequentially, only lines beginning with a valid URL will be read, the file can thus contain other information which will be discarded.\n\nThe output directory can be created on demand, but it must be writable.\n\n# output as raw text\n$ trafilatura -i list.txt -o txtfiles/\n# output in XML format\n$ trafilatura --xml -i list.txt -o xmlfiles/\n# output in XML format, backup of HTML files\n$ trafilatura --xml -i list.txt -o xmlfiles/ --backup-dir htmlfiles/\n\n\nThe second and third instructions create a collection of XML files which can be edited with a basic text editor or a full-fledged text-editing software or IDE such as the Atom editor.\n\nHint\n\nTrafilatura automatically throttles the requests made to a given server, making it the prefered method if you do not want to worry about downloads.\n\nSee documentation page on downloads for more information.\n\nAlternative / existing archives\n\nAlternatively, you can download a series of web documents with generic command-line tools such as wget and (re-)process the downloaded files at a later stage:\n\n# download if necessary\n$ wget --directory-prefix=download/ --wait 5 --input-file=mylist.txt\n# process a directory with archived HTML files\n$ trafilatura --input-dir download/ --output-dir corpus/ --xmltei --no-comments\n\n\nPrevious\n\nTutorials\n\nNext\n\nTutorial: From a list of links to a frequency list\n\n On this page\nGet your system up and running\nContent discovery\nLink filtering\nProcess a list of links\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "A Python package & command-line tool to gather text on the Web — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nA Python package & command-line tool to gather text on the Web\n    \n\n\nDescription\n\nTrafilatura is a Python package and command-line tool designed to gather text on the Web. It includes discovery, extraction and text processing components. Its main applications are web crawling, downloads, scraping, and extraction of main texts, metadata and comments. It aims at staying handy and modular: no database is required, the output can be converted to various commonly used formats.\n\nGoing from raw HTML to essential parts can alleviate many problems related to text quality, first by avoiding the noise caused by recurring elements (headers, footers, links/blogroll etc.) and second by including information such as author and date in order to make sense of the data. The extractor tries to strike a balance between limiting noise (precision) and including all valid parts (recall). It also has to be robust and reasonably fast, it runs in production on millions of documents.\n\nThis tool can be useful for quantitative research in corpus linguistics, natural language processing, computational social science and beyond: it is relevant to anyone interested in data science, information extraction, text mining, and scraping-intensive use cases like search engine optimization, business analytics or information security.\n\nFeatures\nWeb crawling and text discovery:\n\nFocused crawling and politeness rules\n\nSupport for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)\n\nURL management (blacklists, filtering and de-duplication)\n\nSeamless and parallel processing, online and offline:\n\nURLs, HTML files or parsed HTML trees usable as input\n\nEfficient and polite processing of download queues\n\nConversion of previously downloaded files\n\nRobust and efficient extraction:\n\nMain text (with LXML, common patterns and generic algorithms: jusText, fork of readability-lxml)\n\nMetadata (title, author, date, site name, categories and tags)\n\nFormatting and structural elements: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting\n\nComments (if applicable)\n\nOutput formats:\n\nText (minimal formatting or Markdown)\n\nCSV (with metadata, tab-separated values)\n\nJSON (with metadata)\n\nXML (with metadata, text formatting and page structure) and TEI-XML\n\nOptional add-ons:\n\nLanguage detection on extracted content\n\nGraphical user interface (GUI)\n\nSpeed optimizations\n\nEvaluation and alternatives\n\nFor detailed results see the benchmark and evaluation script. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the tests directory.\n\nOther evaluations:\n\nMost efficient open-source library in ScrapingHub’s article extraction benchmark\n\nBest overall tool according to Gaël Lejeune & Adrien Barbaresi, Bien choisir son outil d’extraction de contenu à partir du Web (2020, PDF, French)\n\nIn a nutshell\n\nPrimary installation method is with a Python package manager: pip install trafilatura. See installation documentation.\n\nWith Python:\n\n>>> import trafilatura\n>>> downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n>>> trafilatura.extract(downloaded)\n# outputs main content and comments as plain text ...\n\n\nOn the command-line:\n\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n# outputs main content and comments as plain text ...\n\n\nFor more information please refer to usage documentation and tutorials.\n\nLicense\n\nTrafilatura is distributed under the GNU General Public License v3.0. If you wish to redistribute this library but feel bounded by the license conditions please try interacting at arms length, multi-licensing with compatible licenses, or contacting me.\n\nSee also GPL and free software licensing: What’s in it for business?\n\nContext\n\nThese documentation pages also provide information on concepts behind data collection as well as practical tips on how to gather web texts (see tutorials).\n\nContributing\n\nContributions are welcome! See CONTRIBUTING.md for more information. Bug reports can be filed on the dedicated page.\n\nMany thanks to the contributors who submitted features and bugfixes!\n\nRoadmap\n\nFor planned enhancements and relevant milestones see issues page.\n\nAuthor\n\nThis effort is part of methods to derive information from web documents in order to build text databases for research (chiefly linguistic analysis and natural language processing). Extracting and pre-processing web texts to the exacting standards of scientific research presents a substantial challenge for those who conduct such research. Web corpus construction involves numerous design decisions, and this software package can help facilitate text data collection and enhance corpus quality.\n\nBarbaresi, A. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction, Proceedings of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131.\n\nBarbaresi, A. “Generic Web Content Extraction with Open-Source Software”, Proceedings of KONVENS 2019, Kaleidoscope Abstracts, 2019.\n\nBarbaresi, A. “Efficient construction of metadata-enhanced web corpora”, Proceedings of the 10th Web as Corpus Workshop (WAC-X), 2016.\n\n \n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n\n\nYou can contact me via my contact page or on GitHub.\n\nSoftware ecosystem\n\nTrafilatura: Italian word for wire drawing.\n\nKnown uses of the software.\n\nCorresponding posts on Bits of Language (blog).\n\nFurther documentation\nInstallation\nPython\nTrafilatura package\nAdditional functionality\nGraphical user interface\nUsage\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nTutorials\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nBlog posts\nVideos\nExternal resources\nEvaluation\nAlternatives\nDescription\nResults (2022-05-18)\nExternal evaluations\nOlder results\nCore functions\nExtraction\nLink discovery\nHelpers\nXML processing\nUses & citations\nNotable projects using this software\nCitations in papers\nPublications citing Trafilatura\nPublications citing Htmldate\nPorts\nBackground\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nIndices and tables\n\nIndex\n\nModule Index\n\nSearch Page\n\nNext\n\nInstallation\n\n On this page\nA Python package & command-line tool to gather text on the Web\nDescription\nIn a nutshell\nLicense\nContext\nFurther documentation\nIndices and tables\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Installation of graphical user interface — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/installation-gui.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nInstallation of graphical user interface\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nInstallation\nInstallation...\nInstallation of graphical user interface\n\nThe graphical user interface (GUI) is recommended for users who prefer not to write scripts or type commands in a terminal window.\n\nInstallation\nOpen a terminal window:\n\nOpen the command-line interface\n\nInstall Python:\n\nEnter python3 --version (or if doesn’t work python --version) in the terminal window to see if it’s already installed (the answer should read Python 3.X.X where X is a number)\n\nPython installation\n\nInstall the necessary software versions straight from the repositories by copying the following instructions into the terminal window (use pip3 or pip otherwise):\n\npip3 install -U trafilatura[gui]\n\nAll instructions for the terminal window are followed by pressing the enter key.\n\nHint\n\nAllow some time for the installation to run.\n\nGetting started\n\nJust type trafilatura_gui in a terminal window and press the enter key.\n\nTroubleshooting\n\nInstallation and terminal:\n\nIntroduction to the command-line\n\nHow to Open a Terminal Window in Mac\n\nHow to Open Terminal in Windows\n\nHow to Start Using the Linux Terminal\n\nInstallation instructions for trafilatura\n\nMac OS X:\n\nThis program needs access to the screen... This problem is related to the way you installed Python or the shell you’re running:\n\nClone the reposoitory and start with “pythonw trafilatura_gui/interface.py” (source)\n\nConfigure your virtual environment (Python3 and wxpython 4.1.0)\n\nLinux (Debian/Ubuntu):\n\nsudo apt install libgtk-3-dev\n\noptional: to save compilation time, use a wxpython wheel from https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ (according to Linux distribution, wxpython version 4.1.0)\n\nScreenshot\n\nPrevious\n\nInstallation\n\nNext\n\nUsage\n\n On this page\nInstallation\nGetting started\nTroubleshooting\nScreenshot\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "software-ecosystem.png (1024×768)",
    "url": "https://trafilatura.readthedocs.io/en/latest/_images/software-ecosystem.png",
    "html": ""
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/installation.rst.txt",
    "html": "Installation\n============\n\n\n\nPython\n------\n\nTrafilatura runs using `Python <https://en.wikipedia.org/wiki/Python_%28programming_language%29>`_, currently one of the most frequently used programming languages.\n\nThis software library/package is tested on Linux, macOS and Windows systems. It is compatible with all recent versions of Python:\n\n-  `Installing Python 3 on Mac OS X <https://docs.python-guide.org/starting/install3/osx/>`_ (& `official documentation for Mac <https://docs.python.org/3/using/mac.html>`_)\n-  `Installing Python 3 on Windows <https://docs.python-guide.org/starting/install3/win/>`_ (& `official documentation for Windows <https://docs.python.org/3/using/windows.html>`_)\n-  `Installing Python 3 on Linux <https://docs.python-guide.org/starting/install3/linux/>`_ (& `official documentation for Unix <https://docs.python.org/3/using/unix.html>`_)\n-  Beginners guide: `downloading Python <https://wiki.python.org/moin/BeginnersGuide/Download>`_\n\n\nThen you need a version of Python to interact with as well as the Python packages needed for the task. A recent version of Python 3 is necessary. Some systems already have such an environment installed, to check it just run the following command in a terminal window:\n\n.. code-block:: bash\n\n    $ python3 --version\n    Python 3.8.6 # version 3.6 or higher is fine\n\nIn case Python is not installed, please refer to the excellent `Djangogirls tutorial: Python installation <https://tutorial.djangogirls.org/en/python_installation/>`_.\n\n\n\nTrafilatura package\n-------------------\n\nTrafilatura is packaged as a software library available from the package repository `PyPI <https://pypi.org/>`_. As such it can notably be installed with ``pip`` or ``pipenv``.\n\n\nInstalling Python packages\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Straightforward: `Installing packages in python using pip <https://thepythonguru.com/installing-packages-in-python-using-pip/>`_ (& `official documentation <https://pip.pypa.io/en/stable/>`_)\n   -  `Using pip on Windows <https://projects.raspberrypi.org/en/projects/using-pip-on-windows/2>`_\n-  Advanced: `Pipenv & Virtual Environments <https://docs.python-guide.org/dev/virtualenvs/>`_\n\n\nBasics\n~~~~~~\n\nPlease refer to `this section <usage-cli.html#introduction>`_ for an introduction on command-line usage.\n\n.. code-block:: bash\n\n    $ pip install trafilatura # pip3 where applicable\n\nThis project is under active development, please make sure you keep it up-to-date to benefit from latest improvements:\n\n.. code-block:: bash\n\n    # to make sure you have the latest version\n    $ pip install -U trafilatura\n    # latest available code base\n    $ pip install --force-reinstall -U git+https://github.com/adbar/trafilatura\n\nOn **Mac OS** it can be necessary to install certificates by hand if you get errors like ``[SSL: CERTIFICATE_VERIFY_FAILED]`` while downloading webpages: execute ``pip install certifi`` and perform the post-installation step by clicking on ``/Applications/Python 3.X/Install Certificates.command``. For more information see this `help page on SSL errors <https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error/42334357>`_.\n\n.. hint::\n    Installation on MacOS is generally easier with `brew <https://formulae.brew.sh/formula/trafilatura>`_.\n\n\nOlder Python versions\n~~~~~~~~~~~~~~~~~~~~~\n\n- Last version for Python 3.5: ``pip install trafilatura==0.9.3``\n- Last version for Python 3.4: ``pip install trafilatura==0.8.2``\n\n\nCommand-line tool\n~~~~~~~~~~~~~~~~~\n\n\nIf you installed the library successfully but cannot start the command-line tool, try adding the user-level ``bin`` directory to your ``PATH`` environment variable.\nIf you are using a Unix derivative (e.g. Linux, OS X), you can achieve this by running the following command: ``export PATH=\"$HOME/.local/bin:$PATH\"``.\n\nFor local or user installations where trafilatura cannot be used from the command-line, please refer to `the official Python documentation <https://docs.python.org/3/library/site.html#cmdoption-site-user-base>`_ and this page on `finding executables from the command-line <https://stackoverflow.com/questions/35898734/pip-installs-packages-successfully-but-executables-not-found-from-command-line>`_.\n\n\nAdditional functionality\n------------------------\n\nOptional modules\n~~~~~~~~~~~~~~~~\n\nA few additional libraries can be installed for extended functionality and faster processing: language detection and faster encoding detection: the ``cchardet`` package may not work on all systems but it is highly recommended.\n\n.. code-block:: bash\n\n    $ pip install cchardet  # single package only\n    $ pip install trafilatura[all]  # all additional functionality\n\n\n*For infos on dependency management of Python packages see* `this discussion thread <https://stackoverflow.com/questions/41573587/what-is-the-difference-between-venv-pyvenv-pyenv-virtualenv-virtualenvwrappe>`_.\n\n\n.. hint::\n    Everything works even if not all packages are installed (e.g. because installation fails).\n\n    You can also install or update relevant packages separately, *trafilatura* will detect which ones are present on your system and opt for the best available combination.\n\n\nbrotli\n    Additional compression algorithm for downloads\ncchardet / faust-cchardet (Python >= 3.11)\n    Faster encoding detection, also possibly more accurate (especially for encodings used in Asia)\nhtmldate[all] / htmldate[speed]\n    Faster and more precise date extraction with a series of dedicated packages\npy3langid\n    Language detection on extracted main text\npycurl\n    Faster downloads, possibly less robust though\n\n\n\nGraphical user interface\n------------------------\n\n\n.. toctree::\n   :maxdepth: 2\n\n   installation-gui\n"
  },
  {
    "title": "URL management — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/url-management.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nURL management\nURL management\n\nThis page shows how to filter a list of URLs, with Python and on the command-line, using the functions provided by the courlan package which is included with Trafilatura.\n\nFiltering of input URLs is useful to avoid hodgepodges like .../tags/abc or “internationalized” rubrics like .../en/..... It is best used on URL lists, before retrieving all pages and especially before massive downloads.\n\nHint\n\nSee the Courlan documentation for more examples.\n\nFiltering a list of URLs\nWith Python\n\nThe function check_url() returns a URL and a domain name if everything is fine:\n\n# load the function from the included courlan package\n>>> from courlan import check_url\n\n# checking a URL returns None or a tuple (cleaned url, hostname)\n>>> check_url('https://github.com/adbar/courlan')\n('https://github.com/adbar/courlan', 'github.com')\n\n# noisy query parameters can be removed\n>>> check_url('https://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.org', strict=True)\n('https://httpbin.org/redirect-to', 'httpbin.org')\n\n# optional argument targeting webpages in English or German\n>>> my_url = 'https://www.un.org/en/about-us'\n>>> url, domain_name = check_url(my_url, language='en')\n>>> url, domain_name = check_url(my_url, language='de')\n\n\nOther useful functions include URL cleaning and validation:\n\n# helper function to clean URLs\n>>> from courlan import clean_url\n\n>>> clean_url('HTTPS://WWW.DWDS.DE:80/')\n'https://www.dwds.de'\n\n# URL validation\n>>> from courlan import validate_url\n>>> validate_url('http://1234')\n(False, None)\n>>> validate_url('http://www.example.org/')\n(True, ParseResult(scheme='http', netloc='www.example.org', path='/', params='', query='', fragment=''))\n\nOn the command-line\n\nMost fonctions are also available through a command-line utility:\n\n# display a message listing all options\n$ courlan --help\n\n# simple filtering and normalization\n$ courlan --inputfile url-list.txt --outputfile cleaned-urls.txt\n\n# strict filtering\n$ courlan --language de --strict --inputfile mylist.txt --outputfile mylist-filtered.txt\n\n# strict filtering including language filter\n$ courlan --language de --strict --inputfile mylist.txt --outputfile mylist-filtered.txt\n\nSampling by domain name\n\nThis sampling methods allows for restricting the number of URLs to keep per host, for example:\n\nBefore\n\nwebsite1.com: 1000 URLs; website2.net: 50 URLs\n\nAfter\n\nwebsite1.com: 50 URLs; website2.net: 50 URLs\n\nWith Python\n>>> from courlan import sample_urls\n>>> my_urls = ['…', '…', '…', ]  # etc.\n>>> my_sample = sample_urls(my_urls, 50)\n# optional: exclude_min=None, exclude_max=None, strict=False, verbose=False\n\nOn the command-line\n$ courlan --inputfile urls.txt --outputfile samples-urls.txt --sample --samplesize 50\n\nBlacklisting\n\nYou can provide a blacklist of URLs which will not be processed and included in the output.\n\nin Python: url_blacklist parameter (expects a set)\n\non the CLI: --blacklist arguments (expects a file containing URLs)\n\nIn Python, you can also pass a blacklist of author names as argument, see documentation.\n\nPrevious\n\nTroubleshooting\n\nNext\n\nTutorials\n\n On this page\nFiltering a list of URLs\nSampling by domain name\nBlacklisting\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "",
    "url": "https://trafilatura.readthedocs.io/en/latest/_sources/quickstart.rst.txt",
    "html": "Quickstart\n==========\n\n\nPrimary installation method is with a Python package manager: ``pip install trafilatura``. See `installation documentation <installation.html>`_.\n\n\nWith Python\n-----------\n\nThe only required argument is the input document (here a downloaded HTML file), the rest is optional.\n\n.. code-block:: python\n\n    # import the necessary functions\n    >>> from trafilatura import fetch_url, extract\n\n    # grab a HTML file to extract data from\n    >>> downloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n\n    # output main content and comments as plain text\n    >>> result = extract(downloaded)\n\n    # change the output format to XML (allowing for preservation of document structure)\n    >>> result = extract(downloaded, output_format=\"xml\")\n\n    # discard potential comment and change the output to JSON\n    >>> extract(downloaded, output_format=\"json\", include_comments=False)\n\nThe use of fallback algorithms can also be bypassed in fast mode:\n\n.. code-block:: python\n\n    # faster mode without backup extraction\n    >>> result = extract(downloaded, no_fallback=True)\n\n\nFor a full list of options see `Python usage <usage-python.html>`_.\n\nThe extraction targets the main text part of a webpage. To extract all text content in a ``html2txt`` manner use this function:\n\n.. code-block:: python\n\n    >>> from trafilatura import html2txt\n    >>> html2txt(downloaded)\n\n\nOn the command-line\n-------------------\n\n\nURLs can be used directly (-u/--URL):\n\n.. code-block:: bash\n\n    # outputs main content and comments as plain text\n    $ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n    # displays help message with all possible options\n    $ trafilatura -h\n\nYou can also pipe a HTML document (and response body) to trafilatura:\n\n.. code-block:: bash\n\n    $ cat myfile.html | trafilatura # use the contents of an already existing file\n    $ < myfile.html trafilatura # same here\n\n\nExtraction options are also available on the command-line, they can be combined:\n\n.. code-block:: bash\n\n    $ < myfile.html trafilatura --json --no-tables\n\n\nFor more information please refer to `usage documentation <usage.html>`_ and `tutorials <tutorials.html>`_.\n"
  },
  {
    "title": "Troubleshooting — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/troubleshooting.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nTroubleshooting\nTroubleshooting\nContent extraction\nSomething is missing\n\nThe extractor uses several fallbacks to make sure enough text is returned. Content extraction is a tradeoff between precision and recall, that is between desired and undesirable content. Being ready to accept more unwanted text makes it easier to gather more of the relevant text in the output. Here are ways to tackle the issue:\n\nOpting for favor_recall (Python) or --recall (CLI)\n\nChanging the minimum acceptable length in the settings\n\nUsing the more basic baseline or html2txt functions instead (which is also faster)\n\n(see also reported issues with The New Yorker)\n\nBeyond raw HTML\n\nWhile downloading and processing raw HTML documents is much faster, it can be necessary to fully render the web page before further processing, e.g. because a page makes exhaustive use of JavaScript or because content is injected from multiple sources.\n\nIn such cases the way to go is to use a browser automation library like Playwright. For available alternatives see this list of headless browsers.\n\nBypassing paywalls\n\nA browser automation library can also be useful to bypass issues related to cookies and paywalls as it can be combined with a corresponding browser extension, e.g. iamdamdev’s bypass-paywalls-chrome or this alternative by magnolia1234.\n\nDownloads\nHTTP library\n\nUsing another download utility (see pycurl with Python and wget or curl on the command-line).\n\nInstalling the additional download utility pycurl manually or using pip3 install trafilatura[all] can alleviate the problem: another download library is used, leading to different results.\n\nSeveral alternatives are available on the command-line, e.g. wget -O - \"my_url\" | trafilatura instead of trafilatura -u \"my_url\".\n\nNote\n\nDownloads may fail because your IP or user agent are blocked. Trafilatura’s crawling and download capacities do not bypass such restrictions.\n\nManaging cookies\n\nThe standard library cookiejar can be used along urllib3 in order to use cookies along with HTTP requests, see this documentation pull request.\n\nAlternatively, cookies can be manually specified in a settings.cfg config file, separated by semicolons, e.g. COOKIE = yummy_cookie=choco; tasty_cookie=strawberry.\n\nWeb page no longer available on the Internet\n\nDownload issues can be addressed by retrieving the files somewhere else, i.e. from already existing internet archives like the Internet Archive or the CommonCrawl.\n\nOn the command-line you can use --archived to use the Internet Archive to retrieve pages for which download failed. A corresponding function in Python could look as follows:\n\n# url is the target\n# downloaded is the result of the download\n# also needs a function fetch_url() or equivalent\nif downloaded is None:\n    new_url = \"https://web.archive.org/web/20/\" + url\n    downloaded = fetch_url(new_url)\n\n\nThis approach is generic as it fetches the last available snapshot from the archive.\n\nDownload first and extract later\n\nSince the they have distinct characteristics it can be useful to separate the infrastructure needed for download from the extraction. Using a custom IP or network infrastructure can also prevent your usual IP from getting banned.\n\nPrevious\n\nDefault settings\n\nNext\n\nURL management\n\n On this page\nContent extraction\nSomething is missing\nBeyond raw HTML\nBypassing paywalls\nDownloads\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Default settings — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/settings.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nDefault settings\nDefault settings\n\nThere are two different files which can be edited in order to modify the default download and extraction settings:\n\nsettings.cfg (values designed to be adapted by the user)\n\nsettings.py (package-wide settings, advanced)\n\nConfiguration file\n\nText extraction can be parametrized by providing a custom configuration file which overrides the standard settings. Useful adjustments include download parameters, minimal extraction length, or de-duplication settings.\n\nFile structure\n\nThe default file included in the package is settings.cfg . Important values include:\n\nDownload\n\nDOWNLOAD_TIMEOUT = 30 the time (in seconds) before requests are dropped\n\nSLEEP_TIME = 5 time between requests (higher is better to avoid detection)\n\nUSER_AGENTS and COOKIE are empty by default\n\nInput\n\nMAX_FILE_SIZE = 20000000 maximum acceptable size of input (in bytes)\n\nMIN_FILE_SIZE = 10 minimum acceptable size of input (in bytes)\n\nExtraction\n\nMIN_EXTRACTED_SIZE = 250 acceptable size in characters (used to trigger fallbacks)\n\nMIN_OUTPUT_SIZE = 1 absolute acceptable minimum for main text output\n\nMIN_EXTRACTED_COMM_SIZE and MIN_OUTPUT_COMM_SIZE work the same for comment extraction\n\nEXTRACTION_TIMEOUT = 30 drop extraction after 30 seconds to prevent malicious HTML bombs, set to 0 if you see errors related to the signal module and/or use a module such as defusedxml\n\nDeduplication (not active by default)\n\nMIN_DUPLCHECK_SIZE = 100 minimum size in characters to run deduplication on\n\nMAX_REPETITIONS = 2 maximum number of duplicates allowed\n\nUsing a custom file on the command-line\n\nWith the --config-file option, followed by the file name or path. All the required variables have to be present in the custom file.\n\nAdapting settings in Python\n\nThe standard settings file can be modified, or a custom configuration file can be provided with the config parameter to the bare_extraction() and extract() functions.\n\nIn the following, a single default value is changed, which has an immediate effect on extraction. The resulting text is indeed too short and ends up being discarded. On the contrary, lowering default values can trigger a more opportunistic extraction.\n\n# load necessary functions and data\n>>> from copy import deepcopy\n>>> from trafilatura import extract\n>>> from trafilatura.settings import DEFAULT_CONFIG\n\n# a very short HTML file\n>>> my_html = \"<html><body><p>Text.</p></body></html>\"\n\n# load the configuration and change the minimum output length\n>>> my_config = deepcopy(DEFAULT_CONFIG)\n>>> my_config['DEFAULT']['MIN_OUTPUT_SIZE'] = '1000'\n\n# apply new settings, extraction will fail\n>>> extract(my_html, config=my_config)\n>>>\n# default extraction works\n>>> extract(my_html)\n'Text.'\n\n\nAlternatively, it is possible to override all standard settings by loading a new configuration file where all necessary values have been specified.\n\n# load the required functions\n>>> from trafilatura import extract\n>>> from trafilatura.settings import use_config\n\n# load the new settings by providing a file name\n>>> newconfig = use_config(\"myfile.cfg\")\n\n# use with a previously downloaded document\n>>> extract(downloaded, config=newconfig)\n\n# provide a file name directly (can be slower)\n>>> extract(downloaded, settingsfile=\"myfile.cfg\")\n\n\nNote\n\nUseful adjustments include download parameters, minimal extraction length, or de-duplication settings. User agent settings can also be specified in a custom settings.cfg file.\n\nPackage settings\n\nFor further configuration it is possible to edit package-wide variables contained in the settings.py file provided with Trafilatura.\n\nThese settings notably include:\n\nLists of HTML elements to accept or to discard\n\nConfiguration of parallel processing\n\nFurther download and deduplication settings\n\nFiles written in CLI mode\n\nHere is how to change them:\n\nFind the locally installed version of the package or clone the repository\n\nEdit settings.py\n\nReinstall the package locally: pip install --no-deps -U . in the home directory of the cloned repository\n\nThese remaining variables greatly alter the functioning of the package!\n\nPrevious\n\nWeb crawling\n\nNext\n\nTroubleshooting\n\n On this page\nConfiguration file\nPackage settings\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Web crawling — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/crawls.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nWeb crawling\nWeb crawling\n\nA crawler is a computer program that automatically and systematically visits web pages. Crawling implies to send robots across the Web in order to “read” web pages and collect information about them. A web crawler usually searches the visited pages for the links (i.e. URLs) that they entail and follows them through. It keeps track of and permanently sorts the URLs seen in order to get to new websites to visit. Essentially, a crawler is a sort of a virtual librarian which looks for info and catalogues it.\n\nThe most well-known operators of web crawlers are companies running web search engines. These programs feed search engines all the information they need to create a (giant) database, the search index. Another use of web crawlers is in Web archiving, which involves large sets of webpages to be periodically collected and archived. Other applications include data mining and text analytics, for example building web corpora for linguistic research.\n\nThis page shows how to perform certain web crawling tasks with Python and on the command-line. The trafilatura package allows for easy focused crawling (see definition below).\n\nNew in version 0.9. Still experimental.\n\nDesign decisions\nIntra vs. inter\n\nA necessary distinction has to be made between intra- and inter-domains crawling:\n\nFocused crawling on web-page level: Finding sources within a web page is relatively easy if the page is not too big or too convoluted. For this Trafilatura offers functions to search for links in sitemaps and feeds.\n\nWeb crawling: Hopping between websites can be cumbersome. Discovering more domains without gathering too much junk or running into bugs is difficult without experience with the subject.\n\nFor practical reasons the first solution (“intra”) is best, along with “good” (i.e. customized as needed) seeds/sources. As an alternative, prefix searches on the Common Crawl index can be used.\n\nSee information on finding sources for more details.\n\nConcept and operation\n\nThe focused crawler aims at the discovery of texts within a websites by exploration and retrieval of links. This tool is commonly known as (web) crawler or spider.\n\nA Web crawler starts with a list of URLs to visit, called the seeds. As the crawler visits these URLs, a parsing module extracts specific elements from fetched web pages. The main part of Trafilatura focuses on metadata, text, and comments. The crawler component additionally targets links: it identifies all the hyperlinks present on the pages and adds them to the list of URLs to visit, called the crawl frontier.\n\nInitially, the URL frontier contains the seed set. As web pages are visited they are removed from it. The fetched pages are parsed and further internal links are extracted. A URL filter is used to determine whether the extracted links should be included based on one of several tests. It prioritizes navigation pages (archives, categories, etc.) over the rest in order to gather as many links as possible in few iterations. The resulting links are then added to the frontier.\n\nThe spider module implements politeness rules as defined by the Robots exclusion standard where applicable. Duplicate removal is included, which concerns both URL- and text-level analysis. It can register if a URL has already been visited or if a web page with the same content has already been seen at another URL.\n\nWith Python\nFocused crawler\n\nThe focused_crawler() function integrates all necessary components. It can be adjusted by a series of arguments:\n\n>>> from trafilatura.spider import focused_crawler\n\nhomepage = 'https://www.example.org'\n# starting a crawl\n>>> to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000)\n# resuming a crawl\n>>> to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_urls)\n\n\nHere the crawler stops after seeing a maximum of 10 URLs or registering a total of 100000 URLs on the website, whichever comes first.\n\nThe collected links can then be downloaded and processed. The links to visit (crawl frontier) are stored as a deque (a double-ended queue) which mostly works like a list. The known URLs are stored as a set. Both can also be converted to a list if necessary:\n\nto_visit, known_urls = list(to_visit), sorted(known_urls)\n\n\nYou can also use a custom configuration and pass politeness rules to the crawler. For more information see the documentation of the function.\n\nNavigation\n\nHint\n\nYou may decide on the course of a crawl by determining if there are still navigation pages to visit:\n\nfrom trafilatura.spider import is_still_navigation\n\nis_still_navigation(to_visit)\n# returns True or False\n\n\nFor more info please refer to the core functions page.\n\nOn the command-line\n\nTwo different options are available on the command-line:\n\n--crawl : crawl a fixed number of pages within the website\n\n--explore : combination of sitemap and crawl (uses sitemaps if possible)\n\nOn the CLI the crawler automatically works its way through a website, stopping at a maximum of 30 page visits or exhaustion of the total number of pages on the website, whichever comes first.\n\n$ trafilatura --crawl \"https://www.example.org\" > links.txt\n\n\nIt can also crawl websites in parallel by reading a list of target sites from a list (-i/--input-file option).\n\nNote\n\nThe --list option does not apply here. Unlike with the --sitemap or --feed options, the URLs are simply returned as a list instead of being retrieved and processed. This happens in order to give a chance to examine the collected URLs prior to further downloads.\n\nReferences\n\nBoldi, P., Codenotti, B., Santini, M., & Vigna, S. (2004). Ubicrawler: A scalable fully distributed web crawler. Software: Practice and Experience, 34(8), 711-726.\n\nCho, J., Garcia-Molina, H., & Page, L. (1998). Efficient crawling through URL ordering. Computer networks and ISDN systems, 30(1-7), 161-172.\n\nCho, J. (2001). Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data, PhD dissertation, Dept. of Computer Science, Stanford University.\n\nHirai, J., Raghavan, S., Garcia-Molina, H., & Paepcke, A. (2000). WebBase: A repository of web pages. Computer Networks, 33(1-6), 277-293.\n\nOlston, C., & Najork, M. (2010). Web crawling. Now Publishers Inc.\n\nShkapenyuk, V., & Suel, T. (2002). Design and implementation of a high-performance distributed web crawler. In Proceedings 18th International Conference on Data Engineering (pp. 357-368). IEEE.\n\nPrevious\n\nDownload web pages\n\nNext\n\nDefault settings\n\n On this page\nDesign decisions\nWith Python\nOn the command-line\nReferences\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Download web pages — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/downloads.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nDownload web pages\nDownload web pages\n\nThis documentation page shows how to run simple downloads and how to configure and execute parallel downloads with threads. Both single and concurrent downloads should respect basic “politeness” rules which are described below.\n\nA main objective of data collection over the Internet such as web crawling is to efficiently gather as many useful web pages as possible. In order to retrieve multiples web pages at once it makes sense to retrieve as many domains as possible in parallel. However, particular rules apply then.\n\nNew in version 0.9: Functions exposed and made usable for convenience.\n\nWith Python\nSimple downloads\n\nRunning simple downloads is straightforward with the fetch_url() fonction. This method is also known as single-threaded downloads as they are processed sequentially.\n\nfrom trafilatura.downloads import fetch_url\n\n# single download\ndownloaded = fetch_url('https://www.example.org')\n\n# sequential downloads using a list\nmylist = [\"https://www.example.org\", \"https://httpbin.org\"]\nfor url in mylist:\n    downloaded = fetch_url(url)\n    # do something with it\n\n\nFor efficiency reasons the function makes use of a connection pool where connections are kept open (unless too many websites are retrieved at once). You may see warnings in logs about it which you can safely ignore.\n\nRawResponse object\n\nThe content (stored here in the variable downloaded) is seamlessly decoded to a Unicode string.\n\nThis default setting can be overriden using the decode=False parameter. fetch_url() then returns a urllib3-like response object providing additional information.\n\nThis RawResponse object comprises the attributes data, status, and url which can be accessed as follows:\n\n# RawResponse object instead of Unicode string\n>>> response = fetch_url('https://www.example.org', decode=False)\n>>> response.status\n200\n>>> response.url\n'https://www.example.org'\n>>> response.data\n# raw HTML in binary format\n\nTrafilatura-backed parallel threads\n\nThreads are a way to run several program parts at once, see for instance An Intro to Threading in Python. Multi-threaded downloads are a good option in order to make a more efficient use of the Internet connection. The threads download pages as they go.\n\nHint\n\nThis only makes sense if you are fetching pages from different websites and want the downloads to run in parallel.\n\nThe following variant of multi-threaded downloads with throttling is implemented, it also uses a compressed dictionary to store URLs and possibly save space. Both happen seamlessly, here is how to run it:\n\nfrom trafilatura.downloads import add_to_compressed_dict, buffered_downloads, load_download_buffer\n\n# list of URLs\nmylist = ['https://www.example.org', 'https://www.httpbin.org/html']\n# number of threads to use\nthreads = 4\n\n# converted the input list to an internal format\nurl_store = add_to_compressed_dict(mylist)\n# processing loop\nwhile url_store.done is False:\n    bufferlist, url_store = load_download_buffer(url_store, sleep_time=5)\n    # process downloads\n    for url, result in buffered_downloads(bufferlist, threads):\n        # do something here\n        print(url)\n        print(result)\n\n\nThis safe but efficient option consists in throttling requests based on domains/websites from which content is downloaded. It is highly recommended!\n\nAsynchronous downloads\n\nAsynchronous processing in probably even more efficient in the context of file downloads from a variety of websites. See for instance the AIOHTTP library.\n\nOn the command-line\n\nDownloads on the command-line are automatically run with threads and domain-aware throttling as described above. The following will read URLs from a file, process the results and save them accordingly:\n\n# basic output as raw text with backup directory\n$ trafilatura -i list.txt -o txtfiles/ --backup-dir htmlbackup/\n\n\nHint\n\nTo check for download errors you can use the exit code (0 if all pages could be downloaded, 1 otherwise) and sift through the logs if necessary.\n\nFor more information, see page on command-line use.\n\nEnforcing politeness rules\n\nMachines consume resources on the visited systems and they often visit sites unprompted. That is why issues of schedule, load, and politeness come into play. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent.\n\nWe want to space out requests to any given server and not request the same content multiple times in a row\n\nWe also should avoid parts of a server that are restricted\n\nWe save time for us and the others if we do not request unnecessary information (see content-aware URL selection)\n\nNote\n\nBeware that there should be a tacit scraping etiquette and that a server may block you after the download of a certain number of pages from the same website/domain in a short period of time.\n\nIn addition, some websites may block Trafilatura’s user agent. Thus, the software waits a few seconds between requests per default.\n\nThis additional constraint means we have to not only care for download speed but also manage a register of known websites and apply the rules so as to keep maximizing speed while not being too intrusive. Here is how to keep an eye on it.\n\nRobots exclusion standard\n\nThe robots.txt file is usually available at the root of a website (e.g. www.example.com/robots.txt). It describes what a crawler should or should not crawl according to the Robots exclusion_standard. Certain websites indeed restrict access for machines, for example by the number of web pages or site sections which are open to them.\n\nThe file lists a series of rules which define how bots can interact with the websites. It should be fetched from a website in order to test whether the URL under consideration passes the robot restrictions, and these politeness policies should be respected.\n\nPython features a module addressing the issue in its core packages, the gist of its operation is described below, for more see urllib.robotparser in the official Python documentation.\n\nimport urllib.robotparser\nfrom trafilatura import get_crawl_delay\n\n# define a website to look for rules\nbase_url = 'https://www.example.org'\n\n# load the necessary components, fetch and parse the file\nrules = urllib.robotparser.RobotFileParser()\nrules.set_url(base_url + '/robots.txt')\nrules.read()\n\n# determine if a page can be fetched by all crawlers\nrules.can_fetch(\"*\", \"https://www.example.org/page1234.html\")\n# returns True or False\n\n\nIn addition, some websites may block certain user agents. By replacing the star with one’s user agent (e.g. bot name) we can check if we have been explicitly banned from certain sections or from all the website, which can happen when rules are ignored.\n\nSpacing downloads\n\nThere should an interval in successive requests to avoid burdening the web servers of interest. That way, you will not slow them down and/or risk getting banned. In addition, Trafilatura includes URLs deduplication.\n\nTo prevent the execution of too many requests within too little time, the optional argument sleep_time can be passed to the load_download_buffer() function. It is the time in seconds between two requests for the same domain/website.\n\nfrom trafilatura.downloads import load_download_buffer\n\n# 30 seconds is a safe choice\nmybuffer, threads, domain_dict, backoff_dict = load_download_buffer(url_store, sleep_time=30)\n# then proceed as instructed above...\n\n\nOne of the rules that can be defined by a robots.txt file is the crawl delay (Crawl-Delay), i.e. the time between two download requests for a given website. This delay (in seconds) can be retrieved as follows:\n\n# get the desired information\nseconds = get_crawl_delay(rules)\n# provide a backup value in case no rule exists (happens quite often)\nseconds = get_crawl_delay(rules, default=30)\n\n\nNote\n\nTrafilatura’s focused crawler implements the delay where applicable. For further info and rules see the documentation page on crawling.\n\nStoring rules\n\nYou can also decide to store the rules for convenience and later use, for example in a domain-based dictionary:\n\n# this module comes with trafilatura\nfrom courlan import extract_domain\n\nrules_dict = dict()\n# storing information\ndomain = extract_domain(base_url)\nrules_dict[domain] = rules\n# retrieving rules info\nseconds = get_crawl_delay(rules_dict[domain])\n\n\nYou can then use such rules with the crawling module.\n\nSummary\n\nHere is the simplest way to stay polite while taking all potential constraints into account:\n\nRead robots.txt files, filter your URL list accordingly and care for crawl delay\n\nUse the framework described above and set the throttling variable to a safe value (your main bottleneck is your connection speed anyway)\n\nOptional: for longer crawls, keep track of the throttling info and revisit robots.txt regularly\n\nSee also page on troubleshooting.\n\nPrevious\n\nGraphical user interface\n\nNext\n\nWeb crawling\n\n On this page\nWith Python\nOn the command-line\nEnforcing politeness rules\nSummary\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Graphical user interface — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/usage-gui.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nGraphical...\nGraphical user interface\n\nFor cases where the other usage options do not appear to be convenient, a graphical user interface (GUI) is available. This type of interface allows for interact with trafilatura through graphical icons and menus instead of text-based user interfaces, typed command labels or text navigation.\n\nAlthough it is still experimental, the interface should provide access to all main functions of trafilatura. For more information please refer to the installation instructions.\n\nScreenshot\n\nPrevious\n\nWith R\n\nNext\n\nDownload web pages\n\n On this page\nScreenshot\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "With R — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/usage-r.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nWith R\nWith R\nIntroduction\n\nR is a free software environment for statistical computing and graphics. The reticulate package provides a comprehensive set of tools for seamless interoperability between Python and R. It basically allows for execution of Python code inside an R session, so that Python packages can be used with minimal adaptations, which is ideal for those who would rather operate from R than having to go back and forth between languages and environments.\n\nThe package provides several ways to integrate Python code into R projects:\n\nPython in R Markdown\n\nImporting Python modules\n\nSourcing Python scripts\n\nAn interactive Python console within R.\n\nComplete vignette: Calling Python from R.\n\nThis tutorial shows how to import a Python scraper straight from R and use the results directly with the usual R syntax: Web scraping with R: Text and metadata extraction.\n\nInstallation\n\nThe reticulate package can be easily installed from CRAN as follows:\n\n> install.packages(\"reticulate\")\n\n\nA recent version of Python 3 is necessary. Some systems already have such an environment installed, to check it just run the following command in a terminal window:\n\n$ python3 --version\nPython 3.8.6 # version 3.6 or higher is fine\n\n\nIn case Python is not installed, please refer to the excellent Djangogirls tutorial: Python installation.\n\nTrafilatura has to be installed with pip, conda, or py_install. Skip the installation of Miniconda if it doesn’t seem necessary, you should only be prompted once; or see Installing Python Packages.\n\nHere is a simple example using the py_install() function included in reticulate:\n\n> library(reticulate)\n> py_install(\"trafilatura\")\n\nDownload and extraction\n\nText extraction from HTML documents (including downloads) is available in a straightforward way:\n\n# getting started\n> install.packages(\"reticulate\")\n> library(reticulate)\n> trafilatura <- import(\"trafilatura\")\n\n# get a HTML document as string\n> url <- \"https://example.org/\"\n> downloaded <- trafilatura$fetch_url(url)\n\n# extraction\n> trafilatura$extract(downloaded)\n[1] \"This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\\nMore information...\"\n\n# extraction with arguments\n> trafilatura$extract(downloaded, output_format=\"xml\", url=url)\n[1] \"<doc sitename=\\\"example.org\\\" title=\\\"Example Domain\\\" source=\\\"https://example.org/\\\" hostname=\\\"example.org\\\" categories=\\\"\\\" tags=\\\"\\\" fingerprint=\\\"lxZaiIwoxp80+AXA2PtCBnJJDok=\\\">\\n  <main>\\n    <div>\\n      <head>Example Domain</head>\\n      <p>This domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.</p>\\n      <p>More information...</p>\\n    </div>\\n  </main>\\n  <comments/>\\n</doc>\"\n\n\nFor a full list of arguments see extraction documentation.\n\nAlready stored documents can also be read directly from R, for example with CSV/TSV output and read_delim(), see information on data import in R.\n\nThe html2txt function extracts all possible text on the webpage, it can be used as follows:\n\n> trafilatura$html2txt(downloaded)\n\nOther functions\n\nSpecific parts of the package can also be imported on demand, which provides access to functions not directly exported by the package. For a list of relevant functions and arguments see core functions.\n\n# using the code for link discovery in sitemaps\n> sitemapsfunc <- py_run_string(\"from trafilatura.sitemaps import sitemap_search\")\n> sitemapsfunc$sitemap_search(\"https://www.sitemaps.org/\")\n[1] \"https://www.sitemaps.org\"\n[2] \"https://www.sitemaps.org/protocol.html\"\n[3] \"https://www.sitemaps.org/faq.html\"\n[4] \"https://www.sitemaps.org/terms.html\"\n# and so on...\n\n# import the metadata part of the package as a function\n> metadatafunc <- py_run_string(\"from trafilatura.metadata import extract_metadata\")\n> downloaded <- trafilatura$fetch_url(\"https://github.com/rstudio/reticulate\")\n> metadatafunc$extract_metadata(downloaded)\n$title\n[1] \"rstudio/reticulate\"\n\n$author\n[1] \"Rstudio\"\n\n$url\n[1] \"https://github.com/rstudio/reticulate\"\n\n$hostname\n[1] \"github.com\"\n# and so on...\n\nGoing further\n\nBasic Text Processing in R\n\nQuanteda is an R package for managing and analyzing text:\n\nQuickstart\n\nQuanteda tutorials\n\nAdvancing Text Mining with R and quanteda\n\nPrevious\n\nOn the command-line\n\nNext\n\nGraphical user interface\n\n On this page\nIntroduction\nInstallation\nDownload and extraction\nOther functions\nGoing further\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "With Python — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/usage-python.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nWith Python\nWith Python\nThe Python programming language\n\nPython can be easy to pick up whether you’re a first time programmer or you’re experienced with other languages:\n\nOfficial Python Tutorial\n\nThe Hitchhiker’s Guide to Python\n\nLearn Python Programming Step by Step\n\nThe Best Python Tutorials (freeCodeCamp)\n\nStep-by-step\nQuickstart\n# load necessary components\n>>> from trafilatura import fetch_url, extract\n\n# download a web page\n>>> url = 'https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/'\n>>> downloaded = fetch_url(url)\n>>> downloaded is None  # assuming the download was successful\nFalse\n\n# extract information from HTML\n>>> result = extract(downloaded)\n>>> print(result)\n# newlines preserved, TXT output ...\n\n\nThe only required argument is the input document (here a downloaded HTML file), the rest is optional.\n\nNote\n\nFor a hands-on tutorial see also the Python Notebook Trafilatura Overview.\n\nFormats\n\nDefault output is set to TXT (bare text) without metadata.\n\nThe following formats are available: bare text, text with Markdown formatting, CSV, JSON, XML, and XML following the guidelines of the Text Encoding Initiative (TEI).\n\nHint\n\nCombining TXT, CSV and JSON formats with certain structural elements (e.g. formatting or links) triggers output in TXT+Markdown format.\n\nThe variables from the example above can be used further:\n\n# newlines preserved, TXT output\n>>> extract(downloaded)\n\n# TXT/Markdown output\n>>> extract(downloaded, include_links=True)\n\n# some formatting preserved in basic XML structure\n>>> extract(downloaded, output_format='xml')\n\n# source URL provided for inclusion in metadata\n>>> extract(downloaded, output_format='xml', url=url)\n\n# links preserved in XML, converting relative links to absolute where possible\n>>> extract(downloaded, output_format='xml', include_links=True)\n\n# source URL must be provided to convert relative links to absolute with TXT output\n>>> extract(downloaded, include_links=True, url=url)\n\nChoice of HTML elements\n\nSeveral elements can be included or discarded:\n\nText elements: comments, tables\n\nStructural elements: formatting, images, links\n\nTheir inclusion can be activated or deactivated using paramaters passed to the extract() function:\n\n# no comments in output\n>>> result = extract(downloaded, include_comments=False)\n\n# skip tables examination\n>>> result = extract(downloaded, include_tables=False)\n\n# output with links\n>>> result = extract(downloaded, include_links=True)\n# and so on...\n\n\nNote\n\nIncluding extra elements works best with conversion to XML formats (output_format=\"xml\") or bare_extraction(). Both ways allow for direct display and manipulation of the elements. Certain elements are only visible in the output if the chosen format allows it (e.g. images and XML).\n\ninclude_formatting=True\n\nKeep structural elements related to formatting (<b>/<strong>, <i>/<emph> etc.)\n\ninclude_links=True\n\nKeep link targets (in href=\"...\")\n\ninclude_images=True\n\nKeep track of images along with their targets (<img> attributes: alt, src, title)\n\ninclude_tables=True\n\nExtract text from HTML <table> elements.\n\nOnly include_tables is activated by default.\n\nHint\n\nIf the output is buggy removing a constraint (e.g. formatting) can greatly improve the result.\n\nOptimizing for precision and recall\n\nThe parameters favor_precision & favor_recall can be passed to the extract() & bare_extraction() functions:\n\n>>> result = extract(downloaded, url, favor_precision=True)\n\n\nThey slightly affect processing and volume of textual output, respectively concerning precision/accuracy (i.e. more selective extraction, yielding less and more central elements) and recall (i.e. more opportunistic extraction, taking more elements into account).\n\nhtml2txt\n\nThis function emulates the behavior of similar functions in other packages, it is normally used as a last resort during extraction but can be called specifically in order to output all possible text:\n\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\n\nLanguage identification\n\nThe target language can also be set using 2-letter codes (ISO 639-1), there will be no output if the detected language of the result does not match and no such filtering if the identification component has not been installed (see above installation instructions) or if the target language is not available.\n\n>>> result = extract(downloaded, url, target_language=\"de\")\n\n\nNote\n\nAdditional components are required: pip install trafilatura[all]\n\nOptimizing for speed\n\nExecution speed not only depends on the platform and on supplementary packages (trafilatura[all], htmldate[speed]), but also on the extraction strategy.\n\nThe available fallbacks make extraction more precise but also slower. The use of fallback algorithms can also be bypassed in fast mode, which should make extraction about twice as fast:\n\n# skip algorithms used as fallback\n>>> result = extract(downloaded, no_fallback=True)\n\n\nThe following combination can lead to shorter processing times:\n\n>>> result = extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\n\nContent hashing\n\nFunctions used to build content hashes can be found in hashing.py.\n\n# create a filename-safe string by hashing the given content\n>>> from trafilatura.hashing import generate_hash_filename\n>>> generate_hash_filename(\"This is a text.\")\n'qAgzZnskrcRgeftk'\n\n\nThe SimHash method (also called Charikar’s hash) allows for near-duplicate detection. It implements a locality-sensitive hashing method based on a rolling hash and comparisons using the hamming distance. Overall it is reasonably fast and accurate for web texts and can be used to detect near duplicates by fixing a similarity threshold.\n\n# create a Simhash for near-duplicate detection\n>>> from trafilatura.hashing import Simhash\n>>> first = Simhash(\"This is a text.\")\n>>> second = Simhash(\"This is a test.\")\n>>> second.similarity(first)\n0.84375\n\n# use existing Simhash\n>>> first_copy = Simhash(existing_hash=first.hash)\n>>> first_copy.similarity(first)\n1.0\n\nExtraction settings\n\nHint\n\nSee also settings page.\n\nDisabling signal\n\nA timeout exit during extraction can be turned off if malicious data are not an issue or if you run into an error like signal only works in main thread. In this case, the following code can be useful as it explicitly changes the required setting:\n\n>>> from trafilatura.settings import use_config\n>>> newconfig = use_config()\n>>> newconfig.set(\"DEFAULT\", \"EXTRACTION_TIMEOUT\", \"0\")\n>>> extract(downloaded, config=newconfig)\n\nMetadata extraction\nDate\n\nAmong metadata extraction, dates are handled by an external module: htmldate. By default, focus is on original dates and the extraction replicates the fast/no_fallback option.\n\nCustom parameters can be passed through the extraction function or through the extract_metadata function in trafilatura.metadata, most notably:\n\nextensive_search (boolean), to activate pattern-based opportunistic text search,\n\noriginal_date (boolean) to look for the original publication date,\n\noutputformat (string), to provide a custom datetime format,\n\nmax_date (string), to set the latest acceptable date manually (YYYY-MM-DD format).\n\n# import the extract() function, use a previously downloaded document\n# pass the new parameters as dict\n>>> extract(downloaded, output_format=\"xml\", date_extraction_params={\n        \"extensive_search\": True, \"max_date\": \"2018-07-01\"\n    })\n\nURL\n\nEven if the page to process has already been downloaded it can still be useful to pass the URL as an argument. See this previous bug for an example:\n\n# define a URL and download the example\n>>> url = \"https://web.archive.org/web/20210613232513/https://www.thecanary.co/feature/2021/05/19/another-by-election-headache-is-incoming-for-keir-starmer/\"\n>>> downloaded = fetch_url(url)\n\n# content discarded since necessary metadata couldn't be extracted\n>>> bare_extraction(downloaded, with_metadata=True)\n>>>\n\n# date found in URL, extraction successful\n>>> bare_extraction(downloaded, with_metadata=True, url=url)\n\nMemory use\n\nTrafilatura uses caches to speed up extraction and cleaning processes. This may lead to memory leaks in some cases, particularly in large-scale applications. If that happens you can reset all cached information in order to release RAM:\n\n>>> from trafilatura.meta import reset_caches\n\n# at any given point\n>>> reset_caches()\n\nInput/Output types\nPython objects as output\n\nThe extraction can be customized using a series of parameters, for more see the core functions page.\n\nThe function bare_extraction can be used to bypass output conversion, it returns Python variables for metadata (dictionary) as well as main text and comments (both LXML objects).\n\n>>> from trafilatura import bare_extraction\n>>> bare_extraction(downloaded)\n\nRaw HTTP response objects\n\nThe fetch_url() function can pass a urllib3 response object straight to the extraction by setting the optional decode argument to False.\n\nThis can be useful to get the final redirection URL with response.url and then pass is directly as a URL argument to the extraction function:\n\n# necessary components\n>>> from trafilatura import fetch_url, bare_extraction\n# load an example\n>>> response = fetch_url(\"https://www.example.org\", decode=False)\n# perform extract() or bare_extraction() on Trafilatura's response object\n>>> bare_extraction(response, url=response.url) # here is the redirection URL\n\nLXML objects\n\nThe input can consist of a previously parsed tree (i.e. a lxml.html object), which is then handled seamlessly:\n\n# define document and load it with LXML\n>>> from lxml import html\n>>> my_doc = \"\"\"<html><body><article><p>\n                Here is the main text.\n                </p></article></body></html>\"\"\"\n>>> mytree = html.fromstring(my_doc)\n# extract from the already loaded LXML tree\n>>> extract(mytree)\n'Here is the main text.'\n\nNavigation\nFeeds\n\nThe function find_feed_urls is a all-in-one utility that attemps to discover the feeds from a webpage if required and/or downloads and parses feeds. It returns the extracted links as list, more precisely as a sorted list of unique links.\n\n# import the feeds module\n>>> from trafilatura import feeds\n\n# use the homepage to automatically retrieve feeds\n>>> mylist = feeds.find_feed_urls('https://www.theguardian.com/')\n>>> mylist\n['https://www.theguardian.com/international/rss', '...'] # and so on\n\n# use a predetermined feed URL directly\n>>> mylist = feeds.find_feed_urls('https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml')\n>>> mylist is not []\nTrue # it's not empty\n\n\nNote\n\nThe links are seamlessly filtered for patterns given by the user, e.g. using https://www.un.org/en/ as argument implies taking all URLs corresponding to this category.\n\nAn optional argument target_lang makes it possible to filter links according to their expected target language. A series of heuristics are applied on the link path and parameters to try to discard unwanted URLs, thus saving processing time and download bandwidth.\n\n# the feeds module has to be imported\n# search for feeds in English\n>>> mylist = feeds.find_feed_urls('https://www.un.org/en/rss.xml', target_lang='en')\n>>> mylist is not []\nTrue # links found as expected\n\n# target_lang set to Japanese, the English links are discarded\n>>> mylist = feeds.find_feed_urls('https://www.un.org/en/rss.xml', target_lang='ja')\n>>> mylist\n[]\n\n\nFor more information about feeds and web crawling see:\n\nThis blog post: Using RSS and Atom feeds to collect web pages with Python\n\nThis Youtube tutorial: Extracting links from ATOM and RSS feeds\n\nSitemaps\n\nYoutube tutorial: Learn how to process XML sitemaps to extract all texts present on a website\n\n# load sitemaps module\n>>> from trafilatura import sitemaps\n\n# automatically find sitemaps by providing the homepage\n>>> mylinks = sitemaps.sitemap_search('https://www.theguardian.com/')\n\n# the target_lang argument works as explained above\n>>> mylinks = sitemaps.sitemap_search('https://www.un.org/', target_lang='en')\n\n\nThe links are also seamlessly filtered for patterns given by the user, e.g. using https://www.theguardian.com/society as argument implies taking all URLs corresponding to the society category.\n\nPrevious\n\nQuickstart\n\nNext\n\nOn the command-line\n\n On this page\nThe Python programming language\nStep-by-step\nExtraction settings\nInput/Output types\nNavigation\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "On the command-line — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/usage-cli.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nOn the command-line\nOn the command-line\nIntroduction\n\nTrafilatura includes a command-line interface and can be conveniently used without writing code.\n\nFor the very first steps please refer to this multilingual, step-by-step Introduction to the command-line interface and this section of the Introduction to Cultural Analytics & Python.\n\nFor instructions related to specific platforms see:\n\nComment Prompt (tutorial for Windows systems)\n\nIntroduction to the Windows Command Line with PowerShell\n\nHow to use the Terminal command line in macOS\n\nor An introduction to the Linux Terminal\n\nAs well as these compendia:\n\nIntroduction to the Bash Command Line (The Programming Historian)\n\nBasic Bash Command Line Tips You Should Know (freeCodeCamp)\n\nQuickstart\n\nURLs can be used directly (-u/--URL):\n\n# outputs main content and comments as plain text ...\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n# outputs main text with basic XML structure ...\n$ trafilatura --xml --URL \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n# displays help message\n$ trafilatura -h\n\n\nYou can also pipe a HTML document (and response body) to trafilatura:\n\n# use the contents of an already existing file\n$ cat myfile.html | trafilatura\n\n# alternative syntax\n$ < myfile.html trafilatura\n\n# use a custom download utility and pipe it to trafilatura\n$ wget -qO- \"https://de.creativecommons.org/index.php/was-ist-cc/\" | trafilatura\n\nExtraction parameters\nChoice of HTML elements\n\nSeveral elements can be included or discarded (see list of options below):\n\nText elements: comments, tables\n\nStructural elements: formatting, images, links\n\nOnly comments and text extracted from HTML <table> elements are extracted by default, --no-comments and --no-tables deactivate this setting.\n\nFurther options:\n\n--formatting\n\nKeep structural elements related to formatting (<b>/<strong>, <i>/<emph> etc.)\n\n--links\n\nKeep link targets (in href=\"...\"), converting relative URLs to absolute where possible\n\n--images\n\nKeep track of images along with their targets (<img> attributes: alt, src, title)\n\nNote\n\nCertain elements are only visible in the output if the chosen format allows it (e.g. images and XML).\n\nIncluding extra elements works best with conversion to XML/XML-TEI. If the output is buggy removing a constraint (e.g. formatting) can greatly improve the result.\n\nOutput format\n\nOutput as TXT without metadata is the default, another format can be selected in two different ways:\n\n--csv, --json, --xml or --xmltei\n\n-out or --output-format {txt,csv,json,xml,xmltei}\n\nHint\n\nCombining TXT, CSV and JSON formats with certain structural elements (e.g. formatting or links) triggers output in TXT+Markdown format.\n\nOptimizing for precision and recall\n\nThe arguments --precision & --recall can be passed to the extractor.\n\nThey slightly affect processing and volume of textual output, respectively concerning precision/accuracy (i.e. more selective extraction, yielding less and more central elements) and recall (i.e. more opportunistic extraction, taking more elements into account).\n\nLanguage identification\n\nPassing the argument --target-language along with a 2-letter code (ISO 639-1) will trigger language filtering of the output if the identification component has been installed and if the target language is available.\n\nNote\n\nAdditional components are required: pip install trafilatura[all]\n\nChanging default settings\n\nSee documentation page on settings.\n\nProcess files locally\n\nIn case web pages have already been downloaded and stored, it is possible to process single files or directories as a whole. It can be especially helpful to separate download and extraction to circumvent blocking mechanisms, either by scrambling IPs used to access the pages or by using web browser automation software to bypass issues related to cookies and paywalls.\n\nTrafilatura will work as well provided web pages (HTML documents) are used as input. Two major command line arguments are necessary:\n\n--input-dir to select a directory to read files from\n\n-o or --output-dir to define a directory to eventually store the results\n\nNote\n\nIn case no directory is selected, results are printed to standard output (STDOUT, e.g. in the terminal window).\n\nProcess a list of links\n\nNote\n\nBeware that there should be a tacit scraping etiquette and that a server may block you after the download of a certain number of pages from the same website/domain in a short period of time.\n\nIn addition, some websites may block the requests user-agent. Thus, trafilatura waits a few seconds per default between requests.\n\nFor more information see the page on downloads.\n\nTwo major command line arguments are necessary here:\n\n-i or --input-file to select an input list to read links from.\n\nThis option allows for bulk download and processing of a list of URLs from a file listing one link per line. The input list will be read sequentially, only lines beginning with a valid URL will be read, the file can thus contain other information which will be discarded.\n\n-o or --output-dir to define a directory to eventually store the results.\n\nThe output directory can be created on demand, but it must be writable.\n\n$ trafilatura -i list.txt -o txtfiles/              # output as raw text\n$ trafilatura --xml -i list.txt -o xmlfiles/        # output in XML format\n\n\nHint\n\nBackup of HTML sources can be useful for archival and further processing:\n\n$ trafilatura --input-file links.txt --output-dir converted/ --backup-dir html-sources/ --xml\n\nInternet Archive\n\nUsing the option --archived will trigger queries to the Internet Archive for web pages which could not be downloaded.\n\nThere is a fair chance to find archived versions for larger websites, whereas pages of lesser-known websites may not have been preserved there. The retrieval process is slow as it depends on a single web portal only, it is best performed for a relatively small number of URLs.\n\nLink discovery\n\nLink discovery can be performed over web feeds (Atom and RSS) or sitemaps.\n\nBoth homepages and particular sitemaps or feed URLs can be used as input.\n\nThe --list option is useful to list URLs prior to processing. This option can be combined with an input file (-i) containing a list of sources which will then be processed in parallel.\n\nFor more information please refer to the tutorial on content discovery.\n\nFeeds\n# automatically detecting feeds starting from the homepage\n$ trafilatura --feed \"https://www.dwds.de/\" --list\n\n# already known feed\n$ trafilatura --feed \"https://www.dwds.de/api/feed/themenglossar/Corona\" --list\n\n# processing a list in parallel\n$ trafilatura -i mylist.txt --feed --list\n\n\nYoutube tutorial: Extracting links from web feeds\n\nSitemaps\n# run link discovery through a sitemap for sitemaps.org and store the resulting links in a file\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list > mylinks.txt\n\n# using an already known sitemap URL\n$ trafilatura --sitemap \"https://www.sitemaps.org/sitemap.xml\" --list\n\n# targeting webpages in German\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --target-language \"de\"\n\n\nFor more information on sitemap use and filters for lists of links see this blog post: Using sitemaps to crawl websites.\n\nYoutube tutorial: Listing all website contents with sitemaps\n\nURL inspection prior to download and processing\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"https://www.sitemaps.org/de\"\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"protocol\"\n\n\nUsing a subpart of the site also acts like a filter, for example --sitemap \"https://www.sitemaps.org/de/\".\n\nFor more information on sitemap use and filters for lists of links see this blog post: Using sitemaps to crawl websites and this tutorial on link filtering.\n\nFurther information\n\nHint\n\nSee also how to modify the default settings.\n\nFor all usage instructions see trafilatura -h:\n\ntrafilatura [-h] [-i INPUTFILE | --input-dir INPUTDIR | -u URL]\n               [--parallel PARALLEL] [-b BLACKLIST] [--list]\n               [-o OUTPUTDIR] [--backup-dir BACKUP_DIR] [--keep-dirs]\n               [--hash-as-name] [--feed [FEED] | --sitemap [SITEMAP] |\n               --crawl [CRAWL] | --explore [EXPLORE]] [--archived]\n               [--url-filter URL_FILTER [URL_FILTER ...]] [-f]\n               [--formatting] [--links] [--images] [--no-comments]\n               [--no-tables] [--only-with-metadata]\n               [--target-language TARGET_LANGUAGE] [--deduplicate]\n               [--config-file CONFIG_FILE]\n               [-out {txt,csv,json,xml,xmltei} | --csv | --json | --xml | --xmltei]\n               [--validate-tei] [-v] [--version]\n\n\nCommand-line interface for Trafilatura\n\noptional arguments:\n-h, --help\n\nshow this help message and exit\n\n-v, --verbose\n\nincrease logging verbosity (-v or -vv)\n\n--version\n\nshow version information and exit\n\nInput:\n\nURLs, files or directories to process\n\n-i INPUTFILE, --input-file INPUTFILE\n\nname of input file for batch processing\n\n--input-dir INPUTDIR\n\nread files from a specified directory (relative path)\n\n-u URL, --URL URL\n\ncustom URL download\n\n--parallel PARALLEL\n\nspecify a number of cores/threads for downloads and/or processing\n\n-b BLACKLIST, --blacklist BLACKLIST\n\nfile containing unwanted URLs to discard during processing\n\nOutput:\n\nDetermines if and how files will be written\n\n--list\n\ndisplay a list of URLs without downloading them\n\n-o OUTPUTDIR, --output-dir OUTPUTDIR\n\nwrite results in a specified directory (relative path)\n\n--backup-dir BACKUP_DIR\n\npreserve a copy of downloaded files in a backup directory\n\n--keep-dirs\n\nkeep input directory structure and file names\n\n--hash-as-name\n\nuse hash value as output file name instead of random default\n\nNavigation:\n\nLink discovery and web crawling\n\n--feed URL\n\nlook for feeds and/or pass a feed URL as input\n\n--sitemap URL\n\nlook for sitemaps for the given website and/or enter a sitemap URL\n\n--crawl URL\n\ncrawl a fixed number of pages within a website starting from the given URL\n\n--explore URL\n\nexplore the given websites (combination of sitemap and crawl)\n\n--archived\n\ntry to fetch URLs from the Internet Archive if downloads fail\n\n--url-filter URL_FILTER\n\nonly process/output URLs containing these patterns (space-separated strings)\n\nExtraction:\n\nCustomization of text and metadata processing\n\n-f, --fast\n\nfast (without fallback detection)\n\n--formatting\n\ninclude text formatting (bold, italic, etc.)\n\n--links\n\ninclude links along with their targets (experimental)\n\n--images\n\ninclude image sources in output (experimental)\n\n--no-comments\n\ndon’t output any comments\n\n--no-tables\n\ndon’t output any table elements\n\n--only-with-metadata\n\nonly output those documents with title, URL and date (for formats supporting metadata)\n\n--target-language TARGET_LANGUAGE\n\nselect a target language (ISO 639-1 codes)\n\n--deduplicate\n\nfilter out duplicate documents and sections\n\n--config-file CONFIG_FILE\n\noverride standard extraction parameters with a custom config file\n\n--precision\n\nfavor extraction precision (less noise, possibly less text)\n\n--recall\n\nfavor extraction recall (more text, possibly more noise)\n\nFormat:\n\nSelection of the output format\n\n-out, --output-format\n\ndetermine output format, possible choices: txt, csv, json, xml, xmltei\n\n--csv\n\nCSV output\n\n--json\n\nJSON output\n\n--xml\n\nXML output\n\n--xmltei\n\nXML TEI output\n\n--validate-tei\n\nvalidate XML TEI output\n\nPrevious\n\nWith Python\n\nNext\n\nWith R\n\n On this page\nIntroduction\nQuickstart\nExtraction parameters\nProcess files locally\nProcess a list of links\nLink discovery\nFurther information\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Uses & citations — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/used-by.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nUses & citations\nUses & citations\n\nTrafilatura is used at several institutions, included in other software packages and cited in research publications, especially in linguistics and natural language processing, social science, information science, and in the context of large language models. This page lists projects and publications mentioning the library.\n\nTo add further references, please edit this page and suggest changes.\n\nNotable projects using this software\nKnown institutional users\n\nData against Feminicide\n\nKagi search engine (notably Teclis component)\n\nMedia Cloud platform for media analysis\n\nThe Internet Archive’s sandcrawler which crawls and processes the scholarly web for the Fatcat catalog of research publications\n\nSciencesPo médialab through its Minet webmining package\n\nVarious software repositories\n\nBenson, to turn a list of URLs into mp3s of the contents of each web page\n\nCommonCrawl downloader, to derive massive amounts of language data\n\nGLAM Workbench for cultural heritage (web archives section)\n\nllama-hub, a library of data loaders for large language models\n\nObsei, a text collection and analysis tool\n\nVulristics, a framework for analyzing publicly available information about vulnerabilities\n\nWebsite-to-Chatbot, a personalized chatbot\n\nFor more see this list of software using Trafilatura.\n\nCitations in papers\nTrafilatura as a whole\n\nTo reference this software in a publication please cite the following paper:\n\nBarbaresi, A. “Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction”, in Proceedings of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131. DOI: 10.18653/v1/2021.acl-demo.15\n\n \n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n\nDate extraction (htmldate)\n\nThe date extraction component htmldate is referenced in the following publication:\n\nBarbaresi, A. “htmldate: A Python package to extract publication dates from web pages”, Journal of Open Source Software, 5(51), 2439, 2020. DOI: 10.21105/joss.02439\n\n \n@article{barbaresi-2020-htmldate,\n  title = {{htmldate: A Python package to extract publication dates from web pages}},\n  author = \"Barbaresi, Adrien\",\n  journal = \"Journal of Open Source Software\",\n  volume = 5,\n  number = 51,\n  pages = 2439,\n  url = {https://doi.org/10.21105/joss.02439},\n  publisher = {The Open Journal},\n  year = 2020,\n}\n\nPublications citing Trafilatura\n\nAlakukku, L. (2022). “Domain specific boilerplate removal from web pages with entropy and clustering”, Master’s thesis, University of Aalto.\n\nAlhamzeh, A., Bouhaouel, M., Egyed-Zsigmond, E., & Mitrović, J. (2021). “DistilBERT-based Argumentation Retrieval for Answering Comparative Questions”, Proceedings of CLEF 2021 – Conference and Labs of the Evaluation Forum.\n\nBender, M., Bubenhofer, N., Dreesen, P., Georgi, C., Rüdiger, J. O., & Vogel, F. (2022). Techniken und Praktiken der Verdatung. Diskurse–digital, 135-158.\n\nBevendorff, J., Gupta, S., Kiesel, J., & Stein, B. (2023). An Empirical Comparison of Web Content Extraction Algorithms.\n\nBozarth, L., & Budak, C. (2021). “An Analysis of the Partnership between Retailers and Low-credibility News Publishers”, Journal of Quantitative Description: Digital Media, 1.\n\nBrandon, C., Doherty, A. J., Kelly, D., Leddin, D., & Margaria, T. (2023). HIPPP: Health Information Portal for Patients and Public. Applied Sciences, 13(16), 9453.\n\nBraun, D. (2021). “Automated Semantic Analysis, Legal Assessment, and Summarization of Standard Form Contracts”, PhD Thesis, Technische Universität München.\n\nChen, X., Zeynali, A., Camargo, C., Flöck, F., Gaffney, D., Grabowicz, P., … & Samory, M. (2022). SemEval-2022 Task 8: Multilingual news article similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) (pp. 1094-1106).\n\nDi Giovanni, M., Tasca, T., & Brambilla, M. (2022). DataScience-Polimi at SemEval-2022 Task 8: Stacking Language Models to Predict News Article Similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) (pp. 1229-1234).\n\nDumitru, V., Iorga, D., Ruseti, S., & Dascalu, M. (2023). Garbage in, garbage out: An analysis of HTML text extractors and their impact on NLP performance. In 2023 24th International Conference on Control Systems and Computer Science (CSCS) (pp. 403-410). IEEE.\n\nFröbe, M., Hagen, M., Bevendorff, J., Völske, M., Stein, B., Schröder, C., … & Potthast, M. (2021). “The Impact of Main Content Extraction on Near-Duplicate Detection”. arXiv preprint arXiv:2111.10864.\n\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., … & Leahy, C. (2020). “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”, arXiv preprint arXiv:2101.00027.\n\nGopalakrishnan, S., Chen, V. Z., Dou, W., Hahn-Powell, G., Nedunuri, S., & Zadrozny, W. W. (2023). Text to Causal Knowledge Graph: A Framework to Synthesize Knowledge from Unstructured Business Texts into Causal Graphs.\n\nHarrando, I., & Troncy, R. (2021). “Explainable Zero-Shot Topic Extraction Using a Common-Sense Knowledge Graph”, In 3rd Conference on Language, Data and Knowledge (LDK 2021). OpenAccess Series in Informatics, Dagstuhl Publishing.\n\nHartmann, S. (2023). Open Corpus Linguistics–or How to overcome common problems in dealing with corpus data by adopting open research practices.\n\nHunter, B., Mathews, F., & Weeds, J. (2023). Using hierarchical text classification to investigate the utility of machine learning in automating online analyses of wildlife exploitation. Ecological Informatics, 102076.\n\nIndig, B., Sárközi-Lindner, Z., & Nagy, M. (2022). Use the Metadata, Luke!–An Experimental Joint Metadata Search and N-gram Trend Viewer for Personal Web Archives. In Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities (pp. 47-52).\n\nJohannsen, B. (2023). Fußball und safety: Eine framesemantische Perspektive auf Diskurse über trans Sportler* innen. Queere Vielfalt im Fußball, 176.\n\nJung, G., Han, S., Kim, H., Kim, K., & Cha, J. (2022). Extracting the Main Content of Web Pages Using the First Impression Area. IEEE Access.\n\nKarabulut, M., & Mayda, İ. (2020). “Development of Browser Extension for HTML Web Page Content Extraction”, In 2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA) (pp. 1-6). IEEE.\n\nKhusainov, A., Suleymanov, D., Gilmullin, R., Minsafina, A., Kubedinova, L., & Abdurakhmonova, N. “First Results of the “TurkLang-7” Project: Creating Russian-Turkic Parallel Corpora and MT Systems”, In CMCL (pp. 90-101).\n\nKüehn, P., Relke, D. N., & Reuter, C. (2023). Common Vulnerability Scoring System Prediction based on Open Source Intelligence Information Sources. Computers & Security, 103286.\n\nKuehn, P., Schmidt, M., & Reuter, C. (2023). ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. arXiv preprint arXiv:2304.11960.\n\nLaippala, V., Rönnqvist, S., Hellström, S., Luotolahti, J., Repo, L., Salmela, A., … & Pyysalo, S. (2020). “From Web Crawl to Clean Register-Annotated Corpora”, Proceedings of the 12th Web as Corpus Workshop (pp. 14-22).\n\nLaippala, V., Salmela, A., Rönnqvist, S., Aji, A. F., Chang, L. H., Dhifallah, A., … & Pyysalo, S. (2022). Towards better structured and less noisy Web data: Oscar with Register annotations. In Proceedings of the Eighth Workshop on Noisy User-generated Text (W-NUT 2022) (pp. 215-221).\n\nMadrid-Morales, D. (2021). “Who Set the Narrative? Assessing the Influence of Chinese Media in News Coverage of COVID-19 in 30 African Countries”, Global Media and China, 6(2), 129-151.\n\nMeier-Vieracker, S. (2022). “Fußballwortschatz digital–Korpuslinguistische Ressourcen für den Sprachunterricht.” Korpora Deutsch als Fremdsprache (KorDaF), 2022/01 (pre-print).\n\nMeng, K. (2021). “An End-to-End Computational System for Monitoring and Verifying Factual Claims” (pre-print).\n\nMiquelina, N., Quaresma, P., & Nogueira, V. B. (2022). Generating a European Portuguese BERT Based Model Using Content from Arquivo. pt Archive. In International Conference on Intelligent Data Engineering and Automated Learning (pp. 280-288). Springer, Cham.\n\nNissopoulou, T. X. (2023). Web content classification analysis, MSc thesis, International Hellenic University.\n\nNolda, A., Barbaresi, A., & Geyken, A. (2023). Korpora für die lexikographische Beschreibung diatopischer Variation in der deutschen Standardsprache. Korpora in der germanistischen Sprachwissenschaft: Mündlich, schriftlich, multimedial, 29.\n\nÖhman, J., Verlinden, S., Ekgren, A., Gyllensten, A. C., Isbister, T., Gogoulou, E., … & Sahlgren, M. (2023). The Nordic Pile: A 1.2 TB Nordic Dataset for Language Modeling. arXiv preprint arXiv:2303.17183.\n\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Pannier, B., … & Launay, J. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.\n\nPiskorski, J., Stefanovitch, N., Da San Martino, G., & Nakov, P. (2023). Semeval-2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. In Proceedings of the the 17th International Workshop on Semantic Evaluation (SemEval-2023) (pp. 2343-2361).\n\nRobertson, F., Lagus, J., & Kajava, K. (2021). “A COVID-19 news coverage mood map of Europe”, Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation (pp. 110-115).\n\nSalmela, A. (2022). “Distinguishing Noise and Main Text Content from Web-Sourced Plain Text Documents Using Sequential Neural Networks”, Master’s thesis, University of Turku.\n\nSawczyn, A., Binkowski, J., Janiak, D., Augustyniak, Ł., & Kajdanowicz, T. (2021). “Fact-checking: relevance assessment of references in the Polish political domain”, Procedia Computer Science, 192, 1285-1293.\n\nSchamel, T., Braun, D., & Matthes, F. (2022). Structured Extraction of Terms and Conditions from German and English Online Shops. In Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5) (pp. 181-190).\n\nSutter, T., Bozkir, A. S., Gehring, B., & Berlich, P. (2022). Avoiding the Hook: Influential Factors of Phishing Awareness Training on Click-Rates and a Data-Driven Approach to Predict Email Difficulty Perception. IEEE Access, 10, 100540-100565.\n\nTer-Akopyan, B. (2022). “Identification of Political Leaning in German News”, Master’s thesis, Ludwig Maximilian University of Munich.\n\nVarlamov, M., Galanin, D., Bedrin, P., Duda, S., Lazarev, V., & Yatskov, A. (2022). A Dataset for Information Extraction from News Web Pages. In 2022 Ivannikov Ispras Open Conference (ISPRAS) (pp. 100-106). IEEE.\n\nWaheed, A., Qunaibi, S., Barradas, D., & Weinberg, Z. (2022). Darwin’s Theory of Censorship: Analysing the Evolution of Censored Topics with Dynamic Topic Models. In Proceedings of the 21st Workshop on Privacy in the Electronic Society (pp. 103-108).\n\nZinn, J. O., & Müller, M. (2021). “Understanding discourse and language of risk”, Journal of Risk Research, 1-14.\n\nPublications citing Htmldate\n\nSee citation page of htmldate’s documentation.\n\nPorts\nGo port\n\ngo-trafilatura\n\nPrevious\n\nCore functions\n\nNext\n\nBackground\n\n On this page\nNotable projects using this software\nCitations in papers\nPublications citing Trafilatura\nPublications citing Htmldate\nPorts\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Background — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/background.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nBackground\nBackground\n\nThe pages below provide background information on scientific approaches to web data collection and processing, corpus linguistics, digital humanities, and natural language processing.\n\nCompendium: Web texts in linguistics and humanities\nWeb corpora as scientific objects\nCorpus types and resulting methods\nCorpus construction steps\nMethodological issues\nReferences\nFinding sources for web corpora\nFrom link lists to web corpora\nExisting resources\nSearch engines\nSelecting random documents from the Web\nSocial networks\nRemarks\nReferences\nWorking with corpus data\nGeneric solutions in Python\nFormats and software used in corpus linguistics\nGeneric NLP solutions\n\nPrevious\n\nUses & citations\n\nNext\n\nCompendium: Web texts in linguistics and humanities\n\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Core functions — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/corefunctions.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nCore functions\nCore functions\n\nTable of contents\n\nExtraction\n\nextract()\n\nbare_extraction()\n\nbaseline()\n\nhtml2txt()\n\ntry_readability()\n\ntry_justext()\n\nextract_metadata()\n\nextract_comments()\n\nLink discovery\n\nsitemap_search()\n\nfind_feed_urls()\n\nfocused_crawler()\n\nHelpers\n\nfetch_url()\n\ndecode_response()\n\nload_html()\n\nsanitize()\n\ntrim()\n\nXML processing\n\nxmltotxt()\n\nvalidate_tei()\n\nExtraction\nextract()\ntrafilatura.extract(filecontent, url=None, record_id=None, no_fallback=False, favor_precision=False, favor_recall=False, include_comments=True, output_format='txt', tei_validation=False, target_language=None, include_tables=True, include_images=False, include_formatting=False, include_links=False, deduplicate=False, date_extraction_params=None, only_with_metadata=False, with_metadata=False, max_tree_size=None, url_blacklist=None, author_blacklist=None, settingsfile=None, config=<configparser.ConfigParser object>, **kwargs)\n[source]\nMain function exposed by the package:\n\nWrapper for text extraction and conversion to chosen output format.\n\nParameters\n:\n\nfilecontent – HTML code as string.\n\nurl – URL of the webpage.\n\nrecord_id – Add an ID to the metadata.\n\nno_fallback – Skip the backup extraction with readability-lxml and justext.\n\nfavor_precision – prefer less text but correct extraction.\n\nfavor_recall – when unsure, prefer more text.\n\ninclude_comments – Extract comments along with the main text.\n\noutput_format – Define an output format: ‘txt’, ‘csv’, ‘json’, ‘xml’, or ‘xmltei’.\n\ntei_validation – Validate the XML-TEI output with respect to the TEI standard.\n\ntarget_language – Define a language to discard invalid documents (ISO 639-1 format).\n\ninclude_tables – Take into account information within the HTML <table> element.\n\ninclude_images – Take images into account (experimental).\n\ninclude_formatting – Keep structural elements related to formatting (only valuable if output_format is set to XML).\n\ninclude_links – Keep links along with their targets (experimental).\n\ndeduplicate – Remove duplicate segments and documents.\n\ndate_extraction_params – Provide extraction parameters to htmldate as dict().\n\nonly_with_metadata – Only keep documents featuring all essential metadata (date, title, url).\n\nmax_tree_size – Discard documents with too many elements.\n\nurl_blacklist – Provide a blacklist of URLs as set() to filter out documents.\n\nauthor_blacklist – Provide a blacklist of Author Names as set() to filter out authors.\n\nsettingsfile – Use a configuration file to override the standard settings.\n\nconfig – Directly provide a configparser configuration.\n\nReturns\n:\n\nA string in the desired format or None.\n\nbare_extraction()\ntrafilatura.bare_extraction(filecontent, url=None, no_fallback=False, favor_precision=False, favor_recall=False, include_comments=True, output_format='python', target_language=None, include_tables=True, include_images=False, include_formatting=False, include_links=False, deduplicate=False, date_extraction_params=None, only_with_metadata=False, with_metadata=False, max_tree_size=None, url_blacklist=None, author_blacklist=None, as_dict=True, config=<configparser.ConfigParser object>)\n[source]\n\nInternal function for text extraction returning bare Python variables.\n\nParameters\n:\n\nfilecontent – HTML code as string.\n\nurl – URL of the webpage.\n\nno_fallback – Use faster heuristics and skip backup extraction.\n\nfavor_precision – prefer less text but correct extraction.\n\nfavor_recall – prefer more text even when unsure.\n\ninclude_comments – Extract comments along with the main text.\n\noutput_format – Define an output format, Python being the default and the interest of this internal function. Other values: “txt”, “csv”, “json”, “xml”, or “xmltei”.\n\ntarget_language – Define a language to discard invalid documents (ISO 639-1 format).\n\ninclude_tables – Take into account information within the HTML <table> element.\n\ninclude_images – Take images into account (experimental).\n\ninclude_formatting – Keep structural elements related to formatting (present in XML format, converted to markdown otherwise).\n\ninclude_links – Keep links along with their targets (experimental).\n\ndeduplicate – Remove duplicate segments and documents.\n\ndate_extraction_params – Provide extraction parameters to htmldate as dict().\n\nonly_with_metadata – Only keep documents featuring all essential metadata (date, title, url).\n\nmax_tree_size – Discard documents with too many elements.\n\nurl_blacklist – Provide a blacklist of URLs as set() to filter out documents.\n\nauthor_blacklist – Provide a blacklist of Author Names as set() to filter out authors.\n\nas_dict – Legacy option, return a dictionary instead of a class with attributes.\n\nconfig – Directly provide a configparser configuration.\n\nReturns\n:\n\nA Python dict() containing all the extracted information or None.\n\nRaises\n:\n\nValueError – Extraction problem.\n\nbaseline()\ntrafilatura.baseline(filecontent)\n[source]\n\nUse baseline extraction function targeting text paragraphs and/or JSON metadata.\n\nParameters\n:\n\nfilecontent – HTML code as binary string or string.\n\nReturns\n:\n\nA LXML <body> element containing the extracted paragraphs, the main text as string, and its length as integer.\n\nhtml2txt()\ntrafilatura.html2txt(content)\n[source]\n\nRun basic html2txt on a document.\n\nParameters\n:\n\ncontent – HTML document as string or LXML element.\n\nReturns\n:\n\nThe extracted text in the form of a string or an empty string.\n\ntry_readability()\ntrafilatura.external.try_readability(htmlinput)\n[source]\n\nSafety net: try with the generic algorithm readability\n\ntry_justext()\ntrafilatura.external.try_justext(tree, url, target_language)\n[source]\n\nSecond safety net: try with the generic algorithm justext\n\nextract_metadata()\ntrafilatura.extract_metadata(filecontent, default_url=None, date_config=None, fastmode=False, author_blacklist=None)\n[source]\n\nMain process for metadata extraction.\n\nParameters\n:\n\nfilecontent – HTML code as string.\n\ndefault_url – Previously known URL of the downloaded document.\n\ndate_config – Provide extraction parameters to htmldate as dict().\n\nauthor_blacklist – Provide a blacklist of Author Names as set() to filter out authors.\n\nReturns\n:\n\nA trafilatura.metadata.Document containing the extracted metadata information or None. trafilatura.metadata.Document has .as_dict() method that will return a copy as a dict.\n\nextract_comments()\ntrafilatura.core.extract_comments(tree, options)\n[source]\n\nTry and extract comments out of potential sections in the HTML\n\nLink discovery\nsitemap_search()\ntrafilatura.sitemaps.sitemap_search(url: str, target_lang: str | None = None) → List[str]\n[source]\n\nLook for sitemaps for the given URL and gather links.\n\nParameters\n:\n\nurl – Webpage or sitemap URL as string. Triggers URL-based filter if the webpage isn’t a homepage.\n\ntarget_lang – Define a language to filter URLs based on heuristics (two-letter string, ISO 639-1 format).\n\nReturns\n:\n\nThe extracted links as a list (sorted list of unique links).\n\nfind_feed_urls()\ntrafilatura.feeds.find_feed_urls(url, target_lang=None)\n[source]\n\nTry to find feed URLs.\n\nParameters\n:\n\nurl – Webpage or feed URL as string. Triggers URL-based filter if the webpage isn’t a homepage.\n\ntarget_lang – Define a language to filter URLs based on heuristics (two-letter string, ISO 639-1 format).\n\nReturns\n:\n\nThe extracted links as a list (sorted list of unique links).\n\nfocused_crawler()\ntrafilatura.spider.focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=None, known_links=None, lang=None, config=<configparser.ConfigParser object>, rules=None)\n[source]\n\nBasic crawler targeting pages of interest within a website.\n\nParameters\n:\n\nhomepage – URL of the page to first page to fetch, preferably the homepage of a website.\n\nmax_seen_urls – maximum number of pages to visit, stop iterations at this number or at the exhaustion of pages on the website, whichever comes first.\n\nmax_known_urls – stop if the total number of pages “known” exceeds this number.\n\ntodo – provide a previously generated list of pages to visit / crawl frontier, must be in collections.deque format.\n\nknown_links – provide a previously generated set of links.\n\nlang – try to target links according to language heuristics.\n\nconfig – use a different configuration (configparser format).\n\nrules – provide politeness rules (urllib.robotparser.RobotFileParser() format).\n\nReturns\n:\n\nList of pages to visit, deque format, possibly empty if there are no further pages to visit. Set of known links.\n\nHelpers\nfetch_url()\ntrafilatura.fetch_url(url, decode=True, no_ssl=False, config=<configparser.ConfigParser object>)\n[source]\n\nFetches page using urllib3 and decodes the response.\n\nParameters\n:\n\nurl – URL of the page to fetch.\n\ndecode – Decode response instead of returning urllib3 response object (boolean).\n\nno_ssl – Don’t try to establish a secure connection (to prevent SSLError).\n\nconfig – Pass configuration values for output control.\n\nReturns\n:\n\ndata (headers + body), status (HTML code as string) and url or None in case the result is invalid or there was a problem with the network.\n\nReturn type\n:\n\nRawResponse object\n\ndecode_response()\ntrafilatura.utils.decode_response(response)\n[source]\n\nRead the urllib3 object corresponding to the server response, check if it could be GZip and eventually decompress it, then try to guess its encoding and decode it to return a unicode string\n\nload_html()\ntrafilatura.load_html(htmlobject)\n[source]\n\nLoad object given as input and validate its type (accepted: lxml.html tree, trafilatura/urllib3 response, bytestring and string)\n\nsanitize()\ntrafilatura.utils.sanitize(text)\n[source]\n\nConvert text and discard incompatible and invalid characters\n\ntrim()\ntrafilatura.utils.trim(string)\n[source]\n\nRemove unnecessary spaces within a text string\n\nXML processing\nxmltotxt()\ntrafilatura.xml.xmltotxt(xmloutput, include_formatting)\n[source]\n\nConvert to plain text format and optionally preserve formatting as markdown.\n\nvalidate_tei()\ntrafilatura.xml.validate_tei(xmldoc)\n[source]\n\nCheck if an XML document is conform to the guidelines of the Text Encoding Initiative\n\nPrevious\n\nEvaluation\n\nNext\n\nUses & citations\n\n On this page\nExtraction\nLink discovery\nHelpers\nXML processing\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Evaluation — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/evaluation.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nEvaluation\nEvaluation\n\nAlthough text is ubiquitous on the Web, extracting information from web pages can prove to be difficult. Should the tooling be adapted to particular news outlets or blogs that are targeted (which often amounts to the development of web scraping tools) or should the extraction be as generic as possible to provide opportunistic ways of gathering information?\n\nThe extraction focuses on the main content, which is usually the part displayed centrally, without the left or right bars, the header or the footer, but including potential titles and (optionally) comments. This task is also known as web scraping, boilerplate removal, DOM-based content extraction, main content identification, or web page cleaning.\n\nAlternatives\n\nAlthough a few corresponding Python packages are not actively maintained the following alternatives exist.\n\nThese packages keep the structure intact but do not focus on main text extraction:\n\nhtml2text converts HTML pages to Markup language\n\nhtml_text converts HTML code to plain text\n\ninscriptis converts HTML to text with a particular emphasis on nested tables\n\nThese packages focus on main text extraction:\n\nboilerpy3 is a Python version of the boilerpipe algorithm for boilerplate removal and fulltext extraction\n\ndragnet is not maintained anymore, it is provided for reference only (in older evaluations)\n\ngoose3 can extract information for embedded content but doesn’t preserve markup\n\njusText is designed to preserve mainly text containing full sentences along with some markup, it has been explicitly developed to create linguistic resources\n\nnewspaper3k is mostly geared towards newspaper texts, provides additional functions but no structured text or comment extraction\n\nnews-please is a news crawler that extracts structured information\n\nreadability-lxml cleans the page and preserves some markup\n\nreadabilipy contains a Python wrapper for Mozilla’s Node.js package, as well as article extraction routines written in pure Python\n\ntrafilatura is the library documented here, several options are tested regarding main text extraction only, without metadata or comments\n\nThe tools are compared to the raw page source and to a meaningful baseline consisting of extracting the raw text contained in the JSON article element or in a combination of paragraph, code and quote elements.\n\nDescription\n\nTest set: The experiments below are run on a collection of documents which are either typical for Internet articles (news outlets, blogs). Some are harder to process due to mixed content (lists, tables) or not fully valid HTML code. They are selected from large collections of web pages in German, for the sake of completeness documents in other languages are added (notably English, French, other European languages, Chinese and Arabic, about 20-30% of the total).\n\nEvaluation: Decisive document segments are singled out which are not statistically representative but very significant in the perspective of working with the texts, most notably left/right columns, additional header, author or footer information such as imprints or addresses, as well as affiliated and social network links, in short boilerplate. Raw text segments are expected which is also a way to evaluate the quality of HTML extraction in itself.\n\nTime: The execution time is provided as an indication. As the baseline extraction is simple and fast, it is used for the benchmark. Certain packages are noticeably slower than the rest: goose3 and newspaper, while news-please’s execution time isn’t comparable because of operations unrelated to text extraction. ReadabiliPy is very slow for unclear reasons.\n\nErrors: The boilerpy3, newspaper3k, and readabilipy modules do not work without errors on every HTML file in the test set, probably because of malformed HTML, encoding or parsing bugs. These errors are ignored in order to complete the benchmark.\n\nResults: The baseline beats a few systems, showing its interest. justext is highly configurable and tweaking its configuration (as it is done here) can lead to better performance than its generic settings. goose3 is the most precise algorithm, albeit at a significant cost in terms of recall. The packages focusing on raw text extraction html_text and inscriptis are roughly comparable and achieve the best recall as they try to extract all the text. Rule-based approaches such as trafilatura’s obtain balanced results despite a lack of precision. Combined with an algorithmic approach they perform significantly better than the other tested solutions.\n\nRoadmap: Further evaluations will be run, including additional tools and languages. Comment extraction still has to be evaluated, although most libraries don not offer this functionality.\n\nThe evaluation script is available on the project repository: tests/README.rst. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the tests directory.\n\nResults (2022-05-18)\n\n750 documents, 2236 text & 2250 boilerplate segments, Python 3.8\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nDiff.\n\n\n\n\nraw HTML\n\n\t\n\n0.527\n\n\t\n\n0.874\n\n\t\n\n0.546\n\n\t\n\n0.658\n\n\t\n\n0\n\n\n\n\nhtml2text 2020.1.16\n\n\t\n\n0.486\n\n\t\n\n0.709\n\n\t\n\n0.481\n\n\t\n\n0.577\n\n\t\n\n7.6x\n\n\n\n\nhtml_text 0.5.2\n\n\t\n\n0.529\n\n\t\n\n0.958\n\n\t\n\n0.554\n\n\t\n\n0.682\n\n\t\n\n2.2x\n\n\n\n\ninscriptis 2.2.0 (html to txt)\n\n\t\n\n0.534\n\n\t\n\n0.959\n\n\t\n\n0.563\n\n\t\n\n0.686\n\n\t\n\n3.5x\n\n\n\n\nnewspaper3k 0.2.8\n\n\t\n\n0.895\n\n\t\n\n0.593\n\n\t\n\n0.762\n\n\t\n\n0.713\n\n\t\n\n12x\n\n\n\n\njustext 3.0.0 (custom)\n\n\t\n\n0.865\n\n\t\n\n0.650\n\n\t\n\n0.775\n\n\t\n\n0.742\n\n\t\n\n5.2x\n\n\n\n\nboilerpy3 1.0.6 (article mode)\n\n\t\n\n0.814\n\n\t\n\n0.744\n\n\t\n\n0.787\n\n\t\n\n0.777\n\n\t\n\n4.1x\n\n\n\n\nbaseline (text markup)\n\n\t\n\n0.757\n\n\t\n\n0.827\n\n\t\n\n0.781\n\n\t\n\n0.790\n\n\t\n\n1x\n\n\n\n\ngoose3 3.1.9\n\n\t\n\n0.934\n\n\t\n\n0.690\n\n\t\n\n0.821\n\n\t\n\n0.793\n\n\t\n\n22x\n\n\n\n\nreadability-lxml 0.8.1\n\n\t\n\n0.891\n\n\t\n\n0.729\n\n\t\n\n0.820\n\n\t\n\n0.801\n\n\t\n\n5.8x\n\n\n\n\nnews-please 1.5.22\n\n\t\n\n0.898\n\n\t\n\n0.734\n\n\t\n\n0.826\n\n\t\n\n0.808\n\n\t\n\n61x\n\n\n\n\nreadabilipy 0.2.0\n\n\t\n\n0.877\n\n\t\n\n0.870\n\n\t\n\n0.874\n\n\t\n\n0.874\n\n\t\n\n248x\n\n\n\n\ntrafilatura 1.2.2 (fast)\n\n\t\n\n0.914\n\n\t\n\n0.886\n\n\t\n\n0.902\n\n\t\n\n0.900\n\n\t\n\n4.8x\n\n\n\n\ntrafilatura 1.2.2 (precision)\n\n\t\n\n0.932\n\n\t\n\n0.874\n\n\t\n\n0.905\n\n\t\n\n0.902\n\n\t\n\n9.4x\n\n\n\n\ntrafilatura 1.2.2 (standard)\n\n\t\n\n0.914\n\n\t\n\n0.904\n\n\t\n\n0.910\n\n\t\n\n0.909\n\n\t\n\n7.1x\n\nExternal evaluations\n\nTrafilatura is the most efficient open-source library in ScrapingHub’s article extraction benchmark.\n\nBest overall tool according to Gaël Lejeune & Adrien Barbaresi, Bien choisir son outil d’extraction de contenu à partir du Web (2020, PDF, in French).\n\nComparison on a small sample of Polish news texts and forums.\n\nOlder results\nOlder results (2021-06-07)\n\n500 documents, 1487 text and 1496 boilerplate segments\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nDiff.\n\n\n\n\nraw HTML\n\n\t\n\n0.527\n\n\t\n\n0.878\n\n\t\n\n0.547\n\n\t\n\n0.659\n\n\t\n\n0\n\n\n\n\nhtml2text 2020.1.16\n\n\t\n\n0.488\n\n\t\n\n0.714\n\n\t\n\n0.484\n\n\t\n\n0.580\n\n\t\n\n8.9x\n\n\n\n\nhtml_text 0.5.2\n\n\t\n\n0.526\n\n\t\n\n0.958\n\n\t\n\n0.548\n\n\t\n\n0.679\n\n\t\n\n1.9x\n\n\n\n\ninscriptis 1.1 (html to txt)\n\n\t\n\n0.531\n\n\t\n\n0.958\n\n\t\n\n0.556\n\n\t\n\n0.683\n\n\t\n\n2.4x\n\n\n\n\njustext 2.2.0 (custom)\n\n\t\n\n0.870\n\n\t\n\n0.584\n\n\t\n\n0.749\n\n\t\n\n0.699\n\n\t\n\n6.1x\n\n\n\n\nnewspaper3k 0.2.8\n\n\t\n\n0.921\n\n\t\n\n0.574\n\n\t\n\n0.763\n\n\t\n\n0.708\n\n\t\n\n12.9x\n\n\n\n\nboilerpy3 1.0.2 (article mode)\n\n\t\n\n0.851\n\n\t\n\n0.696\n\n\t\n\n0.788\n\n\t\n\n0.766\n\n\t\n\n4.8x\n\n\n\n\ngoose3 3.1.9\n\n\t\n\n0.950\n\n\t\n\n0.644\n\n\t\n\n0.806\n\n\t\n\n0.767\n\n\t\n\n18.8x\n\n\n\n\nbaseline (text markup)\n\n\t\n\n0.746\n\n\t\n\n0.804\n\n\t\n\n0.766\n\n\t\n\n0.774\n\n\t\n\n1x\n\n\n\n\ndragnet 2.0.4\n\n\t\n\n0.906\n\n\t\n\n0.689\n\n\t\n\n0.810\n\n\t\n\n0.783\n\n\t\n\n3.1x\n\n\n\n\nreadability-lxml 0.8.1\n\n\t\n\n0.917\n\n\t\n\n0.716\n\n\t\n\n0.826\n\n\t\n\n0.804\n\n\t\n\n5.9x\n\n\n\n\nnews-please 1.5.21\n\n\t\n\n0.924\n\n\t\n\n0.718\n\n\t\n\n0.830\n\n\t\n\n0.808\n\n\t\n\n60x\n\n\n\n\ntrafilatura 0.8.2 (fast)\n\n\t\n\n0.925\n\n\t\n\n0.868\n\n\t\n\n0.899\n\n\t\n\n0.896\n\n\t\n\n3.9x\n\n\n\n\ntrafilatura 0.8.2\n\n\t\n\n0.934\n\n\t\n\n0.890\n\n\t\n\n0.914\n\n\t\n\n0.912\n\n\t\n\n8.4x\n\nOlder results (2020-11-06)\n\n500 documents, 1487 text and 1496 boilerplate segments\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nDiff.\n\n\n\n\nraw HTML\n\n\t\n\n0.527\n\n\t\n\n0.878\n\n\t\n\n0.547\n\n\t\n\n0.659\n\n\t\n\n0\n\n\n\n\nhtml2text 2020.1.16\n\n\t\n\n0.488\n\n\t\n\n0.714\n\n\t\n\n0.484\n\n\t\n\n0.580\n\n\t\n\n8.9x\n\n\n\n\nhtml_text 0.5.2\n\n\t\n\n0.526\n\n\t\n\n0.958\n\n\t\n\n0.548\n\n\t\n\n0.679\n\n\t\n\n1.9x\n\n\n\n\ninscriptis 1.1 (html to txt)\n\n\t\n\n0.531\n\n\t\n\n0.958\n\n\t\n\n0.556\n\n\t\n\n0.683\n\n\t\n\n2.4x\n\n\n\n\njustext 2.2.0 (tweaked)\n\n\t\n\n0.870\n\n\t\n\n0.584\n\n\t\n\n0.749\n\n\t\n\n0.699\n\n\t\n\n6.1x\n\n\n\n\nnewspaper3k 0.2.8\n\n\t\n\n0.921\n\n\t\n\n0.574\n\n\t\n\n0.763\n\n\t\n\n0.708\n\n\t\n\n12.9x\n\n\n\n\ngoose3 3.1.6\n\n\t\n\n0.950\n\n\t\n\n0.629\n\n\t\n\n0.799\n\n\t\n\n0.757\n\n\t\n\n19.0x\n\n\n\n\nboilerpy3 1.0.2 (article mode)\n\n\t\n\n0.851\n\n\t\n\n0.696\n\n\t\n\n0.788\n\n\t\n\n0.766\n\n\t\n\n4.8x\n\n\n\n\nbaseline (text markup)\n\n\t\n\n0.746\n\n\t\n\n0.804\n\n\t\n\n0.766\n\n\t\n\n0.774\n\n\t\n\n1x\n\n\n\n\ndragnet 2.0.4\n\n\t\n\n0.906\n\n\t\n\n0.689\n\n\t\n\n0.810\n\n\t\n\n0.783\n\n\t\n\n3.1x\n\n\n\n\nreadability-lxml 0.8.1\n\n\t\n\n0.917\n\n\t\n\n0.716\n\n\t\n\n0.826\n\n\t\n\n0.804\n\n\t\n\n5.9x\n\n\n\n\nnews-please 1.5.13\n\n\t\n\n0.923\n\n\t\n\n0.711\n\n\t\n\n0.827\n\n\t\n\n0.804\n\n\t\n\n184x\n\n\n\n\ntrafilatura 0.6.0\n\n\t\n\n0.924\n\n\t\n\n0.849\n\n\t\n\n0.890\n\n\t\n\n0.885\n\n\t\n\n3.9x\n\n\n\n\ntrafilatura 0.6.0 (+ fallbacks)\n\n\t\n\n0.933\n\n\t\n\n0.877\n\n\t\n\n0.907\n\n\t\n\n0.904\n\n\t\n\n8.4x\n\nOlder results (2020-07-16)\n\n400 documents, 1186 text and 1198 boilerplate segments\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nDiff.\n\n\n\n\nraw HTML\n\n\t\n\n0.524\n\n\t\n\n0.879\n\n\t\n\n0.543\n\n\t\n\n0.657\n\n\t\n\n0\n\n\n\n\nhtml2text 2020.1.16\n\n\t\n\n0.485\n\n\t\n\n0.718\n\n\t\n\n0.480\n\n\t\n\n0.579\n\n\t\n\n8.4x\n\n\n\n\nhtml_text 0.5.1\n\n\t\n\n0.521\n\n\t\n\n0.962\n\n\t\n\n0.542\n\n\t\n\n0.676\n\n\t\n\n1.8x\n\n\n\n\ninscriptis 1.0 (html to txt)\n\n\t\n\n0.527\n\n\t\n\n0.965\n\n\t\n\n0.551\n\n\t\n\n0.681\n\n\t\n\n1.9x\n\n\n\n\nnewspaper3k 0.2.8\n\n\t\n\n0.916\n\n\t\n\n0.577\n\n\t\n\n0.763\n\n\t\n\n0.708\n\n\t\n\n11.8x\n\n\n\n\njustext 2.2.0 (tweaked)\n\n\t\n\n0.867\n\n\t\n\n0.651\n\n\t\n\n0.777\n\n\t\n\n0.744\n\n\t\n\n4.9x\n\n\n\n\ngoose3 3.1.6\n\n\t\n\n0.953\n\n\t\n\n0.635\n\n\t\n\n0.803\n\n\t\n\n0.762\n\n\t\n\n17.3x\n\n\n\n\nbaseline (text markup)\n\n\t\n\n0.738\n\n\t\n\n0.804\n\n\t\n\n0.760\n\n\t\n\n0.770\n\n\t\n\n1x\n\n\n\n\nboilerpy3 1.0.2 (article mode)\n\n\t\n\n0.847\n\n\t\n\n0.711\n\n\t\n\n0.792\n\n\t\n\n0.773\n\n\t\n\n4.4x\n\n\n\n\ndragnet 2.0.4\n\n\t\n\n0.906\n\n\t\n\n0.704\n\n\t\n\n0.816\n\n\t\n\n0.792\n\n\t\n\n2.8x\n\n\n\n\nreadability-lxml 0.8.1\n\n\t\n\n0.913\n\n\t\n\n0.739\n\n\t\n\n0.835\n\n\t\n\n0.817\n\n\t\n\n5.4x\n\n\n\n\nnews-please 1.4.25\n\n\t\n\n0.918\n\n\t\n\n0.739\n\n\t\n\n0.837\n\n\t\n\n0.819\n\n\t\n\n56.4x\n\n\n\n\ntrafilatura 0.5.1\n\n\t\n\n0.927\n\n\t\n\n0.854\n\n\t\n\n0.894\n\n\t\n\n0.889\n\n\t\n\n3.1x\n\n\n\n\ntrafilatura 0.5.1 (+ fallbacks)\n\n\t\n\n0.933\n\n\t\n\n0.885\n\n\t\n\n0.911\n\n\t\n\n0.908\n\n\t\n\n6.8x\n\nOlder results (2020-03-19)\n\n300 documents, 869 text and 878 boilerplate segments\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nTime\n\n\n\n\nraw HTML\n\n\t\n\n0.519\n\n\t\n\n0.885\n\n\t\n\n0.535\n\n\t\n\n0.654\n\n\t\n\n0\n\n\n\n\nbaseline (text markup)\n\n\t\n\n0.726\n\n\t\n\n0.776\n\n\t\n\n0.742\n\n\t\n\n0.750\n\n\t\n\n1.14\n\n\n\n\nhtml2text 2020.1.16\n\n\t\n\n0.499\n\n\t\n\n0.787\n\n\t\n\n0.501\n\n\t\n\n0.611\n\n\t\n\n11.00\n\n\n\n\ninscriptis 1.0 (html to txt)\n\n\t\n\n0.521\n\n\t\n\n0.962\n\n\t\n\n0.541\n\n\t\n\n0.676\n\n\t\n\n2.47\n\n\n\n\njustext 2.2.0 (German stoplist)\n\n\t\n\n0.849\n\n\t\n\n0.529\n\n\t\n\n0.719\n\n\t\n\n0.652\n\n\t\n\n6.37\n\n\n\n\nnewspaper 0.2.8\n\n\t\n\n0.923\n\n\t\n\n0.591\n\n\t\n\n0.772\n\n\t\n\n0.721\n\n\t\n\n14.80\n\n\n\n\ngoose3 3.1.6\n\n\t\n\n0.957\n\n\t\n\n0.640\n\n\t\n\n0.807\n\n\t\n\n0.767\n\n\t\n\n21.54\n\n\n\n\nboilerpy3 1.0.2 (article mode)\n\n\t\n\n0.841\n\n\t\n\n0.734\n\n\t\n\n0.799\n\n\t\n\n0.784\n\n\t\n\n5.65\n\n\n\n\ndragnet 2.0.4\n\n\t\n\n0.909\n\n\t\n\n0.722\n\n\t\n\n0.825\n\n\t\n\n0.804\n\n\t\n\n3.64\n\n\n\n\nreadability-lxml 0.7.1\n\n\t\n\n0.928\n\n\t\n\n0.743\n\n\t\n\n0.844\n\n\t\n\n0.826\n\n\t\n\n6.59\n\n\n\n\nnews-please 1.4.25\n\n\t\n\n0.926\n\n\t\n\n0.747\n\n\t\n\n0.844\n\n\t\n\n0.827\n\n\t\n\n70.81\n\n\n\n\ntrafilatura 0.3.1 (rule-based)\n\n\t\n\n0.901\n\n\t\n\n0.831\n\n\t\n\n0.871\n\n\t\n\n0.865\n\n\t\n\n5.43\n\n\n\n\ntrafilatura 0.3.1 (+ justext)\n\n\t\n\n0.897\n\n\t\n\n0.868\n\n\t\n\n0.884\n\n\t\n\n0.882\n\n\t\n\n6.97\n\n\n\n\ntrafilatura 0.4\n\n\t\n\n0.914\n\n\t\n\n0.869\n\n\t\n\n0.894\n\n\t\n\n0.891\n\n\t\n\n4.87\n\n\n\n\ntrafilatura 0.4 (+ fallback)\n\n\t\n\n0.925\n\n\t\n\n0.904\n\n\t\n\n0.916\n\n\t\n\n0.914\n\n\t\n\n9.94\n\nOlder results (2020-01-29)\n\n100 documents, 266 text and 294 boilerplate segments\n\n\n\n\nPython Package\n\n\t\n\nPrecision\n\n\t\n\nRecall\n\n\t\n\nAccuracy\n\n\t\n\nF-Score\n\n\t\n\nTime\n\n\n\n\nraw HTML\n\n\t\n\n0.492\n\n\t\n\n0.902\n\n\t\n\n0.511\n\n\t\n\n0.637\n\n\t\n\n0\n\n\n\n\ninscriptis 1.0 (html to txt)\n\n\t\n\n0.504\n\n\t\n\n0.989\n\n\t\n\n0.532\n\n\t\n\n0.668\n\n\t\n\n0.87\n\n\n\n\njustext 2.2.0 (German stoplist)\n\n\t\n\n0.886\n\n\t\n\n0.553\n\n\t\n\n0.754\n\n\t\n\n0.681\n\n\t\n\n2.22\n\n\n\n\ngoose3 3.1.6\n\n\t\n\n0.935\n\n\t\n\n0.594\n\n\t\n\n0.787\n\n\t\n\n0.726\n\n\t\n\n7.64\n\n\n\n\nnewspaper 0.2.8\n\n\t\n\n0.920\n\n\t\n\n0.609\n\n\t\n\n0.789\n\n\t\n\n0.733\n\n\t\n\n5.34\n\n\n\n\nboilerpy3 1.0.2 (default mode)\n\n\t\n\n0.767\n\n\t\n\n0.756\n\n\t\n\n0.775\n\n\t\n\n0.761\n\n\t\n\n1.89\n\n\n\n\ndragnet 2.0.4\n\n\t\n\n0.904\n\n\t\n\n0.673\n\n\t\n\n0.811\n\n\t\n\n0.772\n\n\t\n\n1.25\n\n\n\n\nreadability-lxml 0.7.1\n\n\t\n\n0.894\n\n\t\n\n0.699\n\n\t\n\n0.818\n\n\t\n\n0.785\n\n\t\n\n2.34\n\n\n\n\nnews-please 1.4.25\n\n\t\n\n0.900\n\n\t\n\n0.714\n\n\t\n\n0.827\n\n\t\n\n0.797\n\n\t\n\n22.99\n\n\n\n\ntrafilatura 0.3.1 (rule-based)\n\n\t\n\n0.872\n\n\t\n\n0.895\n\n\t\n\n0.887\n\n\t\n\n0.883\n\n\t\n\n1.87\n\n\n\n\ntrafilatura 0.3.1 (+ justext)\n\n\t\n\n0.889\n\n\t\n\n0.936\n\n\t\n\n0.914\n\n\t\n\n0.912\n\n\t\n\n2.19\n\nPrevious\n\nTutorial: DWDS-Korpusdaten reproduzieren\n\nNext\n\nCore functions\n\n On this page\nAlternatives\nDescription\nResults (2022-05-18)\nExternal evaluations\nOlder results\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Tutorials — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/tutorials.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nTutorials\nTutorials\nTutorial: Gathering a custom web corpus\nGet your system up and running\nContent discovery\nLink filtering\nProcess a list of links\nTutorial: From a list of links to a frequency list\nGet your system up and running\nProcess a list of links\nBuild frequency lists\nTutorial: Validation of TEI files\nProducing TEI files\nValidating existing files\nText embedding\nWhy perform text embedding with crawled data?\nSetup Epsilla\nCrawl project homepages and store their vector embeddings in Epsilla\nPerform vector search\nTutorial: DWDS-Korpusdaten reproduzieren\nZiel\nVon einer Abfrage zur Einsicht der Quellen\nInteresse und Gestaltungsmöglichkeiten\nDownload und Verarbeitung der Daten\nBlog posts\n\nExtracting the main text content from web pages using Python\n\nValidating TEI-XML documents with Python\n\nEvaluating scraping and text extraction tools for Python\n\nFiltering links to gather texts on the web\n\nUsing sitemaps to crawl websites on the command-line\n\nUsing RSS and Atom feeds to collect web pages with Python\n\nWeb scraping with R: Text and metadata extraction\n\nWeb scraping with Trafilatura just got faster\n\nVideos\nYoutube playlist\n\nWeb scraping how-tos and tutorials.\n\nExternal resources\nGLAM-Workbench\n\nHarvesting collections of text from archived web pages\n\nCompare two versions of an archived web page\n\nUser Ethics & Legal Concerns\n\nDownload von Web-Daten & Daten aufbereiten und verwalten (Tutorials in German by Noah Bubenhofer)\n\nPrevious\n\nURL management\n\nNext\n\nTutorial: Gathering a custom web corpus\n\n On this page\nBlog posts\nVideos\nExternal resources\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Usage — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/usage.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nUsage\nQuickstart\nWith Python\nOn the command-line\nWith Python\nThe Python programming language\nStep-by-step\nExtraction settings\nInput/Output types\nNavigation\nOn the command-line\nIntroduction\nQuickstart\nExtraction parameters\nProcess files locally\nProcess a list of links\nLink discovery\nFurther information\nWith R\nIntroduction\nInstallation\nDownload and extraction\nOther functions\nGoing further\nGraphical user interface\nScreenshot\nDownload web pages\nWith Python\nOn the command-line\nEnforcing politeness rules\nSummary\nWeb crawling\nDesign decisions\nWith Python\nOn the command-line\nReferences\nDefault settings\nConfiguration file\nPackage settings\nTroubleshooting\nContent extraction\nDownloads\nURL management\nFiltering a list of URLs\nSampling by domain name\nBlacklisting\n\nPrevious\n\nInstallation of graphical user interface\n\nNext\n\nQuickstart\n\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "A Python package & command-line tool to gather text on the Web — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/index.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\nA Python package & command-line tool to gather text on the Web\n    \n\n\nDescription\n\nTrafilatura is a Python package and command-line tool designed to gather text on the Web. It includes discovery, extraction and text processing components. Its main applications are web crawling, downloads, scraping, and extraction of main texts, metadata and comments. It aims at staying handy and modular: no database is required, the output can be converted to various commonly used formats.\n\nGoing from raw HTML to essential parts can alleviate many problems related to text quality, first by avoiding the noise caused by recurring elements (headers, footers, links/blogroll etc.) and second by including information such as author and date in order to make sense of the data. The extractor tries to strike a balance between limiting noise (precision) and including all valid parts (recall). It also has to be robust and reasonably fast, it runs in production on millions of documents.\n\nThis tool can be useful for quantitative research in corpus linguistics, natural language processing, computational social science and beyond: it is relevant to anyone interested in data science, information extraction, text mining, and scraping-intensive use cases like search engine optimization, business analytics or information security.\n\nFeatures\nWeb crawling and text discovery:\n\nFocused crawling and politeness rules\n\nSupport for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)\n\nURL management (blacklists, filtering and de-duplication)\n\nSeamless and parallel processing, online and offline:\n\nURLs, HTML files or parsed HTML trees usable as input\n\nEfficient and polite processing of download queues\n\nConversion of previously downloaded files\n\nRobust and efficient extraction:\n\nMain text (with LXML, common patterns and generic algorithms: jusText, fork of readability-lxml)\n\nMetadata (title, author, date, site name, categories and tags)\n\nFormatting and structural elements: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting\n\nComments (if applicable)\n\nOutput formats:\n\nText (minimal formatting or Markdown)\n\nCSV (with metadata, tab-separated values)\n\nJSON (with metadata)\n\nXML (with metadata, text formatting and page structure) and TEI-XML\n\nOptional add-ons:\n\nLanguage detection on extracted content\n\nGraphical user interface (GUI)\n\nSpeed optimizations\n\nEvaluation and alternatives\n\nFor detailed results see the benchmark and evaluation script. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the tests directory.\n\nOther evaluations:\n\nMost efficient open-source library in ScrapingHub’s article extraction benchmark\n\nBest overall tool according to Gaël Lejeune & Adrien Barbaresi, Bien choisir son outil d’extraction de contenu à partir du Web (2020, PDF, French)\n\nIn a nutshell\n\nPrimary installation method is with a Python package manager: pip install trafilatura. See installation documentation.\n\nWith Python:\n\n>>> import trafilatura\n>>> downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n>>> trafilatura.extract(downloaded)\n# outputs main content and comments as plain text ...\n\n\nOn the command-line:\n\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n# outputs main content and comments as plain text ...\n\n\nFor more information please refer to usage documentation and tutorials.\n\nLicense\n\nTrafilatura is distributed under the GNU General Public License v3.0. If you wish to redistribute this library but feel bounded by the license conditions please try interacting at arms length, multi-licensing with compatible licenses, or contacting me.\n\nSee also GPL and free software licensing: What’s in it for business?\n\nContext\n\nThese documentation pages also provide information on concepts behind data collection as well as practical tips on how to gather web texts (see tutorials).\n\nContributing\n\nContributions are welcome! See CONTRIBUTING.md for more information. Bug reports can be filed on the dedicated page.\n\nMany thanks to the contributors who submitted features and bugfixes!\n\nRoadmap\n\nFor planned enhancements and relevant milestones see issues page.\n\nAuthor\n\nThis effort is part of methods to derive information from web documents in order to build text databases for research (chiefly linguistic analysis and natural language processing). Extracting and pre-processing web texts to the exacting standards of scientific research presents a substantial challenge for those who conduct such research. Web corpus construction involves numerous design decisions, and this software package can help facilitate text data collection and enhance corpus quality.\n\nBarbaresi, A. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction, Proceedings of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131.\n\nBarbaresi, A. “Generic Web Content Extraction with Open-Source Software”, Proceedings of KONVENS 2019, Kaleidoscope Abstracts, 2019.\n\nBarbaresi, A. “Efficient construction of metadata-enhanced web corpora”, Proceedings of the 10th Web as Corpus Workshop (WAC-X), 2016.\n\n \n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n\n\nYou can contact me via my contact page or on GitHub.\n\nSoftware ecosystem\n\nTrafilatura: Italian word for wire drawing.\n\nKnown uses of the software.\n\nCorresponding posts on Bits of Language (blog).\n\nFurther documentation\nInstallation\nPython\nTrafilatura package\nAdditional functionality\nGraphical user interface\nUsage\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nTutorials\nTutorial: Gathering a custom web corpus\nTutorial: From a list of links to a frequency list\nTutorial: Validation of TEI files\nText embedding\nTutorial: DWDS-Korpusdaten reproduzieren\nBlog posts\nVideos\nExternal resources\nEvaluation\nAlternatives\nDescription\nResults (2022-05-18)\nExternal evaluations\nOlder results\nCore functions\nExtraction\nLink discovery\nHelpers\nXML processing\nUses & citations\nNotable projects using this software\nCitations in papers\nPublications citing Trafilatura\nPublications citing Htmldate\nPorts\nBackground\nCompendium: Web texts in linguistics and humanities\nFinding sources for web corpora\nWorking with corpus data\nIndices and tables\n\nIndex\n\nModule Index\n\nSearch Page\n\nNext\n\nInstallation\n\n On this page\nA Python package & command-line tool to gather text on the Web\nDescription\nIn a nutshell\nLicense\nContext\nFurther documentation\nIndices and tables\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Installation — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/installation.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nInstallation of graphical user interface\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nInstallation\nInstallation\nPython\n\nTrafilatura runs using Python, currently one of the most frequently used programming languages.\n\nThis software library/package is tested on Linux, macOS and Windows systems. It is compatible with all recent versions of Python:\n\nInstalling Python 3 on Mac OS X (& official documentation for Mac)\n\nInstalling Python 3 on Windows (& official documentation for Windows)\n\nInstalling Python 3 on Linux (& official documentation for Unix)\n\nBeginners guide: downloading Python\n\nThen you need a version of Python to interact with as well as the Python packages needed for the task. A recent version of Python 3 is necessary. Some systems already have such an environment installed, to check it just run the following command in a terminal window:\n\n$ python3 --version\nPython 3.8.6 # version 3.6 or higher is fine\n\n\nIn case Python is not installed, please refer to the excellent Djangogirls tutorial: Python installation.\n\nTrafilatura package\n\nTrafilatura is packaged as a software library available from the package repository PyPI. As such it can notably be installed with pip or pipenv.\n\nInstalling Python packages\n\nStraightforward: Installing packages in python using pip (& official documentation) - Using pip on Windows\n\nAdvanced: Pipenv & Virtual Environments\n\nBasics\n\nPlease refer to this section for an introduction on command-line usage.\n\n$ pip install trafilatura # pip3 where applicable\n\n\nThis project is under active development, please make sure you keep it up-to-date to benefit from latest improvements:\n\n# to make sure you have the latest version\n$ pip install -U trafilatura\n# latest available code base\n$ pip install --force-reinstall -U git+https://github.com/adbar/trafilatura\n\n\nOn Mac OS it can be necessary to install certificates by hand if you get errors like [SSL: CERTIFICATE_VERIFY_FAILED] while downloading webpages: execute pip install certifi and perform the post-installation step by clicking on /Applications/Python 3.X/Install Certificates.command. For more information see this help page on SSL errors.\n\nHint\n\nInstallation on MacOS is generally easier with brew.\n\nOlder Python versions\n\nLast version for Python 3.5: pip install trafilatura==0.9.3\n\nLast version for Python 3.4: pip install trafilatura==0.8.2\n\nCommand-line tool\n\nIf you installed the library successfully but cannot start the command-line tool, try adding the user-level bin directory to your PATH environment variable. If you are using a Unix derivative (e.g. Linux, OS X), you can achieve this by running the following command: export PATH=\"$HOME/.local/bin:$PATH\".\n\nFor local or user installations where trafilatura cannot be used from the command-line, please refer to the official Python documentation and this page on finding executables from the command-line.\n\nAdditional functionality\nOptional modules\n\nA few additional libraries can be installed for extended functionality and faster processing: language detection and faster encoding detection: the cchardet package may not work on all systems but it is highly recommended.\n\n$ pip install cchardet  # single package only\n$ pip install trafilatura[all]  # all additional functionality\n\n\nFor infos on dependency management of Python packages see this discussion thread.\n\nHint\n\nEverything works even if not all packages are installed (e.g. because installation fails).\n\nYou can also install or update relevant packages separately, trafilatura will detect which ones are present on your system and opt for the best available combination.\n\nbrotli\n\nAdditional compression algorithm for downloads\n\ncchardet / faust-cchardet (Python >= 3.11)\n\nFaster encoding detection, also possibly more accurate (especially for encodings used in Asia)\n\nhtmldate[all] / htmldate[speed]\n\nFaster and more precise date extraction with a series of dedicated packages\n\npy3langid\n\nLanguage detection on extracted main text\n\npycurl\n\nFaster downloads, possibly less robust though\n\nGraphical user interface\nInstallation of graphical user interface\nInstallation\nGetting started\nTroubleshooting\nScreenshot\n\nPrevious\n\nA Python package & command-line tool to gather text on the Web\n\nNext\n\nInstallation of graphical user interface\n\n On this page\nPython\nTrafilatura package\nAdditional functionality\nGraphical user interface\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  },
  {
    "title": "Quickstart — trafilatura 1.6.2 documentation",
    "url": "https://trafilatura.readthedocs.io/en/latest/quickstart.html",
    "html": "Skip to main content\nInstallation\nUsage\nTutorials\nEvaluation\nCore functions\nMore \nSearch\n⌘\n+\nK\nGitHub\nTwitter\n\nSection Navigation\n\nQuickstart\nWith Python\nOn the command-line\nWith R\nGraphical user interface\nDownload web pages\nWeb crawling\nDefault settings\nTroubleshooting\nURL management\nAdvertising for Developers Reach your niche with powerful contextual targeting powered by ML\nAd by EthicalAds   ·   ℹ️\nv: latest\nUsage\nQuickstart\nQuickstart\n\nPrimary installation method is with a Python package manager: pip install trafilatura. See installation documentation.\n\nWith Python\n\nThe only required argument is the input document (here a downloaded HTML file), the rest is optional.\n\n# import the necessary functions\n>>> from trafilatura import fetch_url, extract\n\n# grab a HTML file to extract data from\n>>> downloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n\n# output main content and comments as plain text\n>>> result = extract(downloaded)\n\n# change the output format to XML (allowing for preservation of document structure)\n>>> result = extract(downloaded, output_format=\"xml\")\n\n# discard potential comment and change the output to JSON\n>>> extract(downloaded, output_format=\"json\", include_comments=False)\n\n\nThe use of fallback algorithms can also be bypassed in fast mode:\n\n# faster mode without backup extraction\n>>> result = extract(downloaded, no_fallback=True)\n\n\nFor a full list of options see Python usage.\n\nThe extraction targets the main text part of a webpage. To extract all text content in a html2txt manner use this function:\n\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\n\nOn the command-line\n\nURLs can be used directly (-u/–URL):\n\n# outputs main content and comments as plain text\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n\n# displays help message with all possible options\n$ trafilatura -h\n\n\nYou can also pipe a HTML document (and response body) to trafilatura:\n\n$ cat myfile.html | trafilatura # use the contents of an already existing file\n$ < myfile.html trafilatura # same here\n\n\nExtraction options are also available on the command-line, they can be combined:\n\n$ < myfile.html trafilatura --json --no-tables\n\n\nFor more information please refer to usage documentation and tutorials.\n\nPrevious\n\nUsage\n\nNext\n\nWith Python\n\n On this page\nWith Python\nOn the command-line\n Show Source\n\n© Copyright 2023, Adrien Barbaresi.\n\n\nBuilt with the PyData Sphinx Theme 0.14.3."
  }
]